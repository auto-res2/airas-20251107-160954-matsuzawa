{
  "research_topic": "Improving fine-tuning performance of language models.",
  "queries": [
    "parameter-efficient fine-tuning",
    "LoRA fine-tuning",
    "adapter-based tuning",
    "fine-tuning regularization",
    "hyperparameter search fine-tuning"
  ],
  "research_study_list": [
    {
      "title": "Spectral Adapter: Fine-Tuning in Spectral Space",
      "abstract": "Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for\npretrained deep neural networks have captured widespread interest. In this\nwork, we study the enhancement of current PEFT methods by incorporating the\nspectral information of pretrained weight matrices into the fine-tuning\nprocedure. We investigate two spectral adaptation mechanisms, namely additive\ntuning and orthogonal rotation of the top singular vectors, both are done via\nfirst carrying out Singular Value Decomposition (SVD) of pretrained weights and\nthen fine-tuning the top spectral space. We provide a theoretical analysis of\nspectral fine-tuning and show that our approach improves the rank capacity of\nlow-rank adapters given a fixed trainable parameter budget. We show through\nextensive experiments that the proposed fine-tuning model enables better\nparameter efficiency and tuning performance as well as benefits multi-adapter\nfusion.",
      "full_text": "Spectral Adapter: Fine-Tuning in Spectral Space Fangzhao Zhang Electrical Engineering Stanford University zfzhao@stanford.edu Mert Pilanci Electrical Engineering Stanford University pilanci@stanford.edu Abstract Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for pre- trained deep neural networks have captured widespread interest. In this work, we study the enhancement of current PEFT methods by incorporating the spectral information of pretrained weight matrices into the fine-tuning procedure. We investigate two spectral adaptation mechanisms, namely additive tuning and or- thogonal rotation of the top singular vectors, both are done via first carrying out Singular Value Decomposition (SVD) of pretrained weights and then fine-tuning the top spectral space. We provide a theoretical analysis of spectral fine-tuning and show that our approach improves the rank capacity of low-rank adapters given a fixed trainable parameter budget. We show through extensive experiments that the proposed fine-tuning model enables better parameter efficiency and tun- ing performance as well as benefits multi-adapter fusion. Code is released at https://github.com/pilancilab/spectral_adapter. 1 Introduction Size of language and vision model undergoes a drastic explosion in recent days and results in billions of parameters up to date. While fine-tuning has been used a lot for adapting pretrained large models to various downstream tasks, fine-tuning tasks become increasingly hard with current size of pretrained models due to the huge demand of computing resource. Meanwhile, exchange and storing of fine- tuned models are also expensive given their enormous size. To alleviate these rising problems for fine-tuning large pretrained models, a recent line of research has digged into the Parameter-Efficient Fine-Tuning (PEFT) model family and harnessed great attention. A high-level philosophy behind those PEFT methods is to train a reduced number of parameters compared to full fine-tuning, which instantly saves computing resource and enables light-weight fine-tuned model exchange. Among all PEFT methods, Low-Rank Adaptation (LoRA) [ 20] model is a huge success attributed to its simplicity and effectiveness. Specifically, LoRA proposes to tune an additive trainable low-rank matrix and brings zero inference latency after merging the adapter into pretrained model weights. Since its emergence, numerous variants of LoRA have been developed. For instance, AdaLoRA [65], IncreLoRA [62], and DyLoRA [ 54] propose to dynamically adjust LoRA rank distribution for improving tuning efficiency, QLoRA [10] combines LoRA with model quantization to further save computing resource, LoRA+ [ 16] and PrecLoRA [ 61] study the optimization landscape of LoRA training, and more recent variant DoRA [32] decomposes pretrained weights into magnitude and direction components and applies LoRA for direction tuning, see Apppendix A for a more comprehensive review of different LoRA variants. Other PEFT methods such as Orthogonal Fine- Tuning (OFT) proposes to multiply pretrained weights by tunable orthogonal matrices for preservation of hypersphere energy between pretrained neurons. Though these different PEFT methods focus on improving fine-tuning efficiency with reduced parameters, rare attention has been paid to utilize pretrained model weights’ information beyond its magnitude in the fine-tuning procedure. Prior research in statistical machine learning such as [36] has studied the Empirical Spectral Distribu- tion (ESD) of deep models’ weight matrices and found that the ESDs for larger model weights are arXiv:2405.13952v2  [cs.LG]  4 Nov 2024Figure 1: Training loss of fine-tuning Llama3 8B model with Orca Math dataset [38] and evaluation score on GSM8K benchmark [7]. We follow experimental setup in [53], see Appendix F.1 for details. All methods except full fine-tuning maintain approximately 0.23% trainable parameters. usually more structured and contain indicative information to distinguish between different training stages. More recent work such as [3] investigates the \"dark matter\" effect of bottom spectral space of model weights and recognizes its critical role in attention sink phenomenon observed in [57]. Both work contributes to decrypting spectral information of model weights and sheds light on building insightful understanding of the connection between weight matrices’ spectral information and model performance. In this work, we explore further the value of model weights’ spectral pattern and unravel its effectiveness in enhancing fine-tuning tasks. We showcase via extensive empirical observation that integration of spectral information of pretrained model weights improves current PEFT methods’ parameter efficiency, tuning effect, and arises as a natural solution to multi-adapter fusion problems. Moreover, the suggested fine-tuning model maintains better practicality compared to prior spectral tuning models, which will be investigated further below. Though any technique for weight fine-tuning can be directly applied to fine-tune singular vector matrices of pretrained model weights, we investigate two specific forms of such extension, namely additive tuning and orthogonal rotating the top singular vector space, which we address as Spectral AdapterA and Spectral AdapterR respectively in later content. The spectral adaptation mechanisms being considered are formally depicted in Section 2. As a warmup, to show that incorporating spectral information is indeed helpful, Figure 1 displays the training loss of fine-tuning Llama3 8B model on HuggingFace Orca Math dataset and validation score on GSM8K benchmark, from which it can be clearly observed that Spectral AdapterA performs superior to recent variants of PEFT methods and behaves closest to full fine-tuning, here we follow experimental setup in [53], see Appendix F.1 for details and more investigation. In below, we first introduce the fine-tuning model being studied in Section 2 and we then provide some theoretic insights in Section 3. After that, we detail the advantage of our spectral adapter in enhancing fine-tuning result, improving model’s parameter efficiency, and helping with multi-adapter fusion as well as address any concern with respect to practicality issues in Section 4. Conclusion and future work is discussed in Section 5. For sake of page limitation, literature review is deferred to Appendix A. To summarize, the proposed spectral adaptation mechanism demonstrates the first attempt to fine-tune spectral space of pretrained model weights in a parameter-efficient and storage-economic way which improves current PEFT methods from aspects involving tuning results, parameter efficiency, and multi-adapter fusion. We hope this work serves as a building block and motivates further and deeper insightful investigation for exploring spectral structure of pretrained model weights, which becomes increasingly meaningful especially in current large model regime. 2 Spectral Adapter: Incorporating Spectral Information into Fine-Tuning Motivated by the intrinsic low-rank of weight shifts in fine-tuning procedure studied in [1], LoRA [20] proposes to add a low-rank factorized trainable matrix to pretrained model weights and tune only these additive parameters for downstream task adaptation, which usually injects far fewer trainable parameters compared to full fine-tuning and results in light-weight tuned adapters. LoRA serves as an outstanding representative of PEFT family and is now widely-used for different fine-tuning tasks. 2Figure 2: Compared to LoRA which proposes to add low-rank trainable matrices to pretrained weights, we study two types of spectral adapters: Spectral AdapterA considers additively tuning the top columns of singular vector matrices and Spectral AdapterR considers orthogonally rotating the top columns of singular vector matrices. Inspired by the parameter efficiency of LoRA and the close connection between matrix rank and its spectral representation, here we study two spectral fine-tuning mechanisms, both are completed via first carrying out Singular Value Decomposition (SVD) of pretrained model weights and then fine- tuning the top columns of singular vector matrices obtained via the SVD. More precisely, consider a pretrained weight matrix with its spectral representation of form W =USV T , we define additive spectral adapter as Spectral AdapterA(W) ∶=[U1 +AU U2]S[V1 +AV V2], and correspondingly the rotational version Spectral AdapterR(W) ∶=[U1RU U2]S[V1RV V2], where U1, V1 denote the top- r columns of U and V and U2, V2 denote the rest of the columns. A =(AU , AV ) consists of trainable matrices of shape same as (U1, V1) and R =(RU , RV ) consists of two trainable orthogonal matrices of shape r by r such that RT U RU =RT V RV =I. As we show in later sections, the orthogonality constraint is efficiently handled with the Cayley parameterization, see Section 4.3 for details. The proposed fine-tuning model architecture can be visualized from Figure 2. Here Spectral AdapterA more resembles LoRA as it is of additive form while Spectral AdapterR more resembles prior Orthogonal Fine-Tuning (OFT) method which we compare further in Section 4. To ensure zero initialization as often done for PEFT methods, we initialize AU and AV both at zero. For rotational spectral adapter, we initialize RU and RV as identity matrices. A more thorough literature review suggests that prior work considering tuning model weights’ spectral representation (FSGAN[ 47], SVDiff [ 15]) has been proposed for alleviating overfitting when fine-tuning different vision models. These methods only look at tuning the singular values of flattened CNN weights and thus have fixed amount of trainable parameters. Moreover, these methods require storing all U, Sand V during training while only the diagonal vector of S is tuned, which nearly doubles the storage requirement compared to pretraining when fine-tuning on downstream tasks. Contrarily, we consider incorporating spectral information in generic fine-tuning procedure for different layers (flattened CNN weights, dense linear weights, etc.) and our method enables flexible parameter budget choices by varying values of r. Methodology-wise, we consider tuning the top-r columns of U and V by additive and rotational tuning, both requiring only these top columns to be stored additionally and the left part can be merged into a single weight matrix. See Section 4.4 for more investigation on practicality of the proposed method. 3 Theoretical Insights After introducing the model architecture of spectral adapter we consider, the main question now remains whether tuning the spectral representation of pretrained weights is indeed an improvement over existing PEFT methods. Before we step into our empirical observations, we first provide some theoretical insights for the proposed spectral adaptation mechanism. In this section, we show advantage of our spectral adapter method compared to LoRA from two theoretic perspectives by 3analyzing both the rank capacity of the adapters (Section 3.1) and the subspace alignment of pretrained weight matrices (Section 3.2). Specifically, we will see that Spectral AdapterA has larger rank capacity than LoRA adapter, which indicates the tuned weight has more adaptation freedom and thus is more desirable. Moreover, the dominant spectral direction of pretrained weight matrix identifies more ideal neuron alignment under the setting we consider in Section 3.2, which justifies the robustness of tuning top singular vectors in our spectral adapter. In Appendix D, we show that Spectral AdapterA is approximately equivalent to DoRA [32] for vector-form weights. 3.1 Adapter Rank Capacity For any pretrained weight matrixW, suppose that the adapter is given by the parameterizationfθ(W) where θ represents trainable weights. For instance with LoRA adapter, fθ(W) =W +ABT , where θ ={A, B} is trainable. We define the rank capacity of an adapter fθ(W) as follows: R(fθ; W) ∶= max θ rank(fθ(W))−min θ rank(fθ(W)), which describes the range of matrix ranks the tuned weight can achieve given a specific adapter form. Then, the following lemma shows that Spectral AdapterA has twice the rank capacity of LoRA adapter under an equal number of trainable parameters. Lemma 3.1. Suppose that W ∈Rn×m is an arbitrary full row-rank matrix and n ≤m without loss of generality. Consider rank-r LoRA and rank-r additive spectral adapter, which have an equal number of trainable parameters. We have R(LoRA; W) =r, R(Spectral AdapterA; W) =2r. See Appendix B for proof. Therefore when pretrained model weight matrix is close to full row-rank, as what has been observed in [20], Spectral AdapterA has nearly double rank capacity compared to LoRA adapter. Furthermore, some prior work explicitly imposes low-rank constraint when training original NNs [50, 43, 66, 22, 68, 24, 9]. Using LoRA adapter to fine-tune such pretrained model weights would destroy their rank constraints while applying spectral adapter preserves the constraints. Next we proceed to show that top spectral space of pretrained weight matrices is more aligned with ideal neuron direction under a simple setting via subspace decomposition analysis of pretrained model weights. This observation corroborates our choice of tuning top singular vectors in our proposed spectral adaptation mechanism. Empirically, we observe that tuning top directions performs superior to tuning bottom ones, see Appendix F.3 and F.5.1 for related experiments. 3.2 Weight Subspace Alignment Figure 3: Top singu- lar vector of pretrained weight recognizes more ideal neuron direction. Il- lustration plot for Section 3.2. Consider two-layer ReLU network with m hidden nodes and univariate output. For squared loss objective, we can write out the training problem explicitly as min W(1),W(2) ∥(XW (1))+W(2) −y∥2 2 +β(∥W(1)∥2 F +∥W(2)∥2 2), where X ∈ Rn×d is the data matrix, (W(1) ∈ Rd×m, W(2) ∈ Rm) are first and second layer weights respectively and y ∈Rn is the label vector. For better visualization, we take d = 3. Consider the case that all data points lie on xy−plane, which mimics the usual observation that data points occupy a low-dimensional manifold. Then we can decompose each first layer neuron W(1) j ∈ Rd into W(1) j = wj1 +wj2 where wj1 ∈ R(X), wj2 ⊥ R(X). With simple algebra, for non-zero weight decay which is often the default setting for current deep learning optimizers, one can derive wj2 =0 and thus W(1) j =wj1 ∈R(X). Therefore all optimal neurons lie also in xy−plane. However, due to optimization errors, some of the trained neurons might be slightly deviated from xy−plane, as illustrated in Figure 3, where ui indicates pretrained neuron directions, though most of them lie in xy−plane, some might deviate (i.e., u4). u⋆ indicates the top singular vector direction of pretrained weight W(1) which here recognizes the xy−plane orientation, and thus fine-tuning u⋆ is noiseless and is expected to be more robust. 44 Empirical Results: The Impact of Spectral Information We experiment our proposed spectral adapter with fine-tuning large language models and diffusion models and compare against various recent PEFT methods. From language model experiments, we observe that Spectral Adapter A performs superior to various PEFT baselines and harnesses higher scores on different benchmarks, which again verifies the effectiveness of incorporating spectral information into the fine-tuning procedure, see Section 4.1 for details. For diffusion model experiments, we will see that the advantage of spectral adapter comes in two-fold: Spectral AdapterA offers a natural solution to existing problems in multi-adapter fusion procedure and Spectral AdapterR manifests finer-grained parameter budgets as well as better parameter efficiency, see Section 4.2 and 4.3 respectively. For a fair comparison with all baselines, we use their official implementation and follow hyperparameter setting in their original reports as long as available. See each individual section for corresponding experimental details. All experiments are done with NVIDIA RTX A6000 GPU. 4.1 Language Model Fine-Tuning: Enhancing Fine-Tuning Results with Spectral Adapter A For large language model experiments, we present experimental results for fine-tuning DeBERTaV3- base model (185M) and Mistral model (7B) on GLUE and GSM8K tasks respectively. Our Spectral AdapterA method achieves superior tuning results compared to various recent PEFT methods in most experiments. DeBERTaV3-base Experiment. Table 1 shows fine-tuning results of DeBERTaV3-base model on GLUE benchmarks with various PEFT methods. For a fair comparison, we use official implemen- tations for LoRA, DoRA, OFT and AdaLoRA in HuggingFace PEFT library, with hyperparameter setting for LoRA [20] and AdaLoRA [65] following their original reports. We use same hyperpa- rameter setting as LoRA for DoRA and follow the setting used in BOFT [33], a variant of OFT, for OFT experiments. We abbreviate Spectral AdapterA as SpectralA for presentation simplicity and we tune hyperparameters for Spectral AdapterA. See Appendix F.2 for hyperparameter details and F.3 for loss/validation plot comparison. We fine-tune all q, k, vmatrices in attention layers. Our Spectral AdapterA achieves highest average score and best scores for most tasks with fewest trainable parameters. Method # Param GLUE MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg. LoRAr=24 0.72% 88.87 95.06 87.00 65.84 91.87 91.45 81.22 90.43 86.47 DoRAr=24 0.73% 88.91 95.29 88.72 65.84 92.01 91.51 80.14 90.10 86.57 OFTr=4 0.72% 89.16 95.06 87.74 66.75 93.28 91.33 78.70 89.72 86.47 AdaLoRAr=24 1.07% 89.44 94.95 89.70 63.06 93.17 91.48 83.75 91.22 87.10 SpectralA r=24 0.72% 89.79 95.75 90.19 69.44 93.35 91.65 83.39 90.64 88.03 Table 1: Accuracy comparison of fine-tuning DeBERTaV3-base with various PEFT methods on GLUE benchmarks. SpectralA is abbreviation for Spectral AdapterA. See Section 4.1 for experimental details. Mistral 7B Experiment. We experiment our Spectral Adapter A with Mistral 7B model [23] fine-tuned for GSM8K task [ 7]. Since all baseline model reports include no fine- tuning tasks with the Mistral family, we use official implementations of all baseline meth- ods for comparison and we fix learning rate to be 2.5e − 5 for all methods following [ 51]. Method #Param GSM8K Pre-Trained − 37.91 ±1.34 LoRAr=8 0.16% 44.81 ±1.37 DoRAr=8 0.17% 43.82 ±1.37 SpectralA r=8 0.16% 49.73 ±1.38 Table 2: Accuracy comparison of fine-tuning Mis- tral 7B model with different PEFT methods on GSM8K benchmark. See Section 4.1 for experi- mental details. We take r = 8 for LoRA, DoRA and Spectral AdapterA to maintain approximately same num- ber of trainable parameters for all methods. Ta- ble 2 presents the accuracy comparison where SpectralA stands for Spectral Adapter A. From the result, we observe that our Spectral AdapterA scores higher than both LoRA and DoRA by a large margin and increases the pretrained model baseline significantly, which verifies the effective- ness of the proposed spectral adaptation mecha- nism. See Appendix F.4 for more about experi- mental details. Note for a different learning rate, DoRA performs better than LoRA while still worse than our method, see also Appendix F.4 for details. 54.2 Diffusion Model Fusion: Improving Multi-Object Fine-Tuning with Spectral Adapter A Figure 4: Distributing different concept tunings along different spectral space helps with identity preservation in multi-adapter fusion, see Section 4.2 for details. Multi-adapter fusion is a current bottleneck in diffusion model fine-tuning tasks with LoRA adapters. Simply adding different LoRA adapters tuned for distinct objects will result in problems involving identity loss and concept binding [12]. To tackle this toughness, different methods emerge such as Gradient Fusion [12] and Orthogonal Adaptation [42]. Specifically, Orthogonal Adaptation method proposes to fix LoRA parameter B to have orthogonal basis and train A solely. Experiments there show that merging LoRA weights with such orthogonal basis helps preserving individual object characteristics compared to its non-orthogonal counterpart. In Orthogonal Adaptation [ 42], the authors maintain B by manually keeping large orthogonal matrices for different layer sizes and sample r columns from corresponding orthogonal matrix to form B for each LoRA adapter. With knowledge from random matrix theory, such sampled matrices are likely to have orthogonal basis. Notably, our Spectral AdapterA naturally operates on orthogonal singular vectors and thus introduces an elegant solution to multi-adapter fusion problems by distributing different concept tunings along different columns of singular vector matrices, which maps to wireless communications where the signals are distributed over non-overlapping frequencies. A subtlety here lies in the choice of column space for different fine-tuning tasks: (1) Sample-based methods can be adopted if data privacy is considered and different tuning tasks are done independently. In Appendix F.5, we show that tuning top columns manifests better generation quality compared to both tuning bottom columns and sampling random orthogonal basis as what has been done in Orthogonal Adaptation [42]. Thus there is a trade-off between high-quality generation and concept collapsing, i.e., sampling from top singular vectors is more encouraged while column overlapping between concepts happens more often compared to sampling from the whole set. (2) On the other hand, if fine-tuning tasks are not isolated and can collaborate on the column scheduling, then more deliberate tuning scheduling can be adopted, for example in a two-concept tuning task with r =4, the first concept can allocate first to fourth columns and the second concept then claims fifth to eighth columns. Figure 4 demonstrates steps for the same method for three-concept tuning task. Since we expect fine-tuned weights to stay close to original weights, though both row space and column space are tuned in spectral adapter, this adaptation mechanism approximates orthogonal-basis tuning for different objects and thus we expect it helps improving identity preservation for multi-adapter fusion. In this section, we investigate this effect via extensive diffusion model experiments. Our experiments follow [42] and build on [12] which studies multi-LoRA fusion. We experiment with multi-object tuning and face generation tasks. Due to space limitation, we present some multi-object tuning results below and we leave the rest to Appendix F.5. For all tasks, we compare against baselines including Gradient Fusion [12], Orthogonal Adaptation [42], and FedAvg [37]. We start with a simple review for these baseline methods. Baseline Review To merge different LoRA adapters, say we have a set of LoRA parameters{∆θ1, . . . ,∆θn} where ∆θi = AiBT i and pretrained parameter θ0, FedAvg [ 37] proposes to merge them in to a single parameter by taking a weighted average as θmerged =θ0 +∑i λi∆θi, where λi is the weight attached to parameter ∆θi and is usually taken to satisfy ∑i λi = 1, i.e., θmerged is a convex combination of individual adapters. Gradient Fusion [12] instead considers solving an auxiliary optimization problem of form θmerged =argminθ ∑n i=1 ∥(θ0 +∆θi)Xi −θXi∥2 F where Xi represents the input activation of the i-th concept. Orthogonal Adaptation [42] follows FedAvg method and replaces original LoRA 6Figure 5: Generation results of Chilloutmix diffusion model [8] with different fused adapters tuned on three custom animal concepts. See Section 4.2 for details. parameters with orthogonal-based LoRA adapters. For our method, to merge different spectral adapters, let θ0 = U0S0V T 0 denote the spectral representation of pretrained model weight. Given a set of spectral adapters {(Ui, Vi), . . . ,(Un, Vn)} with zero-padding to make the shape the same as (U0, V0), we follow FedAvg and compute θmerged = (U0 +∑i λiUi)S0(V0 +∑i λiVi)T . In the following experiments, we take λi =1/n as in [42] for all FedAvg, Orthogonal Adaptation, and our Spectral AdapterA fusion. Notably, all FedAvg, Orthogonal Adaptation, and our Spectral AdapterA fusion can be done approximately instantly while Gradient Fusion usually takes around 10 ∼ 15 minutes for solving its auxiliary optimization problems for all concept adapters. Multi-Object Generation We follow default training setting in [ 12] and fine-tune the Chilloutmix diffusion model [ 8] on three custom animal concepts, see original animals in \"reference\" in Figure 5. For better spatial alignment, we adopt T2I-Adapter [39] with sketch condition and we set guidance equal to one, see also \"reference\" in Figure 5 for the sketch condition being used. LoRA rank r =8 is adopted. For baseline comparisons, we use original code for Gradient Fusion [ 12] and Orthogonal Adaptation [42]. We adapt code of Gradient Fusion for FedAvg method since there is no official implementation available. Custom animal name is replaced with special token < Vanimal> for fine-tuning. For our Spectral AdapterA, we follow the method depicted in Figure 4 and tune first, second, and third top eighth columns of singular vector matrices for different animal concepts. Figure 5 shows the generation results with different methods for selected prompts. Notably, baseline methods sometimes fail to capture the custom animal concepts while Spectral AdapterA recognizes all custom animals and generates visually satisfactory images. For better measurement, we also compute the alignment scores for each generated image with both reference images and prompt texts. It can be witnessed that our method achieves better alignment scores compared to baselines. See Appendix F.7 for details on alignment score computation. 4.3 Diffusion Model Expressiveness: Improving Parameter Efficiency with Spectral AdapterR Spectral AdapterR is closely connected to prior Orthogonal Fine-Tuning (OFT ) [45] method which proposes to multiply the pretrained model weights by trainable orthogonal matrices in the fine- tuning procedure. Motivation behind OFT is to preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. Unlike OFT which orthogonally rotates neurons, Spectral Adapter R multiplies the top- r columns of singular vector space U and V by orthogonal trainable matrices. For our implementation, several options are available for maintaining a trainable orthogonal matrix such as adding an orthogonality penalty in the objective function considered in [65] or via Cayley parameterization considered in [ 45]. We follow [ 45] and adopt Cayley parameterization which is supported by Pytorch [44]. Specifically, the orthogonal matrix R is 7constructed via R =(I +Q)(I −Q)−1 with a skew-symmetric matrix Q maintained as (A −AT )/2 where A is our trainable parameter. Compared to adding an auxiliary orthogonality penalty, this parametrization is exact and thus the SVD form is preserved after tuning with Spectral AdapterR and can be adopted directly for subsequent fine-tuning tasks, which we state formally as a lemma below: Lemma 4.1. With the Cayley parametrization, Spectral AdapterR is an exact rotation operation and thus preserves the structure of the SVD of the fine-tuned weight. Subsequent fine-tunings can be applied consequently without recomputing the SVD each time. See Appendix C for the proof of above lemma. Unlike LoRA which requires number of trainable parameters to scale with weight size, when tuning top-r columns of U an V , Spectral AdapterR only requires two trainable matrices of size r ×r and thus can be more parameter-efficient especially for large pretrained weight. For common weight size such as W ∈ R1024×1024, LoRA with only r = 1 introduces same number of trainable parameters as Spectral AdapterR with r =32. For a thorough analysis on parameter efficiency improvement brought by Spectral AdapterR, we here also compare with different variants of LoRA which are proposed for trainable parameter savings. We review all baselines in detail below. Baseline Review We compare our Spectral Adapter R with LoRA [ 20], SVDiff [ 15], LiDB [ 48], OFT [ 45], and VeRA [25]. Though the other methods are proposed for vision model tuning, VeRA is originally proposed for LLM tuning and we extend it here to diffusion model tuning due to its parameter efficiency. Consider a pretrained weight W ∈Rn×n, SVDiff originally proposes to tune all singular values of flattened CNN weights, here we extend it to tune all singular values of text encoder and U-Net weights for our comparison, thus trainable parameter attached to W will be of size n and is nonadjustable. LiDB stands for Lightweight Dreambooth and proposes to cut down trainable parameter budget by introducing auxiliary frozen matrixAaux ∈Rn×a and Baux ∈Rb×n, then it mimics LoRA but uses AauxABT Baux in replace of ABT with trainable (A ∈ Ra×r, B∈ Rb×r). Thus with a, b< n, LiDB requires (a +b)r < 2nr trainable parameters. In below, we use a = 50, b= 100 as default in [48]. OFT multiplies the weight matrix by a trainable orthogonal matrix via Cayley parametrization discussed above, thus its complete version requires n2 trainable parameters. For parameter efficiency, OFT proposes to use block-diagonal trainable matrix with all diagonal blocks being orthogonal. Thus with r diagonal blocks, the number of trainable parameter will be r ×(n/r)2. Method Granularity #Param Auxiliary Param LoRA / ∞ 2nr∝n noSVDiff / 1 n∝n noLiDB / ∞ (a+b)r∝r yes OFT / #factors ofn1 (n/r)2 ∝nr no VeRA / ∞ n+r∝n yes Spectral AdapterR , n 2r2 ∝r no 1 Ceiling operation is ignored for this count. Table 3: Baseline methods comparison for parameter effi- ciency. Granularity indicates number of trainable parameter budgets available. See Section 4.3 for details. Further reduction of trainable parame- ter is achieved via sharing the diagonal blocks, which demands only (n/r)2 parameters. In below comparison, we use this shared block-diagonal version for best parameter efficiency of OFT. VeRA proposes to use ΛaAΛbBT in replace of ABT where Λa and Λb are diagonal matrices of size n ×n and r ×r respectively. Thus the total num- ber of trainable parameters by VeRA is (n +r) ∝n. Table 3 compares dif- ferent properties across all methods, where n represents weight size and r represents rank for all methods except for OFT, where r denotes number of diagonal blocks. Parameter Efficiency We fine-tune the Chilloumix diffusion model [8] with various PEFT methods on custom vase concept and present the generation results for prompt \"a <Vvase> on a table\" in Figure 6 for various trainable parameter budgets, where grey dash denotes that the corresponding parameter budget is unobtainable with a given adapter no matter how the hyperparameter is chosen and empty entry without grey dash 8Figure 6: Generation results for prompt “a <Vvase> on a table” after fine-tuning Chilloutmix diffusion model [8] on custom vase images with different PEFT methods. See Section 4.3 for details. represents that there is a way to achieve the corresponding parameter budget though the generation result is skipped for better visualization. We follow default LoRA implementation in [12] for LoRA baseline and adjust it for all other methods. From Figure 6, it can be observed that LoRA, OFT, and LiDB start to generate vase close to custom vase with at least 200k trainable parameters. SVDiff and VeRA are unable to generate ideal vase images even if scaled to large parameter budget. On the contrary, Spectral AdapterR starts to recognize the custom vase concept with only 20k trainable parameters and has finer-grained parameter choices compared to other methods, i.e., notably Spectral AdapterR can have as few as1k parameters while other methods start with at least tens of thousands of trainable parameters. In a word, Spectral AdapterR enjoys finer-grained parameter budget choices and manifests better visual quality with fewer parameters, thus achieves enhanced parameter efficiency compared to various other PEFT methods. Figure 7: Generation results for prompt “a yellow <Vchair>” after fine-tuning Chilloutmix diffusion model [8] on custom chair images with different PEFT methods. Spectral R is abbreviation for Spectral AdapterR. See Section 4.3 for details. Figure 7 above presents generation results of Chilloutmix diffusion model [8] tuned on custom chair concept with different methods under various parameter budgets. The prompt used is \"a yellow <Vchair>\". See \"reference\" in Figure 7 for original chair images. From the generation results, it can be observed that LoRA generates reasonable chairs for all rank r =1, 2, 3 though it already induces 273k parameters even if rank is set to 1. OFT and VeRA start to recognize custom chair with >100k parameters. SVDiff has a single fixed trainable parameter budget of size around 100k. LiDB forms a competitive candidate and generates satisfactory images with smallest trainable parameter budget among all baseline methods. However, our Spectral AdapterR still generates images better aligned to 9reference images with as few as 20k trainable parameters and has finer-grained parameter budget choices compared to LiDB. See Appendix F.6 for hyperparameter setting and Appendix F.7 for alignment score computation details. 4.4 Final Note: A Closer Look at SVD Cost Figure 8: Runtime and GPU storage cost plot. See Section 4.4 for details. To alleviate the concerns with respect to online training cost and show that our pro- posed method is very practical, we provide runtime and GPU storage cost bar plot in Figure 8, which shows runtime and GPU storage cost for LoRA and for our Spec- tral AdapterA when used for fine-tuning diffusion model in Section 4.2 and Mistral 7B model in Section 4.1. Here we adopt rank r = 8 for both LoRA and Spectral AdapterA. It can be observed that our Spec- tral Adapter A introduces negligible run- time and storage overhead for current large model size. Modern numerical tools such as randomized SVD [13] can also be exploited for further runtime reduction and the SVD procedure can be paral- lelized when multiple machines are available. See Appendix E for further investigation. 5 Conclusion and Limitations In this work, we investigate the incorporation of spectral information of pretrained model weights into current PEFT models by introducing a spectral adaptation mechanism which updates only the top singular vectors of pretrained weights. We investigate the additive and rotational variants of such spectral adaptation mechanism. Theoretically, we show the motivation of tuning top singular vectors by comparing the rank capacity of different fine-tuning models and carrying out weight decomposition of pretrained model layers. Empirically, we verify the superiority of our proposed spectral adaptation method compared to various recent PEFT methods from different aspects via extensive experiments. To our best knowledge, this is the first work considering incorporating spectral information as a practical generic paradigm for fine-tuning tasks and enhances fine-tuning results, parameter efficiency, as well as benefits multi-adapter fusion of existing PEFT methods. For future work, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Other PEFT methods such as AdaLoRA [65] can also be dynamically combined with spectral adaptation. A limitation of the current work remains in the choice of tuning top spectral space. Though its validity has been theoretically verified under simple settings, further investigation on tuning different columns of singular vector matrices is critical to understanding the role of spectral information in fine-tuning procedure. Besides, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Moreover, the time consumption of singular value decomposition procedure increases as model grows larger and thus faster singular value decomposition method also benefits. 106 Acknowledgement This work was supported in part by the National Science Foundation (NSF) under Grant DMS- 2134248; in part by the NSF CAREER Award under Grant CCF-2236829; in part by the U.S. Army Research Office Early Career Award under Grant W911NF-21-1-0242; and in part by the Office of Naval Research under Grant N00014-24-1-2164. References [1] A. Aghajanyan, L. Zettlemoyer, and S. Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning, 2020. [2] A. Asai, M. Salehi, M. E. Peters, and H. Hajishirzi. Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts, 2022. [3] N. Cancedda. Spectral filters, dark signals, and attention sinks, 2024. [4] A. Chavan, Z. Liu, D. Gupta, E. Xing, and Z. Shen. One-for-all: Generalized lora for parameter- efficient fine-tuning, 2023. [5] Y . Chen, D. Hazarika, M. Namazifar, Y . Liu, D. Jin, and D. Hakkani-Tur. Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention. arXiv preprint arXiv:2205.03720, 2022. [6] A. Chronopoulou, M. E. Peters, A. Fraser, and J. Dodge. Adaptersoup: Weight averaging to improve generalization of pretrained language models, 2023. [7] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021. [8] C. M. Creator. Chilloutmix diffusion model. https://civitai.com/models/6424/chilloutmix. [9] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas. Predicting parameters in deep learning, 2014. [10] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023. [11] A. Edalati, M. Tahaei, I. Kobyzev, V . P. Nia, J. J. Clark, and M. Rezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter, 2022. [12] Y . Gu, X. Wang, J. Z. Wu, Y . Shi, C. Yunpeng, Z. Fan, W. Xiao, R. Zhao, S. Chang, W. Wu, Y . Ge, S. Ying, and M. Z. Shou. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. arXiv preprint arXiv:2305.18292, 2023. [13] N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions, 2010. [14] K. Hambardzumyan, H. Khachatrian, and J. May. Warp: Word-level adversarial reprogramming, 2021. [15] L. Han, Y . Li, H. Zhang, P. Milanfar, D. Metaxas, and F. Yang. Svdiff: Compact parameter space for diffusion fine-tuning, 2023. [16] S. Hayou, N. Ghosh, and B. Yu. Lora+: Efficient low rank adaptation of large models, 2024. [17] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig. Towards a unified view of parameter-efficient transfer learning, 2022. [18] S. He, R.-Z. Fan, L. Ding, L. Shen, T. Zhou, and D. Tao. Mera: Merging pretrained adapters for few-shot learning. arXiv preprint arXiv:2308.15982, 2023. [19] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. At- tariyan, and S. Gelly. Parameter-efficient transfer learning for nlp, 2019. 11[20] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models, 2021. [21] C. Huang, Q. Liu, B. Y . Lin, T. Pang, C. Du, and M. Lin. Lorahub: Efficient cross-task generalization via dynamic lora composition, 2024. [22] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014. [23] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023. [24] M. Khodak, N. Tenenholtz, L. Mackey, and N. Fusi. Initialization and regularization of factorized neural layers, 2022. [25] D. J. Kopiczko, T. Blankevoort, and Y . M. Asano. Vera: Vector-based random matrix adaptation, 2024. [26] T. Lei, J. Bai, S. Brahma, J. Ainslie, K. Lee, Y . Zhou, N. Du, V . Zhao, Y . Wu, B. Li, et al. Conditional adapters: Parameter-efficient transfer learning with fast inference. Advances in Neural Information Processing Systems, 36, 2024. [27] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning, 2021. [28] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021. [29] Y . Li, Y . Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and T. Zhao. Loftq: Lora-fine- tuning-aware quantization for large language models, 2023. [30] Z. Lin, A. Madotto, and P. Fung. Exploring versatile generative language model via parameter- efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020. [31] Q. Liu, X. Wu, X. Zhao, Y . Zhu, D. Xu, F. Tian, and Y . Zheng. Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications, 2023. [32] S.-Y . Liu, C.-Y . Wang, H. Yin, P. Molchanov, Y .-C. F. Wang, K.-T. Cheng, and M.-H. Chen. Dora: Weight-decomposed low-rank adaptation, 2024. [33] W. Liu, Z. Qiu, Y . Feng, Y . Xiu, Y . Xue, L. Yu, H. Feng, Z. Liu, J. Heo, S. Peng, Y . Wen, M. J. Black, A. Weller, and B. Schölkopf. Parameter-efficient orthogonal finetuning via butterfly factorization, 2023. [34] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang. Gpt understands, too, 2023. [35] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks, 2021. [36] C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning, 2018. [37] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication- efficient learning of deep networks from decentralized data, 2023. [38] A. Mitra, H. Khanpour, C. Rosset, and A. Awadallah. Orca-math: Unlocking the potential of slms in grade school math, 2024. [39] C. Mou, X. Wang, L. Xie, Y . Wu, J. Zhang, Z. Qi, Y . Shan, and X. Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models, 2023. [40] mrm8488. Lora finetune deberta-v3 huggingface blog, 2021. Available at https://huggingface.co/mrm8488/deberta-v3-small-finetuned-mnli/commits/main. [41] J. Pfeiffer, A. Kamath, A. Rücklé, K. Cho, and I. Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. 12[42] R. Po, G. Yang, K. Aberman, and G. Wetzstein. Orthogonal adaptation for modular customiza- tion of diffusion models, 2023. [43] D. Povey, G. Cheng, Y . Wang, K. Li, H. Xu, M. A. Yarmohammadi, and S. Khudanpur. Semi- orthogonal low-rank matrix factorization for deep neural networks. In Interspeech, 2018. [44] pytorch group. Pytorch orthogonal parameterization method implementation, 2023. [45] Z. Qiu, W. Liu, H. Feng, Y . Xue, Y . Feng, Z. Liu, D. Zhang, A. Weller, and B. Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning, 2023. [46] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision, 2021. [47] E. Robb, W.-S. Chu, A. Kumar, and J.-B. Huang. Few-shot adaptation of generative adversarial networks, 2020. [48] N. Ruiz, Y . Li, V . Jampani, W. Wei, T. Hou, Y . Pritch, N. Wadhwa, M. Rubinstein, and K. Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models, 2023. [49] A. Rücklé, G. Geigle, M. Glockner, T. Beck, J. Pfeiffer, N. Reimers, and I. Gurevych. Adapter- drop: On the efficiency of adapters in transformers, 2021. [50] T. N. Sainath, B. Kingsbury, V . Sindhwani, E. Arisoy, and B. Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6655–6659, 2013. [51] H. Skogström. Lora finetune mistral 7b valohai blog, 2024. https://valohai.com/blog/finetune- mistral/. [52] A. Tang, L. Shen, Y . Luo, Y . Zhan, H. Hu, B. Du, Y . Chen, and D. Tao. Parameter efficient multi-task model fusion with partial linearization, 2023. [53] K. Turgutlu. Answer.ai qdora report, 2024. https://www.answer.ai/posts/2024-04-26-fsdp-qdora- llama3.html. [54] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi. Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation, 2023. [55] T. Vu, B. Lester, N. Constant, R. Al-Rfou, and D. Cer. Spot: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021. [56] Z. Wang, R. Panda, L. Karlinsky, R. Feris, H. Sun, and Y . Kim. Multitask prompt tuning enables parameter-efficient transfer learning, 2023. [57] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks, 2024. [58] L. Xu, H. Xie, S.-Z. J. Qin, X. Tao, and F. L. Wang. Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment, 2023. [59] Y . Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, Z. Chen, X. Zhang, and Q. Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models, 2023. [60] A. X. Yang, M. Robeyns, X. Wang, and L. Aitchison. Bayesian low-rank adaptation for large language models, 2024. [61] F. Zhang and M. Pilanci. Riemannian preconditioned lora for fine-tuning foundation models, 2024. [62] F. F. Zhang, L. Li, J.-C. Chen, Z. Jiang, B. Wang, and Y . Qian. Increlora: Incremental parameter allocation method for parameter-efficient fine-tuning. ArXiv, abs/2308.12043, 2023. 13[63] L. Zhang, L. Zhang, S. Shi, X. Chu, and B. Li. Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning, 2023. [64] M. Zhang, H. Chen, C. Shen, Z. Yang, L. Ou, X. Yu, and B. Zhuang. Loraprune: Pruning meets low-rank parameter-efficient fine-tuning, 2023. [65] Q. Zhang, M. Chen, A. Bukharin, N. Karampatziakis, P. He, Y . Cheng, W. Chen, and T. Zhao. Adalora: Adaptive budget allocation for parameter-efficient fine-tuning, 2023. [66] Y . Zhang, E. Chuangsuwanich, and J. Glass. Extracting deep neural network bottleneck features using low-rank matrix factorization. In2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 185–189. IEEE, 2014. [67] H. Zhao, H. Tan, and H. Mei. Tiny-attention adapter: Contexts are more important than the number of parameters, 2022. [68] Y . Zhao, J. Li, and Y . Gong. Low-rank plus diagonal adaptation for deep neural networks. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5005–5009. IEEE, 2016. [69] Y . Zhu, J. Feng, C. Zhao, M. Wang, and L. Li. Counter-interference adapter for multilingual machine translation, 2021. [70] B. Zi, X. Qi, L. Wang, J. Wang, K.-F. Wong, and L. Zhang. Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices, 2023. 14Appendix A Prior Work Here we provide an overview of recent PEFT methods. Dating back to 2019, Houlsby et al. [ 19] develop the idea of parameter-efficient fine-tuning and introduce Adapter model, which injects trainable components between pretrained model layers, though the number of trainable parameters has been reduced due to the small size of adapters, this method incurs inference latency and is thus not desirable. Later improvement of Adapter fine-tuning focuses on improving inference latency [49, 26], fusing multiple adapters [6, 41, 18], modifying adapter model architecture [67], introducing parallelism [17, 69], and creating task-specific and layer-specific adapter [ 35, 30]. Another line of fine-tuning is prompt-tuning [27] which usually adds the trainable components into the prompt. Variants of prompt-tuning involve WARP [14], prefix-tuning [28], P-tuning [34], and ATTEMPT [2] which consider injecting different forms of trainable components. Multitask prompt-tuning is considered in [55, 56]. The more relevant PEFT methods to our spectral adaptation mechanism involves LoRA [20] and OFT [45], which inspires our Spectral AdapterA and Spectral AdapterR respectively. LoRA originates from the observation that model fine-tuning is intrinsically low-rank [1]. Variants of LoRA involve different methods proposing dynamic allocation of LoRA rank budgets [54, 62, 65, 5]. LoRA has been combined with model pruning [64] and quantization [10, 59, 29]. Some other variants further cut down the trainable parameter budget or activation storage by modifying LoRA model [25, 11, 63]. DoRA [32] fixes LoRA’s low-rank limitation by decomposing pretrained model weights and isolating their magnitudes. Laplace-LoRA [ 60] incorporates Bayesian inference into LoRA parameters to improve calibration. LoRAHub [21], MOELoRA [31], and L-LoRA [52] consider multitask LoRA. Delta-LoRA [70] updates pretrained weights simultaneously from information of LoRA parameters. GLoRA [4] generalizes LoRA by introducing a prompt module. Another line of variants focuses on analyzing the optimization scheme of LoRA model [ 61, 16]. OFT studies the multiplicative fine-tuning and its variant BOFT [33] improves OFT by utilizing butterfly parametrization for better information delivery efficiency. [58] offers a comprehensive review of recent development of PEFT methods. B Rank Capacity Proof Proof. Consider weight matrix W ∈ Rn×m with n ≤ m of full row rank. For LoRA parameter A ∈Rm×r, B∈Rn×r with n ≥r, final weight matrix W +ABT has rank in [n −r, n]. With Spectral AdapterA parameters AS ∈ Rm×r, BS ∈ Rn×r where n ≥ 2r. Let Xr denote the first r columns of any matrix X and X−r denote the rest columns, final weight matrix ((Ur +AS)Sr(Vr +BS)T )+ U−rS−rV T −r has rank in [n−2r, n]. Therefore, R(LoRA; W) =r and R(Spectral AdapterA; W) = 2r can be derived trivially. C Cayley Parameterization Proof Proof. With any trainable square matrix A, we set Q = (A −AT )/2 and thus Q = −QT and Q is skew-symmetric thereby. Now we show that for any skew-symmetric Q, (I +Q)(I −Q)−1 is orthogonal. Let O =(I +Q)(I −Q)−1, then OT O =((I +Q)(I −Q)−1)T (I +Q)(I −Q)−1 =(I −QT )−1(I +QT )(I +Q)(I −Q)−1 by Q skew-symmetric, =(I +Q)−1(I −Q)(I +Q)(I −Q)−1 since (I −Q) and (I +Q) have same eigen-basis and are commutable, =I, which shows that the Cayley parametrization is exact and no re-SVD is needed for orthogonality preservation. 15D Connection to DoRA In DoRA [32], the authors observe that plain LoRA method tends to either increase or decrease the magnitude and direction updates proportionally and thus lacks ability to make slight direction change together with large magnitude change, to come across this limitation, the authors propose to decompose pretrained model weights into magnitude and direction and update them separately. The magnitude is replaced with a trainable scalar and the direction is updated with original LoRA method. Experiments in [32] show that such decomposition helps improve effectiveness of LoRA significantly. Here we show that our Spectral AdapterA is closely connected to the weight decomposition trick used in DoRA when pretrained model weight is of vector form. We note that in DoRA, after the weight decomposition, each column becomes unit-length while in Spectral AdapterA, we also operates on matrices with unit-length columns. Specifically, consider a pretrained model weight w0 ∈Rn×1, then DoRA becomes w =w w0 +ba ∥w0 +ba∥2 , where w is a trainable scalar initialized at ∥w0∥2. band a are trainable parameters of size n ×1 and 1 ×1 respectively, with ba =0 at initialization. Comparably, Spectral AdapterA becomes w =( w0 ∥w0∥2 +a′)∥w0∥2(1 +b′), with trainable vectora′ ∈Rn×1 and trainable scalarb′ both initialized at zero. We can thus equivalently view ∥w0∥2(1 +b′) as a single trainable scalar initialized at ∥w0∥2, which then plays the role of magnitude adapter as w in DoRA. a′ is adopted for directional adaptation since it directly operates on the normalized base vector. E Cost Investigation (More Detailed) Here we address the potential concern about the overhead of our proposed spectral adaptation mechanism. Firstly, we note that spectral adapter introduces similar number of trainable parameters and can be merged into original model weights, thus it is lightweight for sharing and introduces no additional inference latency, which preserves the strengths of additive fine-tuning methods. Therefore, the major overhead concern exists in the runtime and GPU storage overhead during online training. Note our method involves only matrix multiplication in the forward procedure and thus should run as quick as LoRA. Though the SVD procedure can bring additional runtime overhead, it needs to be done only once for a single model and can be reused for later fine-tuning on various downstream tasks. Besides, modern numerical tools such as randomized SVD [ 13] can also be exploited and the SVD procedure can be parallelized when multiple machines are available. As for GPU storage, unlike SVDiff [15] where all SVD components are required for training procedure thus introducing significant GPU storage burden, our method requires only the top spectral space to be stored additionally and consumes similar GPU storage to LoRA for relatively small tuning ranks (which is usually the case). F Supplemental Materials for Experiments F.1 Experimental Setup for Figure 1 For Figure 1 experiments, we follow QDoRA [53] experimental setup for fine-tuning Llama3 8B model, where all k_proj, q_proj, v_proj, up_proj, down_proj, and gate_proj weights are tuned. We adopt the same data processing method and train on 10K Orca Math data (shuffled) as in [53]. We fix learning rate as 1e −5 for all methods as in QDoRA and train for one epoch with batch size 8. r =8 is adopted for LoRA, DoRA, AdaLoRA, and Spectral AdapterA while for OFT, we set number of diagonal blocks to be 800 to maintain similar amount of trainable parameters. LoRA alpha is set to be 16 following DoRA [32] convention and AdaLoRA hyperparameter is set following what has been used for MNLI benchmark in the original AdaLoRA report [65] with regularization set to 1e −3 which we find works better. For evaluation, we test on GSM8K [7] benchmark for exact matching. For more comparisons, Figure 9 provides training loss for smaller rank r = 4 (oft_r = 1600) and larger rank r =64 (oft_r =95). All settings are the same except that LoRA alpha is always kept as 16Figure 9: More experiments with Llama3 8B model with different number of trainable parameters. In the left plot, the training loss of LoRA and DoRA overlaps. See Appendix F.1 for details. twice as rank number. From Figure 9 we can observe that though increasing trainable parameters closes the gap between different tuning methods, our spectral adapter method is always superior to other PEFT methods and stays closest to full fine-tuning. F.2 Hyperparameter Setting for DeBERTaV3-base Experiment (Section 4.1) Dataset learning rate batch size #epochs optimizer weight decay MNLI 1e −4 32 1 AdamW 0.01 RTE 3e −4 32 10 AdamW 0.01 QNLI 1e −4 32 1 AdamW 0.01 MRPC 7e −4 32 13 AdamW 0.01 QQP 1e −4 32 10 AdamW 0.01 SST-2 1e −4 32 5 AdamW 0.01 CoLA 3e −4 32 8 AdamW 0.01 STS-B 5e −4 32 30 AdamW 0.01 Table 4: Hyperparameters for DeBERTaV3-base model fine-tuning with Spectral AdapterA in Section 4.1 Table 4 shows the hyperparameter setting for our Spectral AdapterA used for fine-tuning DeBERTaV3- base model in Section 4.1. We set number of diagonal blocks to be 4 and enable block sharing for OFT to maintain similar amount of trainable parameters. F.3 More About DeBERTaV3-base Experiment Left plot in Figure 10 presents the training loss and validation score comparisons of LoRA, SVDiff and our Spectral AdapterA for fine-tuning DeBERTaV3-base model on CoLA benchmark. We set learning rates for both LoRA and Spectral AdapterA as what has been used in popular public blog [40] for LoRA fine-tuning with DeBERTaV3-base model, which is not tuned in favor of our method. For SVDiff, since it is originally proposed for vision model tuning, we extend it to this experiment by tuning all singular values of pretrained weights. We find the same learning rate leads to poor fine-tuning results with SVDiff, we thus pick the best learning rate among [1e −3, 1e −4, 1e −5] according to validation performance and set learning rate to be 1e −3. We use r = 8 for LoRA and Spectral AdapterA. From Figure 10, it can be observed that Spectral AdapterA achieves better training and validation performance compared to both LoRA and SVDiff. Interestingly, in LoRA [20], the authors provide a correlation analysis between the LoRA additive component △W = ABT and original pretrained weight matrix W (see Section H.3 in [ 20]), and they find that the additive component does not contain the top singular directions of W. The authors therefore conclude that the learned LoRA component amplifies \"task-specific\" directions which are not emphasized in the pretrained weight matrix. Naively, this seems to suggest that tuning top singular subspace of pretrained weights is not ideal and one should identify the desired \"task-specific\" directions to improve LoRA. Here we show that this is not the case and fine-tuning top directions provides a significant improvement to LoRA. In the right plot of Figure 10 above, we experiment 17Figure 10: Left plot presents training loss and validation results for fine-tuning DeBERTaV3-base model with LoRA, SVDiff, and Spectral AdapterA on CoLA benchmark. Right plot compares the same statistics between LoRA and spectral adapter with top ranks and bottom ranks tuned respectively. tuning the top eighth rank and the bottom eighth rank of singular vector space in our Spectral AdapterA, which we present as \"Spectral Top\" and \"Spectral Bottom\" respectively. Remarkably, \"Spectral Top\" converges faster and scores higher than LoRA, which is then superior to \"Spectral Bottom\". This result unravels the fact that tuning different part of spectral space brings different tuning effect and tuning the top columns of singular vector space improves LoRA tuning significantly. See Section 3 for more theoretic insights. F.4 Hyperparameter Setting for Mistral 7B Experiment (Section 4.1) Method lr lora alpha batch size #epochs lora dropout weight decay LoRA 2.5e −5 16 4 2 0.05 0.01 DoRA 2.5e −5 16 4 2 0.05 0.01 Spectral AdapterA 2.5e −5 - 4 2 - 0.01 Table 5: Hyperparameters for Mistral 7B model fine-tuning task in Section 4.1 Table 5 shows training hyperparameter setting for fine-tuning Mistral 7B model in Section 4.1. We train with bfloat16 precision and fine-tune all q_proj, k_proj, v_proj, o_proj, and gate_proj weights. We evaluate with lm-evaluation-harness [47]. Table 6 shows accuracy comparison of different tuning methods with learning rate 1e −5. Our Spectral AdapterA still exceeds both LoRA and DoRA. F.5 Supplemental Materials for Multi-Adapter Fusion Experiment (Section 4.2) F.5.1 Comparison of Single Object Generation We present more experimental results to show that Spectral AdapterA with top ranks tuned behaves at least as good as LoRA with same parameter budget and is better than Orthogonal Adaptation [42], which is likely due to that Orthogonal Adaptation fixes LoRA parameter B and thus has limited expressiveness. We also show that tuning bottom ranks in spectral adapter behaves worse than all other methods. Figure 11 shows generation results for custom toy concept tuning, where Orthogonal Adaptation and Spectral AdapterA (bottom) generate inaccurate happy-face octopus, sad-face octopus, and green tortoise. Figure 12 shows generation results for custom animal concept tuning, where Orthogonal Adaptation and Spectral AdapterA (bottom) sometimes miss first dog concept. Method #Param GSM8K Pre-Trained − 38.82 LoRAr=8 0.16% 43.29 ±1.36 DoRAr=8 0.17% 43.52 ±1.37 SpectralA r=8 0.16% 46.47 ±1.37 Table 6: Supplemental experiments of fine-tuning Mistral 7B model with different PEFT methods with a different learning rate on GSM8K benchmark. See Section F.4 for experimental details. 18Figure 11: Generation results for single toy concept tuning with LoRA, Orthogonal Adaptation, and Spectral AdapterA with top and bottom ranks tuned respectively. F.5.2 More Multi-Adapter Fusion Generation Results Here we present more results for multi-adapter fusion generation. Figure 13 shows generation results for multi-object generation for custom toy concepts and Figure 14 presents generation results for multi-character generation for three computer scientists. See below for experimental details. Multi-Object Generation. As in Section 4.2, we fine-tune Chilloutmix diffusion model [8] on four custom toy concepts, see \"reference\" in Figure 13 for original toy images. We use r =8 for all methods and tune first, second, third, and fourth top eighth columns of singular vector space of pretrained weights for first, second, third, and fourth toys in our Spectral AdapterA. We follow all default experimental settings in [ 12] and tune all embedding layer, U-Net, and text-encoder. For better spatial alignment, we employ T2I-Adapter with sketch condition listed in \"reference\" in Figure 13. We randomly select three scenes and prompt fused-adapters for the results, see \"prompts\" in Figure 13 for individual prompt being used. From Figure 13, it can be observed that FedAvg and Orthogonal Adaptation generate unsatisfactory happy-face octopus and green tortoise toys. On the contrary, our spectral adapter generates high-quality images similar to Gradient Fusion while saving 19Figure 12: Generation results for single animal concept tuning with LoRA, Orthogonal Adaptation, and Spectral AdapterA with top and bottom ranks tuned respectively. Figure 13: Generation results of Chilloutmix diffusion model [8] tuned on four custom toy concepts with different fused adapters. See Appendix F.5.2 for details. much more time. Multi-Character Generation. We also experiment fine-tuning Chilloutmix diffusion model [ 8] with photos of three computer scientists Yoshua Bengio, Yann LeCun, and Geoffrey Hinton. As in multi-object generation, we use r = 8 for all methods and tune first, second, and third top eighth columns of singular vector space of pretrained weights for Bengio, Lecun, and Hinton in our Spectral AdapterA. We use T2I-Adapter [ 39] with keypose condition. See \"reference\" in Figure 14 for scientists’ photos and keypose condition being used. Figure 14 shows generation results for prompt 20\"<Vbengio> and <Vlecun> and <Vhinton>, standing near a lake, 4K, high quality, high resolution\" with different fused adapters, from which it can be observed that our spectral adapter generates picture of most consistent styles across characters and renders all scientists’ faces clearly. Figure 14: Generation results of Chilloutmix diffusion model [8] tuned on photos of three computer scientists with different fused adapters. See Appendix F.5.2 for details. F.6 Supplemental Materials for Parameter Efficiency Experiment (Section 4.3) In this section, we present more tuning results with various parameter budgets for parameter efficiency experiment studied in Section 4.3, see Section 4.3 for baseline method explanation. Table 7 shows the learning rates used for each baseline method and Table 8 shows learning rates used for our method, the rest experimental settings are default as in [12]. Method text encoder lr unet lr LoRA 1e −5 1e −4 VeRA (r =1) 1e −3 1e −4 VeRA (r =1024, 4096) 5e −3 1e −4 OFTA 1e −5 1e −4 LiDB 5e −4 1e −4 SVDiff 1e −3 1e −4 Table 7: Hyperparameters for baseline methods for diffusion model fine-tuning task in Section 4.3 Method vase chair table text unet text unet text unet Spectral AdapterR (r =2, 40) 1e −3 1e −2 1e −2 1e −2 1e −3 1e −2 Spectral AdapterR (r =4) 5e −3 5e −3 1e −3 1e −2 Spectral AdapterR (r =8) 5e −4 5e −2 1e −3 1e −2 1e −3 1e −2 Spectral AdapterR (r =16) 1e −2 1e −3 1e −3 1e −2 Spectral AdapterR (r =24) 1e −4 1e −2 1e −3 1e −3 1e −4 1e −2 Spectral AdapterR (r =32) 1e −4 5e −2 Table 8: Hyperparameters for Spectral AdapterR for diffusion model fine-tuning task in Section 4.3 Figure 15 shows generation results of Chilloutmix diffusion model [8] fine-tuned on custom table concept with different methods under various parameter budgets. The prompt used is “a <Vtable>”. LoRA generates acceptable images for all rank r =1, 2, 3 though it starts with 273k parameters even if rank is set to 1. OFT generates desirable images only for parameter budget > 400k. VeRA and LiDB start to generate reasonable images with >300k trainable parameters and SVDiff has only a single fixed parameter budget. Meanwhile, our Spectral AdapterR recognizes the shape of custom table with as few as 6k parameters and produces ideal images since 100k parameters. See Appendix F.7 for alignment score computation details. 21Figure 15: Generation results for prompt “a <Vtable>” after fine-tuning Chilloutmix diffusion model [8] on custom table images with different PEFT methods. Spectral R is abbreviation for Spectral AdapterR. See Appendix F.6 for details. F.7 Alignment Score Computation For better quantitative measurement, we compute alignment scores for our Figure 5,6,7,15 results. Specifically, we first compute CLIP [46] embedding for all generated/reference images and prompt texts, then we compute the cosine similarity between generated images’ embedding and reference images’ embedding to serve as their alignment score. Likewise, text score stands for cosine similarity between generated images’ embeddings and their corresponding prompt texts’ embeddings. Intuition here is that if an image is close to another image (or text), their CLIP vectors are expected to stay close as well. For Figure 5 alignment score computation, we crop each generated image vertically into three columns, then we compute their alignment scores to each corresponding reference animal, we finally take the mean of these three scores. For Figure 6, 7, 15 scores, we compute average score over three random trials, with each trial consisting of 8 generated images. 22",
      "meta_data": {
        "arxiv_id": "2405.13952v2",
        "authors": [
          "Fangzhao Zhang",
          "Mert Pilanci"
        ],
        "published_date": "2024-05-22T19:36:55Z",
        "pdf_url": "https://arxiv.org/pdf/2405.13952v2.pdf"
      }
    },
    {
      "title": "Parameter-Efficient Fine-Tuning Design Spaces",
      "abstract": "Parameter-efficient fine-tuning aims to achieve performance comparable to\nfine-tuning, using fewer trainable parameters. Several strategies (e.g.,\nAdapters, prefix tuning, BitFit, and LoRA) have been proposed. However, their\ndesigns are hand-crafted separately, and it remains unclear whether certain\ndesign patterns exist for parameter-efficient fine-tuning. Thus, we present a\nparameter-efficient fine-tuning design paradigm and discover design patterns\nthat are applicable to different experimental settings. Instead of focusing on\ndesigning another individual tuning strategy, we introduce parameter-efficient\nfine-tuning design spaces that parameterize tuning structures and tuning\nstrategies. Specifically, any design space is characterized by four components:\nlayer grouping, trainable parameter allocation, tunable groups, and strategy\nassignment. Starting from an initial design space, we progressively refine the\nspace based on the model quality of each design choice and make greedy\nselection at each stage over these four components. We discover the following\ndesign patterns: (i) group layers in a spindle pattern; (ii) allocate the\nnumber of trainable parameters to layers uniformly; (iii) tune all the groups;\n(iv) assign proper tuning strategies to different groups. These design patterns\nresult in new parameter-efficient fine-tuning methods. We show experimentally\nthat these methods consistently and significantly outperform investigated\nparameter-efficient fine-tuning strategies across different backbone models and\ndifferent tasks in natural language processing.",
      "full_text": "PARAMETER -EFFICIENT FINE -TUNING DESIGN SPACES Jiaao Chen†∗, Aston Zhang‡, Xingjian Shi‡, Mu Li‡, Alex Smola‡, Diyi Yang⋄ †Georgia Institute of Technology,‡Amazon Web Services, ⋄Stanford University ABSTRACT Parameter-efﬁcient ﬁne-tuning aims to achieve performance comparable to ﬁne-tuning, using fewer trainable parameters. Several strategies (e.g., Adapters, preﬁx tuning, BitFit, and LoRA) have been proposed. However, their designs are hand-crafted separately, and it remains unclear whether cer- tain design patterns exist for parameter-efﬁcient ﬁne-tuning. Thus, we present a parameter-efﬁcient ﬁne-tuning design paradigm and discover design patterns that are applicable to different experi- mental settings. Instead of focusing on designing another individual tuning strategy, we introduce parameter-efﬁcient ﬁne-tuning design spaces that parameterize tuning structures and tuning strate- gies. Speciﬁcally, any design space is characterized by four components: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Starting from an initial design space, we progressively reﬁne the space based on the model quality of each design choice and make greedy selection at each stage over these four components. We discover the following design patterns: (i) group layers in a spindle pattern; (ii) allocate the number of trainable parameters to layers uni- formly; (iii) tune all the groups; (iv) assign proper tuning strategies to different groups. These design patterns result in new parameter-efﬁcient ﬁne-tuning methods. We show experimentally that these methods consistently and signiﬁcantly outperform investigated parameter-efﬁcient ﬁne-tuning strategies across different backbone models and different tasks in natural language processing1. 1 Introduction Large pretrained models have achieved the state-of-the-art performances across a wide variety of downstream natural language processing tasks through ﬁne-tuning on task-speciﬁc labeled data [Devlin et al., 2019, Liu et al., 2019, Yang et al., 2019, Joshi et al., 2019, Sun et al., 2019, Clark et al., 2019, Lewis et al., 2020a, Bao et al., 2020, He et al., 2020, Raffel et al., 2020, Ziems et al., 2022]. However, ﬁne-tuning all the parameters and storing them separately for different tasks is expensive in terms of computation and storage overhead (e.g., 355M parameters for RoBERTa [Liu et al., 2019] and 175B parameters for GPT- 3 [Brown et al., 2020]). This makes it difﬁcult to deploy in real-world natural language processing (NLP) systems composed of multiple tasks. To adapt general knowledge in pretrained models to speciﬁc down-stream tasks in a more parameter-efﬁcient way, various strategies have been proposed where only a small number of (extra) parameters are learned while the remaining pretrained parameters are frozen [Houlsby et al., 2019a, Pfeiffer et al., 2021, Li and Liang, 2021, Brown et al., 2020, Lester et al., 2021a, Schick and Sch ¨utze, 2021, Ziems et al., 2022]. Adapter tuning [Houlsby et al., 2019a] is among the earliest strategies to steer pretrained models with a limited number of parameters. It inserts adapters (small neural modules) to each layer of the pretrained network and only the adapters are trained at the ﬁne-tuning time. Inspired by the success of prompting methods that control pretrained language models through textual prompts [Brown et al., 2020], preﬁx tuning [Li and Liang, 2021] and prompt tuning [Lester et al., 2021b] prepend additional tunable tokens to the input or hidden layers and only train these soft prompts when ﬁne-tuning on downstream tasks. BitFit [Zaken et al., 2021] updates the bias terms in pretrained models while freezing the remaining parameters. LoRA [Hu et al., 2021] decomposes attention weight gradients into low-rank matrices to reduce the number of trainable parameters. With promising results from such research, He et al. [2022] proposed a uniﬁed view of these existing strategies and ∗Work done during an internship at Amazon Web Services. Correspondence to Jiaao Chen<jiaaochen@gatech.edu> and Aston Zhang <astonz@amazon.com>. 1Code is available at: https://github.com/amazon-science/peft-design-spaces . arXiv:2301.01821v1  [cs.CL]  4 Jan 2023P P P L P L A B L A B L… Layer Grouping P L Strategy Assignment Trainable Parameter Allocation Tunable Groups p ⇥ p Figure 1: A parameter-efﬁcient ﬁne-tuning design space. It is characterized by (i) layer grouping (how to group consecutive layers), (ii) trainable parameter allocation (how to allocate the number of trainable parameters to layers), (iii) tunable groups (which groups will be ﬁnetuned), and (iv) strategy assignment (how to assign proper strategies, such as among Adapter, Preﬁx, BitFit, and LoRA, to groups). illustrated differences and connections among them. Like its antecedents, the resulting method is stillequally assigned to different pretrained layers. Despite being effective, most parameter-efﬁcient ﬁne-tuning strategies have been developed via manual design pro- cesses, without much consideration of whether design patterns exist across these different strategies and how such patterns might apply to different backbone models and downstream tasks. Moreover, different strategies are usually applied separately; thus, it is unclear which strategy works best when and where [Mao et al., 2022], as well as how these different strategies reinforce or complement each other. In this light, our goal is to understand the parameter- efﬁcient ﬁne-tuning design in a more comprehensive view and discover design patterns that are both interpretable and applicable across different experimental settings. Instead of designing yet another individual strategy that is equally applied to different pretrained layers, we introduce parameter-efﬁcient ﬁne-tuning design spaces that parameterize both tuning structures and strategies. More con- cretely, any of these design spaces is characterized by four major components as shown in Figure 1: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Starting from a relatively unconstrained parameter-efﬁcient ﬁne-tuning design space, we progressively reﬁne the space by comparing the overall quality of models randomly sampled from design spaces enforced with different constraints (e.g., each group has the same number of layers). Throughout the experimental process, we discover several design patterns for parameter-efﬁcient ﬁne-tuning, such as group layers in a spindle pattern, allocate the number of trainable parameters to layers uniformly, tune all the groups, and assign proper tuning strategies to different groups. We fur- ther introduce new parameter-efﬁcient ﬁne-tuning methods that adopt all these discovered design patterns. Extensive experiments show that our methods consistently outperform investigated parameter-efﬁcient ﬁne-tuning strategies. Al- though we use T5 [Raffel et al., 2020] and classiﬁcation tasks as the working example, we ﬁnd that our methods with all these discovered design patters are applicable to other backbones (e.g., RoBERTa [Liu et al., 2019], BART [Lewis et al., 2020b], and XLNet [Yang et al., 2019]) and different natural language processing tasks (e.g., summarization, machine translation, and eight SuperGLUE datasets). Our contributions can be summarized as follows: (i) We introduce parameter-efﬁcient ﬁne-tuning design spaces. (ii) Based on these design spaces, we discover several design patterns in parameter-efﬁcient ﬁne-tuning via comprehen- sive experiments. (iii) Our discovered design patterns lead to parameter-efﬁcient ﬁne-tuning methods, consistently outperforming investigated parameter-efﬁcient ﬁne-tuning strategies across different backbone models and different NLP tasks. 22 Related Work Our work is closely related to and built upon the research about the network design spaces and parameter-efﬁcient ﬁne-tuning. We discuss the connections and differences below. Network Design Spaces A lot of works designed neural network models via an ad-hoc discovery of new design choices that improve performances [Radosavovic et al., 2019], such as the use of deeper architectures or residuals. Recently, there have been works [Radosavovic et al., 2020, You et al., 2020, Radosavovic et al., 2019] performing at the design space level to discover new design principles for convolutional neural networks [Radosavovic et al., 2020] and graph neural networks [You et al., 2020]. Inspired by this line of research, we focus on the design space perspective to rethink parameter-efﬁcient ﬁne-tuning, with the goal of discovering design patterns that are applicable to different experimental settings. Parameter-Efﬁcient Fine-Tuning for NLP As pretrained models grow in size, storing ﬁne-tuned models becomes exceedingly expensive, and ﬁne-tuning becomes infeasible for those without extremely high compute resources. A growing body of research has been devoted to ﬁnding parameter-efﬁcient alternatives for adapting large-scale pre- trained models with reduced memory and storage costs. Houlsby et al. [2019b] proposed to adapt large models using bottleneck layers (with skip-connections) between each layer. This idea has been extended in many domains [Stick- land and Murray, 2019, Pfeiffer et al., 2020, Rebufﬁ et al., 2017, Lin et al., 2020]. Other works have aimed to avoid introducing additional parameters by identifying and training only a subset of all model parameters [Zhao et al., 2020, Guo et al., 2020, Mallya et al., 2018, Radiya-Dixit and Wang, 2020, Sung et al., 2021, Zaken et al., 2021]. Recent works also explored the idea of rank decomposition based on parameterized hypercomplex multiplications via the Kro- necker product [Zhang et al., 2021a] and injecting trainable rank decomposition matrices into each layer [Hu et al., 2021, Karimi Mahabadi et al., 2021]. Li and Liang [2021] introduced preﬁx-tuning that prepends a set of preﬁxes to autoregressive language models or prepends preﬁxes for both encoders and decoders. The preﬁx parameters are updated while the pretrained parameters are ﬁxed. Lester et al. [2021a] proposed a similar method, but only added virtual tokens at the embedding layer of large-scale models rather than discrete prompts [Deng et al., 2022, Zhong et al., 2022]. Bari et al. [2022] proposed semi-parametric prompt tuning that converges more easily, where memory prompts are input-adaptive without the need for tuning. Recently, He et al. [2022] and Ding et al. [2022] proposed a uniﬁed view of the existing parameter-efﬁcient ﬁne-tuning strategies and illustrated the difference and connections among them. Mao et al. [2022] also introduced a uniﬁed framework to combine different methods through mixture- of-experts. In contrast to these aforementioned works that assign their individual method equally to different pretrained layers, we focus on more general design spaces of parameter-efﬁcient ﬁne-tuning. This could provide a more comprehensive view of parameter-efﬁcient ﬁne-tuning in terms of both the tuning structures and tuning strategies. Through experiments where we progressively reﬁne design spaces, we discover design patterns for parameter-efﬁcient ﬁne-tuning. 3 Components of Design Spaces When deﬁning design spaces of parameter-efﬁcient ﬁne-tuning, we aim to cover key design components and provide a representative set of choices in each design component. Note that our goal is not to enumerate all possible design spaces, but to demonstrate how the use of design spaces can help inform parameter-efﬁcient ﬁne-tuning research. Concretely, in our work, the parameter-efﬁcient ﬁne-tuning design spaces are formed by a representative set of choices in parameter-efﬁcient ﬁne-tuning, which consists of the following four components: (i) layer grouping, (ii) trainable parameter allocation, (iii) tunable groups, and (iv) strategy assignment. Following the illustrated design space exam- ple in Figure 1, we describe these four design components in detail below and will explore their design choices in Section 4. Layer Grouping Different layers in pretrained models capture different information and behave differently. For example, Jawahar et al. [2019] found that the {3, 4, 5, 6, 7, 9, 12}-th layers have the most representation power in BERT and every layer captures a different type of information ranging from the surface, syntactic, to the semantic level representation of text. For instance, the 9th layer has predictive power for semantic tasks such as checking random swapping of coordinated clausal conjuncts, while the 3rd layer performs best in surface tasks like predicting sentence length. Therefore when adapting these pretrained models to downstream tasks, how to group layers with similar behaviors together is critical to the design and application of proper parameter-efﬁcient ﬁne-tuning strategies. For this design component, we study the patterns of how to group consecutive layers in pretrained models (e.g., transformer layers in T5) during the ﬁne-tuning process. 3Trainable Parameter Allocation In parameter-efﬁcient ﬁne-tuning, the total number of trainable parameters is usually preset, such as a small portion of the total number of parameters in the pretrained models. We will study different design choices for how to allocate a predeﬁned number of trainable parameters to layers. Tunable Groups Zaken et al. [2021] found that not all the parameters need to be tuned during ﬁne-tuning on the downstream tasks. For instance, BitFit [Zaken et al., 2021] only updates the bias parameters in pretrained models while freezing the remaining parameters. Thus, we study which groups need to be learned during parameter-efﬁcient ﬁne-tuning to attain better performances. Strategy Assignment In order to improve the parameter efﬁciency, different sets of strategies [Li and Liang, 2021, Lester et al., 2021a, Houlsby et al., 2019a, Hu et al., 2021] have been proposed where only a small number of (ex- tra) parameters are tuned and the remaining parameters in these pretrained models are frozen to adapt their general knowledge to speciﬁc down-stream tasks. Inspired by effectiveness of offering architectural ﬂexibility [Zhang et al., 2021a,b], we hypothesize that different groups might beneﬁt from different proper strategies (or combinations) for capturing different types of information. More formally, given a set of individual strategies Afor assignment, for any group Gi, assign a subset Ui ⊂A to each layer in Gi. 4 Discovering Design Patterns Building on these four different design components of PEFT design spaces, we will start from a relatively uncon- strained design space and progressively discover the design patterns. 4.1 Design Space Experimental Setup We ﬁrst describe our experimental setup for discovering the design patterns. Note that our process is generic for other tasks and future pretrained backbone models. Datasets Our process for discovering design patterns of PEFT is based on the average performances on the widely- used GLUE benchmark [Wang et al., 2018]. It covers a wide range of natural language understanding tasks. First, single-sentence tasks include (i) Stanford Sentiment Treebank (SST-2) and (ii) Corpus of Linguistic Acceptability (CoLA). Second, similarity and paraphrase tasks include (i) Quora Question Pairs (QQP), (ii) Semantic Textual Sim- ilarity Benchmark (STS-B), and (iii) Microsoft Research Paraphrase Corpus (MRPC). Third, inference tasks include (i) Multi-Genre Natural Language Inference (MNLI), (ii) Question Natural Language Inference (QNLI), and (iii) Rec- ognizing Textual Entailment (RTE). To compare performances, the Matthews correlation is measured for CoLA; the Spearman correlation is used for STS-B, and accuracy is measured for the rest GLUE tasks. Pretrained Backbone Models and Model Settings We use T5-base/3b [Raffel et al., 2020] as the main pretrained backbone models for discovering design patterns via our PEFT design spaces. We use Hugging Face 2 for our imple- mentations and follow the default settings. During the exploration, we set the total number of trainable parameters (in the percentage of that in the backbone model) to 0.5% by following He et al. [2022]. 4.2 Discovering Design Patterns Using T5-base In this subsection, we describe the empirical process for discovering the design patterns using T5-base (pretrained backbone model) as the working example. Each PEFT design space (denoted as Si) consists of a set of models ( Si- models) that satisfy constraints characterizing the space with respect to layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. To discover design patterns, we start from a relatively unconstrained PEFT design space ( S0). Then we progressively reﬁne design spaces (from S0 to S1:4) by comparing overall quality of models in design spaces enforced with different constraints (e.g., each group has the same number of layers). To quantify the overall quality of models in any design space Si with a low-compute, low-epoch regime [Radosavovic et al., 2020], we randomly sample 100 models from Si, ﬁne-tune with 3 epochs 3, and compute the average of the GLUE average performances. 2https://huggingface.co/docs/transformers/index 3We set the low epoch by observing whether it is enough for models to obtain stable performances to draw consistent conclusions (See Table 7 in the Appendix). 4We emphasize that our goal is to demonstrate how the perspective of design spaces can help inform PEFT research, rather than to ﬁnd out the “best” design space or method. For computational efﬁciency, it is beyond the scope of this work to enumerate all possible constraints with respect to the design space components (Section 3). 4.2.1 The Initial S0 Design Space The initial relatively unconstrained design space S0 consists of all models without constraints on the design space components (Section 3). Individual PEFT strategies consist of Adapter, Preﬁx, BitFit, and LoRA. One can think of this S0 design space as a set of random models ( S0-models) with random design patterns. Speciﬁcally, without grouping constraints, each layer of the pretrained layer has a half chance to be tuned: if tuned, random strategies (or combinations) with a random amount of trainable parameters are assigned to that layer. Before comparing more subtle design patterns such as how to properly assign tunable strategies among Adapter, Preﬁx, BitFit, and LoRA, we begin with exploring how to group layers and how to allocate the total number of trainable parameters to layers. 4.2.2 The S1 Design Space with Additional Grouping Constraints Inspired by Radosavovic et al. [2020], we also consider 4 groups (G1, . . . , G4, in the order of forward pass) in the experiments 4. Denote by Ni the number of layers in Gi. As illustrated in Figure 2, we compare the following layer grouping patterns: (i) Increasing (Ni+1 > Ni): the number of layers in groups gradually increases; (ii) Uniform (Ni+1 = Ni): the number of layers in groups is the same; (iii) Decreasing (Ni+1 < Ni): the number of layers in groups gradually decreases; (iv) Spindle (N1 < N2 = N3 > N4): the numbers of layers in groups at both ends are smaller; and (v) Bottleneck (N1 > N2 = N3 < N4): the numbers of layers in groups at both ends are bigger. Figure 2: Layer grouping patterns, where the horizontal and vertical axes represent groups (G1, . . . , G4) and numbers of layers in groups. These layer grouping patterns lead to 5 different design spaces. Any of these 5 design spaces consists of all models in the S0 design space that satisfy one of these grouping pattern constraints. To compare the overall model qualities of different design spaces, we (i) randomly sample 100 models from the S0 design space that satisfy each grouping pattern constraint (Figure 2); (ii) ﬁne-tune with 3 epochs; and (iii) compute the average performances for each design space. We will follow this procedure as we progressively add new constraints later. The averaged performances are shown in Table 1 5. We ﬁnd that models from the design space with the spindle grouping pattern (Figure 2) consistently outperform those from the other design spaces across all the 8 GLUE tasks. This may be due to the complexities of information captured in different layers of large pretrained models, which favor information adaptation in the discovered layer grouping pattern. From now on, we will group layers in a spindle pattern. We refer to S0 with this additional design pattern as the new S1 design space. 4.2.3 The S2 Design Space with Additional Parameter Constraints We continue to explore design patterns in trainable parameter allocation to reﬁne the S1 design space. Denote by ni the number of trainable parameters for the i-th layer of the pretrained backbone model, we compare the following design patterns: (i) Increasing (ni+1 ≥ni): the number of trainable parameters in every layer gradually increases (or remains the same); (ii) Uniform (ni+1 = ni): the number of trainable parameters in every layer is the same; and (iii) Decreasing (ni+1 ≤ni): the number of trainable parameters in every layer gradually decreases (or remains the same). Following the procedure described in Section 4.2.2, we obtain 100 models for each of these 3 new design spaces. Table 2 reports the average performances of these 3 design spaces. The uniform allocation design pattern obtains the highest GLUE average performance, making this relatively simple, interpretable design pattern favorable. 4The experimental results with 8 groups are shown in the Table 16 in the Appendix. 5The training time for the step is shown in the Table 18 in the Appendix. 5Table 1: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different layer grouping constraints to the S0 design space. Layer Grouping SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg S0-models 76.9 70.1 72.5 73.3 63.6 71.7 73.8 24.3 65.7 Increasing 85.3 74.9 77.2 77.5 66.8 76.2 76.0 33.0 70.8 Uniform 84.8 73.7 78.1 78.6 68.5 77.8 79.2 36.1 72.1 Decreasing 81.9 72.1 78.3 76.7 67.3 75.9 78.6 28.7 70.0 Spindle 86.9 75.5 79.8 79.4 69.8 78.3 80.1 37.3 73.3 Bottleneck 84.5 74.6 76.9 78.1 69.2 76.2 78.6 32.1 71.3 Table 2: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different parameter allocation constraints to the S1 design space. Param Allocation SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg Increasing 87.2 77.9 79.4 78.7 71.6 77.6 81.4 32.0 73.2 Uniform 87.8 77.4 80.1 80.5 73.9 78.1 80.4 34.3 74.0 Decreasing 86.4 75.8 78.4 77.0 70.4 77.1 78.7 35.8 72.4 We will allocate the number of trainable parameters to layers uniformly. We refer to S1 with this additional design pattern as the new S2 design space. 4.2.4 The S3 Design Space with Additional Tunable Group Constraints Before digging into the strategy assignment design patterns, it is necessary to examine which groups need to be tuned. After all, it is only meaningful to study assigning strategies to different groups after we ﬁnd out which groups need to be ﬁne-tuned. As shown in Table 3, we explore various design patterns in tunable groups to further constrain the S2 design space. Based on the GLUE average performances, we ﬁnd that all the groups need to be tuned to obtain the best performances. This suggests that all the groups of pretrained layers have captured useful information that should be adapted to the downstream tasks. We will tune all the groups. We refer to S2 with this additional design pattern as the new S3 design space. 4.2.5 The S4 Design Space with Additional Strategy Constraints Finally, we study the subtle design pattern with respect to assigning proper strategies by further constraining the derived S3 design space. Speciﬁcally, each design space consists of models that assign a subset of {Adapter (A), Preﬁx (P), BitFit (B), and LoRA (L) }to all layers of any group Gi (i = 1, . . . ,4). We begin by adding different G1 strategy assignment constraints to the S3 space. Following the same pattern discovery procedure (Section 4.2.2), we discover strategy assignment patterns for G1. Then we progressively add Gi (i >1) strategy assignment constraints together with the discovered strategy assignment patterns for all Gj (j = 1, . . . , i−1) to the S3 space. Due to space limit, we present results of this process in the Appendix ( G1 in Table 8, G2 Table 9, G3 in Table 10, and G4 in Table 11), which suggests strategy assignment ofG1-(A, L) – G2-(A, P) – G3-(A, P, B) –G4-(P, B, L) for the T5-base pretrained backbone model. We will assign the discovered proper tuning strategies to groups.We refer to S3 with this additional design pattern as the new S4 design space, which consists of the ﬁnal S4-model. 4.3 Discovering Design Patterns Using T5-3b We then repeat the above process on T5-3b to examine if the design patterns we discovered using smaller models (T5- base) still apply when we use larger models. The results are shown in Table 12 (layer grouping), Table 13 (trainable parameter allocation), Table 14 (tunable groups) and Table 15 (strategy assignment) in the Appendix. We observe that the design patterns still apply when larger models like T5-3b are used: (i) grouping layers in a spindle pattern (Table 12), (ii) uniformly allocating the number of trainable parameters to layers (Table 13), (iii) tuning all the groups 6Table 3: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different tunable group constraints to the S2 design space. Tunable Groups SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1 82.6 72.1 77.6 70.6 65.3 71.9 77.6 27.6 68.2 G2 83.3 72.8 77.5 72.8 63.6 72.8 77.5 27.5 68.4 G3 83.6 73.3 78.2 73.3 66.4 71.3 77.9 22.9 68.4 G4 83.2 73.0 77.9 73.7 63.9 72.0 77.9 27.9 68.7 G1, G2 83.5 73.2 78.0 75.4 67.7 73.2 78.0 28.0 69.6 G3, G4 87.8 74.6 78.3 76.9 68.6 74.3 78.3 28.3 70.7 G1, G2, G3 86.0 75.8 79.0 77.8 71.8 78.8 79.0 33.0 72.6 G2, G3, G4 85.2 76.6 79.1 78.6 70.1 77.6 79.1 31.9 72.2 G1,G2,G3,G4 88.3 77.4 82.1 81.5 74.9 79.4 81.4 34.3 74.9 Table 4: Performances of different tuning methods on the GLUE datasets using the T5-base (upper part) and T5-3b (lower part) pretrained backbone models, respectively. The results are averaged over 20 random runs (with standard deviations as subscripts). The S4-model and the S4-3b-model perform signiﬁcantly better than the second-best PEFT methods in all the eight datasets at the signiﬁcance level p <0.05(∗) or even p <0.01(∗∗). Method SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Average full 95.2 87.1 93.7 89.4 80.1 89.4 90.7 51.1 84.5 Adapter 94.6 85.5 89.8 86.7 75.3 86.7 89.1 59.2 83.3 Preﬁx 94.0 81.6 87.8 83.4 64.3 83.1 84.8 34.0 76.6 BitFit 94.4 84.5 90.6 88.3 74.3 86.6 90.1 57.7 83.3 LoRA 94.8 84.7 91.6 88.5 75.8 86.3 88.7 51.5 82.7 S4-model 95.5∗∗ 1.7 87.6∗∗ 1.0 92.7∗∗ 1.1 88.8∗∗ 1.0 80.4∗ 2.3 87.4∗ 2.0 91.2∗∗ 2.4 62.2∗ 3.2 85.7 full 97.4 91.4 96.3 89.7 91.1 90.6 92.5 67.1 89.5 Adapter 96.3 89.9 94.7 87.8 83.4 90 89.7 65.2 87.1 Preﬁx 96.3 82.8 88.9 85.5 78.3 83.5 85.4 42.7 80.4 BitFit 95.8 89.5 93.5 88.5 86.2 90.7 88.6 64.2 87.1 LoRA 96.2 90.6 94.9 89.1 91.2 91.1 91.1 67.4 88.9 S4-3b-model 97.2∗∗ 1.8 91.6∗∗ 1.2 96.6∗∗ 1.0 89.5∗∗ 1.5 91.5∗ 2.8 91.5∗ 2.5 91.9∗ 2.0 69.7∗ 3.4 89.9 (Table 14), and (iv) tuning different groups with proper strategies (Table 15). For T5-3b, the discovered proper strategy assignment is G1-(P, L) –G2-(A, L) – G3-(P, B, L) –G4-(A, P, B). We refer to the ﬁnal design space asS4-3b and the ﬁnal model in this space as S4-3b-model. 5 Evaluation The S4-model (Section 4.2.5) and S4-3b-model (Section 4.3) adopt all the design patterns that have been discovered by using T5-base and T5-3b, respectively. As a result, they are both new methods of PEFT. We will evaluate their effectiveness when applied to different pretrained backbone models and different NLP tasks. 5.1 Experimental Setup Datasets Besides the GLUE datasets [Wang et al., 2018] (Section 4.1), we further evaluate our methods on two generation tasks used by He et al. [2022]: (i) Abstractive Summarization using XSum [Narayan et al., 2018], and (ii) Machine Translation using the WMT 2016 en-ro dataset [Bojar et al., 2016]. We report ROUGE scores [Lin, 2004] on the XSum test set, and BLEU scores [Papineni et al., 2002] on the en-ro test set. Models and Model Settings We mainly compare our methods with the following baselines: (i) Full Fine-tuning (full): it ﬁne-tunes all the model parameters in the pretrained models; (ii) Adapter [Houlsby et al., 2019a]: it adds adapter modules to each transformer layer; (iii) Preﬁx [Li and Liang, 2021]: it optimizes a set of small continuous vectors prepended to transformer layers; (iv) BitFit [Zaken et al., 2021]: it only updates the bias terms in pretrained models; (v) LoRA [Hu et al., 2021]: it decomposes the attention weight into low-rank matrices to reduce the number of trainable parameters. Besides T5 [Raffel et al., 2020], we additionally apply our methods to other backbone models 7Table 5: Performances of different tuning methods on GLUE datasets using the RoBERTa-base (upper part) and RoBERTa-large (lower part) pretrained backbone models. The results are averaged over 20 random runs (with standard deviations as subscripts). Here we also include two baselines: (i) S0-model, where all the designs are randomly selected for RoBERTa as in the S0 design space; (ii) S3-model, where strategies are randomly assigned to different RoBERTa layer groups as in the S3 design space. The S4-model and S4-3b-model perform signiﬁcantly better than the second-best PEFT methods in all the eight datasets at the signiﬁcance level p <0.05(∗) or even p <0.01(∗∗). Method SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Average full 94.8 87.6 92.8 91.9 80.8 90.3 90.2 63.6 86.5 Adapter 94.2 87.1 93.1 90.2 71.5 89.7 88.5 60.8 84.4 Preﬁx 94.0 86.8 91.3 90.5 74.5 90.3 88.2 61.5 84.6 BitFit 93.7 84.8 91.3 84.5 77.8 90.8 90.0 61.8 84.3 LoRA 94.9 87.5 93.1 90.8 83.1 90.0 89.6 62.6 86.4 S0-model 94.2 95.3 90.4 90.6 75.6 89.6 88.0 60.9 85.6 S3-model 94.3 87.2 92.8 91.0 81.8 90.3 89.2 63.2 86.2 S4-model 94.81.6 87.8∗∗ 0.8 93.4∗∗ 1.3 91.6∗ 1.2 85.8∗∗ 1.8 90.4∗ 2.0 90.0∗∗ 1.8 63.2∗ 3.5 87.1 full 96.4 90.2 94.7 92.2 86.6 92.4 90.9 68.0 88.9 Adapter 96.6 90.5 94.8 91.7 80.1 92.1 90.9 67.8 88.1 Preﬁx 95.7 87.6 92.1 88.7 82.3 89.6 87.4 62.8 85.7 BitFit 96.1 88.0 93.4 90.2 86.2 90.9 92.7 64.2 87.7 LoRA 96.2 90.6 94.7 91.6 87.4 92.0 89.7 68.2 88.8 S0-model 95.5 86.5 92.3 89.8 84.6 89.2 86.3 61.2 85.6 S3-model 96.3 89.4 93.8 90.2 85.9 90.8 90.9 63.4 87.6 S4-3b-model 96.6∗∗ 1.3 90.8∗ 1.1 95.1∗∗ 0.8 92.0∗∗ 1.2 87.22.8 92.3∗ 2.2 91.8∗∗ 1.8 68.4∗ 3.2 89.3 including RoBERTa-base/large [Liu et al., 2019] and BART-base/large [Lewis et al., 2020a]. We use the default settings. We set the total number of trainable parameters (in the percentage of that in the backbone model) by following He et al. [2022]. Speciﬁcally, this value is set to 0.5% for Adapter, Preﬁx, LoRA, and our methods, and 0.1% for BitFit. For all the experiments, we followed Liu et al. [2019] to set the linear decay scheduler with a warmup ratio of 0.06 for training. The batch size was 128 for base models and 64 for large models. The maximum learning rate was 5e −5 and the maximum number of training epochs was set to be either 5 or 10. All the experiments were performed using 8 A100 GPUs. 5.2 Effectiveness on GLUE with T5 Backbones Table 6: Performances of different tuning methods on generation tasks (XSUM and en-ro) using the BART-base (upper part) and BART-large (lower part) pretrained backbone models. Method XSUM(R-1/2/L) en-ro (BLEU) full 40.5/19.2/34.8 34.5 Adapter 37.7/17.9/33.1 33.3 Preﬁx 38.2/18.4/32.4 33.8 BitFit 37.2/17.5/31.4 33.2 LoRA 38.9/18.6/33.5 33.6 PA 39.3/18.7/33.8 33.8 S4-model 40.2/19.3/34.2 34.1 full 45.1/22.3/37.2 37.9 Adapter 43.8/20.8/35.7 35.3 Preﬁx 43.4/20.4/35.5 35.6 BitFit 42.8/18.7/33.2 35.2 LoRA 42.9/19.4/34.8 35.8 PA 43.9/20.6/35.6 36.4 S4-3b-model 44.3/21.7/36.8 37.2 With our discovered design patterns, we ﬁne-tune T5-base (S4-model) and T5-3b ( S4-3b-model) on GLUE and compare them with all the baseline methods. The results are shown in Table 4, where the key measure is the GLUE average performance (last column). We ﬁnd that our S4-model and S4- 3b-model consistently outperform the investigated methods in the key measure. By tuning only 0.5% parameters, our methods even outperform the full ﬁne-tuning baseline where all the parameters are tuned, indicating the effectiveness of our discov- ered PEFT design patterns. 5.3 General Effectiveness on GLUE with RoBERTa Backbones We directly apply the S4-model and S4-3b-model (adopting design patterns discovered using T5- base and T5-3b) to ﬁne-tune the RoBERTa-base and RoBERTa-large pretrained backbone models (with no extra discovery process), respectively. We keep all the other settings the same and evaluate them on GLUE datasets. We also compare with variant methods randomly sampled from two de- 8sign spaces: (i) S0-model, where all the designs are randomly selected for RoBERTa as in S0; (ii) S3-model, where strategies are randomly assigned to different RoBERTa layer groups as in S3. Table 5 shows that (i) the design pat- terns (adopted by S4-model and S4-3b-model) discovered using T5 models are applicable to the RoBERTa backbone models and outperform the investigated methods in GLUE average performances with no extra discovery process;(ii) improved performances fromS0-models, S3-models, to S4-(3b)-models support adding more constraints in the pattern discovery process (Section 4). 5.4 General Effectiveness on Generation Tasks with BART Backbones Like in Section 5.3, we further directly apply the S4-model and S4-3b-model (adopting design patterns discovered using T5-base and T5-3b) to ﬁne-tune the BART-base and BART-large pretrained backbone models (without additional discovery process.), respectively. We evaluate the models on two generation tasks: summarization (XSUM) and machine translation (en-ro) following He et al. [2022]. We also compare with PA (parallel adapter) using the same number of trainable parameters [He et al., 2022]. Table 6 shows that our methods, although adopting design patterns discovered from classiﬁcation tasks using T5, still outperform investigated PEFT strategies on generation tasks with different BART backbones. 6 Conclusion PEFT adapts knowledge in pretrained models to down-stream tasks in a more parameter-efﬁcient fashion. Instead of focusing on designing another strategy in the ﬁrst place, we introduced PEFT design spaces. We empirically discovered several design patterns in PEFT. These design patterns led to new PEFT methods. Experiments showed that these methods consistently outperform investigated PEFT strategies across different backbone models and different tasks in natural language processing. References Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional trans- formers for language understanding. In NAACL-HLT, 2019. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. InAdvances in neural information processing systems, pages 5754–5764, 2019. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics , 8:64–77, 2019. Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223, 2019. Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations, 2019. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, transla- tion, and comprehension. SCL, 2020a. Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Songhao Piao, Jianfeng Gao, Ming Zhou, et al. Unilmv2: Pseudo-masked language models for uniﬁed language model pre-training. arXiv preprint arXiv:2002.12804, 2020. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer, 2020. Caleb Ziems, Jiaao Chen, Camille Harris, Jessica Anderson, and Diyi Yang. V ALUE: Understanding dialect disparity in NLU. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 91: Long Papers) , pages 3701–3720, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.258. URL https://aclanthology.org/2022.acl-long.258. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning , vol- ume 97 of Proceedings of Machine Learning Research , pages 2790–2799. PMLR, 09–15 Jun 2019a. URL http://proceedings.mlr.press/v97/houlsby19a.html. Jonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. AdapterFusion: Non- destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pages 487–503, Online, April 2021. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2021.eacl-main.39. Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation, 2021. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning, 2021a. Timo Schick and Hinrich Sch ¨utze. Exploiting cloze-questions for few-shot text classiﬁcation and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pages 255–269, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.20. URL https://aclanthology.org/2021.eacl-main.20. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 3045–3059, Online and Punta Cana, Dominican Republic, November 2021b. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main.243. Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning for transformer- based masked language-models, 2021. URL https://arxiv.org/abs/2106.10199. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv.org/abs/2106.09685. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a uniﬁed view of parameter-efﬁcient transfer learning. In International Conference on Learning Representations, 2022. Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott Yih, and Madian Khabsa. UniPELT: A uniﬁed framework for parameter-efﬁcient language model tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 6253–6264, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.433. URL https: //aclanthology.org/2022.acl-long.433. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoy- anov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language genera- tion, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Com- putational Linguistics , pages 7871–7880, Online, July 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://www.aclweb.org/anthology/2020.acl-main.703. Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Doll ´ar. On network design spaces for visual recognition, 2019. URL https://arxiv.org/abs/1905.13214. Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll ´ar. Designing network design spaces, 2020. URL https://arxiv.org/abs/2003.13678. Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks, 2020. URL https://arxiv. org/abs/2011.08843. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR, 2019b. Asa Cooper Stickland and Iain Murray. Bert and pals: Projected attention layers for efﬁcient adaptation in multi-task learning. In International Conference on Machine Learning, pages 5986–5995. PMLR, 2019. 10Jonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non- destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. arXiv preprint arXiv:1705.08045, 2017. Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter- efﬁcient transfer learning. arXiv preprint arXiv:2004.03829, 2020. Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Sch ¨utze. Masking as an efﬁcient alternative to ﬁnetuning for pretrained language models. arXiv preprint arXiv:2004.12406, 2020. Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-efﬁcient transfer learning with diff pruning. arXiv preprint arXiv:2012.07463, 2020. Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In Proceedings of the European Conference on Computer Vision (ECCV), pages 67–82, 2018. Evani Radiya-Dixit and Xin Wang. How ﬁne can ﬁne-tuning be? learning efﬁcient language models. In International Conference on Artiﬁcial Intelligence and Statistics, pages 2435–2443. PMLR, 2020. Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neural networks with ﬁxed sparse masks. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Pro- cessing Systems, volume 34, pages 24193–24205. Curran Associates, Inc., 2021. URL https://proceedings. neurips.cc/paper/2021/file/cb2653f548f8709598e8b5156738cc51-Paper.pdf. Aston Zhang, Yi Tay, Shuai Zhang, Alvin Chan, Anh Tuan Luu, Siu Hui, and Jie Fu. Beyond fully-connected lay- ers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters. In International Conference on Learning Representations, 2021a. Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient low-rank hypercomplex adapter layers. Advances in Neural Information Processing Systems, 34:1022–1035, 2021. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P. Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning, 2022. URL https: //arxiv.org/abs/2205.12548. Wanjun Zhong, Yifan Gao, Ning Ding, Zhiyuan Liu, Ming Zhou, Jiahai Wang, Jian Yin, and Nan Duan. Improving task generalization via uniﬁed schema prompt, 2022. URL https://arxiv.org/abs/2208.03229. M Saiful Bari, Aston Zhang, Shuai Zheng, Xingjian Shi, Yi Zhu, Shaﬁq Joty, and Mu Li. Spt: Semi-parametric prompt tuning for multitask prompted learning. arXiv preprint arXiv:2212.10929, 2022. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong Sun. Delta tuning: A comprehensive study of parameter efﬁcient methods for pre-trained language models, 2022. URL https://arxiv.org/abs/2203.06904. Ganesh Jawahar, Benoˆıt Sagot, and Djam´e Seddah. What does BERT learn about the structure of language? In Pro- ceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651–3657, Florence, Italy, July 2019. Association for Computational Linguistics. Aston Zhang, Yi Tay, Yikang Shen, Alvin Chan, and Shuai Zhang. Self-instantiated recurrent units with dynamic soft recursion. Advances in Neural Information Processing Systems, 34:6503–6514, 2021b. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In BlackboxNLP@EMNLP, 2018. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary! topic-aware convo- lutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1206. URL https://aclanthology.org/D18-1206. Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Ji- meno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aur ´elie N ´ev´eol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Mar- cos Zampieri. Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 131–198, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-2301. URL https://aclanthology.org/W16-2301. 11Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //aclanthology.org/W04-1013. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002. 12A More Experimental Results Table 7: Average performances (low-compute, low-epoch regime: 100 random models, tuning epochs = 1, 2, 3, 4, 20 for ﬁve different blocks) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different grouping constraints to the S0 design space. Grouping Patterns SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg 1 epochs Increasing 73.2 63.3 67.8 68.8 63.8 67.2 64.1 11.0 59.9 Uniform 72.8 64.1 63.4 63.4 62.5 69.8 65.8 12.1 59.2 Decreasing 72.4 63.2 65.1 69.8 59.3 62.7 63.6 18.7 59.4 Spindle 72.6 64.8 66.8 71.1 62.1 62.3 64.8 12.3 59.6 Bottleneck 72.2 63.7 65.3 68.3 61.2 63.2 66.6 12.1 59.0 2 epochs Increasing 76.2 69.3 73.2 76.5 65.8 72.2 74.0 21.0 66.0 Uniform 74.8 70.9 74.1 75.6 66.5 73.4 71.2 22.1 66.1 Decreasing 71.4 70.1 72.1 76.8 64.3 71.7 73.6 18.7 64.8 Spindle 76.6 71.9 71.8 74.4 67.5 73.5 71.8 22.3 66.2 Bottleneck 74.2 71.1 69.6 73.3 65.2 73.3 73.6 24.1 65.5 3 epochs Increasing 85.3 74.9 77.2 77.5 66.8 76.2 76.0 33.0 70.8 Uniform 84.8 73.7 78.1 78.6 68.5 77.8 79.2 36.1 72.1 Decreasing 81.9 72.1 78.3 76.7 67.3 75.9 78.6 28.7 69.9 Spindle 86.9 75.5 79.8 79.4 69.8 78.3 80.1 47.3 74.6 Bottleneck 84.5 74.6 76.9 78.1 69.2 76.2 78.6 32.1 71.3 4 epochs Increasing 88.3 78.5 80.2 80.5 70.8 80.2 80.0 37.0 74.4 Uniform 88.8 78.9 81.9 81.5 71.5 80.8 81.4 39.1 75.4 Decreasing 87.6 74.1 80.8 81.7 79.3 78.9 79.6 38.7 75.1 Spindle 89.6 79.8 83.6 82.8 71.8 81.3 82.1 39.3 76.3 Bottleneck 86.5 77.6 82.7 81.1 70.2 70.9 81.6 36.1 73.3 20 epochs Increasing 92.3 83.3 86.2 82.5 71.8 82.2 84.0 51.0 79.1 Uniform 92.8 83.9 86.1 83.6 72.5 83.8 84.2 52.1 79.9 Decreasing 91.4 82.1 85.1 83.1 69.3 81.7 83.6 48.7 78.1 Spindle 93.6 84.8 87.8 84.4 73.5 84.3 85.8 52.3 80.8 Bottleneck 92.1 82.6 85.6 83.3 71.2 83.2 84.6 52.1 79.3 B General Effectiveness on SuperGLUE with XLNet Backbones We also directly use the S4-model and S4-3b-model (adopting design patterns discovered using T5-base and T5-3b) to ﬁne-tune the XLNet-base and XLNet-large pretrained backbone models without any extra discovery process. We keep all the other settings the same and evaluate them on SuperGLUE datasets. Table 17 reiterates the fact that our PEFT design patterns discovered from T5 models are generelizable to the XLNet backbone models and outperform the investigated methods in other tasks (SuperGLUE) with no additional discovery process. C On the Discovery Sequence In this work, we follow the discovery sequence of “grouping patterns – trainable parameter allocation – tunable groups – strategy assignment”: 1. To explore and understand the design patterns in all the layers in large pre-trained models in scale, it is necessary and more efﬁcient to study the layers in the unit of groups. So we start with the grouping patterns. 13Table 8: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G1 strategy assignment con- straints to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1-Adapter (A) 89.8 83.5 84.9 80.8 72.5 80.8 78.5 37.7 76.1 G1-Preﬁx (P) 89.3 83.1 84.4 80.1 70.1 80.0 77.6 33.0 74.7 G1-BitFit (B) 89.0 82.9 84.1 81.4 72.0 81.1 77.0 30.8 74.8 G1-LoRA (L) 89.9 83.6 85.0 81.1 71.8 81.0 78.8 35.3 75.8 G1-(P, L) 89.1 82.8 85.1 81.2 71.9 81.5 79.1 35.0 75.7 G1-(A, P) 89.8 82.8 84.8 81.1 72.2 81.3 79.2 36.4 75.9 G1-(A, L) 89.6 83.8 85.6 81.3 72.9 81.7 79.5 36.8 76.4 G1-(A, P, L) 89.6 83.5 85.2 81.5 72.2 81.4 79.2 35.2 75.9 G1-(P, B, L) 89.3 83.6 85.5 81.6 72.3 81.0 78.8 35.7 76.0 G1-(A, P, B) 89.2 83.3 84.8 81.8 72.5 81.1 78.6 35.6 75.8 G1-(A, B, L) 89.8 83.4 84.8 81.1 72.6 81.6 79.4 34.8 75.9 G1-(A, P, B, L) 90.0 83.1 85.3 81.6 72.6 81.4 79.2 36.5 76.1 Table 9: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G2 strategy assignment con- straints with G1-(L, A) to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G2-Adapter (A) 91.6 84.3 85.5 82.3 73.5 82.8 81.3 38.8 77.5 G2-Preﬁx (P) 89.6 84.0 86.5 81.5 73.3 82.5 80.5 36.2 76.7 G2-BitFit (B) 91.2 83.6 85.7 82.9 72.6 82.6 80.8 33.1 76.5 G2-LoRA (L) 91.4 84.4 86.1 82.0 72.8 81.8 81.6 39.8 77.4 G2-(P, L) 91.6 84.6 86.8 81.8 73.8 82.8 82.0 38.5 77.7 G2-(A, P) 92.2 84.2 87.1 82.2 74.4 83.0 82.5 40.8 78.3 G2-(A, L) 92.0 84.4 86.5 81.8 73.6 82.6 82.2 40.1 77.9 G2-(A, P, L) 91.8 84.8 86.8 81.8 74.1 83.0 82.1 37.9 77.7 G2-(P, B, L) 91.6 84.1 87.1 82.0 74.0 82.9 82.4 35.8 77.4 G2-(A, P, B) 91.8 84.2 86.8 82.1 73.7 83.3 82.2 41.2 78.1 G2-(A, B, L) 92.2 84.3 86.1 82.0 74.1 83.2 82.0 37.6 77.6 G2-(A, P, B, L) 92.0 84.1 87.0 81.9 74.2 83.1 81.3 42.4 78.1 2. Once ﬁguring out the optimal grouping patterns, it is then important to explore how to allocate the trainable parameters to these different groups in order to study more subtle designs with fair comparisons (e.g., this would allow comparing different patterns of strategy assignments without the impact from different trainable parameters.). 3. Next, it becomes inﬂuential to examine which groups need to be learned during ﬁne-tuning before we dig into the strategy assignment patterns. Because it is only meaningful to study assigning strategies to different groups after we ﬁgure out which groups need to be learned. 4. Finally, we study the tuning strategy assignment, which is the most subtle design. 14Table 10: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G3 strategy assignment constraints with G1-(L, A) – G2-(P, A) to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G3-Adapter (A) 92.5 85.3 87.5 83.3 73.9 84.0 83.8 44.9 79.4 G3-Preﬁx (P) 91.5 84.7 86.7 82.6 74.2 83.8 82.9 40.5 78.4 G3-BitFit (B) 91.9 84.3 87.0 82.0 73.6 84.1 83.3 36.1 77.8 G3-LoRA (L) 92.8 85.4 87.8 83.5 74.7 82.4 84.0 44.0 79.3 G3-(P, L) 93.0 85.2 88.3 83.8 75.2 84.4 84.2 37.9 79.0 G3-(A, P) 92.4 85.6 88.1 83.6 75.0 84.2 84.0 41.8 79.3 G3-(A, L) 92.0 85.9 88.2 83.1 75.3 84.3 83.9 42.2 79.4 G3-(A, P, L) 92.6 86.0 87.5 83.4 75.6 84.6 83.5 43.9 79.6 G3-(P, B, L) 92.7 85.8 87.2 83.7 75.2 84.5 83.8 40.8 79.2 G3-(A, P, B) 93.3 85.8 88.6 84.0 75.5 84.9 84.1 42.1 79.8 G3-(A, B, L) 93.7 86.5 88.0 83.2 75.8 84.2 84.2 39.7 79.4 G3-(A, P, B, L) 93.3 85.6 87.7 83.8 75.2 84.3 84.4 41.6 79.4 Table 11: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G4 strategy assignment constraints with G1-(A, L) – G2-(A, P) – G3-(A, P, B) to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G4-Adapter (A) 93.8 85.8 88.6 84.8 76.3 85.8 86.0 48.5 81.2 G4-Preﬁx (P) 93.5 85.2 88.3 83.6 76.8 85.3 85.6 44.8 80.3 G4-BitFit (B) 94.1 85.3 88.9 84.4 77.1 85.4 86.2 46.1 80.9 G4-LoRA (L) 94.0 86.0 89.2 85.0 77.2 85.5 85.8 47.7 81.3 G4-(P, L) 94.3 86.2 89.3 85.8 78.0 86.0 88.2 47.2 81.8 G4-(A, P) 94.1 86.2 89.6 85.4 77.9 86.2 86.9 45.3 81.4 G4-(A, L) 94.2 85.9 89.2 85.5 77.8 86.2 88.0 46.8 81.7 G4-(A, P, L) 94.1 85.8 88.8 85.7 77.4 86.5 87.9 44.8 81.3 G4-(P, B, L) 94.6 86.4 90.4 86.1 78.2 86.8 88.5 47.2 82.3 G4-(A, P, B) 94.5 86.0 89.6 86.0 78.0 86.2 88.1 44.8 81.6 G4-(A, B, L) 94.3 86.4 89.2 85.6 78.2 86.4 88.3 46.6 81.9 G4-(A, P, B, L) 94.2 86.2 89.2 85.9 78.5 86.1 88.0 45.3 81.6 Table 12: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different layer grouping constraints to the S0 design space. Grouping Patterns SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg S0-models 80.3 72.1 74.7 72.8 76.9 75.2 71.0 32.2 69.4 Increasing 84.4 75.7 83.0 78.3 82.7 80.3 76.3 42.1 75.3 Uniform 86.8 77.1 82.6 76.2 83.8 81.6 77.3 48.9 76.8 Decreasing 83.2 74.3 81.8 77.3 82.8 79.9 76.5 40.8 74.5 Spindle 88.6 78.8 83.7 77.7 84.2 80.9 78.3 44.6 77.1 Bottleneck 86.3 77.0 82.2 75.6 83.3 80.2 77.1 41.5 75.4 15Table 13: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different layer parameter constraints to the S1 design space. Parameter Allocation SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg Increasing 90.3 79.3 84.9 79.3 85.2 82.8 79.2 50.1 78.9 Uniform 90.6 80.8 84.6 79.7 85.5 82.4 78.9 50.8 79.1 Decreasing 88.6 78.2 83.5 78.1 84.4 81.5 78.1 49.6 77.7 Table 14: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different tuning groups constraints to the S2 design space. Tunable Groups SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1 88.3 78.3 82.2 77.4 82.1 80.7 76.1 49.4 76.8 G2 89.1 78.8 82.1 77.2 82.3 81.2 76.4 49.6 77.1 G3 89.6 78.5 82.6 78.1 83.8 81.9 77.4 48.7 77.5 G4 89.8 79.3 82.7 77.9 83.5 81.9 77.9 48.5 77.1 G1, G2 90.1 80.2 83.4 78.5 84.3 82.4 78.5 51.1 78.5 G3, G4 90.5 80.6 83.8 78.7 84.2 83 78.2 50.3 78.6 G1, G2, G3 90.6 80.3 84.9 79.3 84.7 82.9 79.3 50.2 79.0 G2, G3, G4 90.8 80.9 84.6 79.1 85.1 83.1 79.1 49.2 78.9 G1, G2, G3, G4 91.1 81.4 85.2 80.4 85.9 83.5 80.0 51.6 79.9 16Table 15: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different strategy assignment con- straints following the process in Section 4.2.5. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1-Adapter (A) 91.1 81.4 86.1 80.5 86.7 83.3 80.1 50.8 80.0 G1-Preﬁx (P) 90.8 81.1 85.5 80.2 86.2 83.1 79.8 50.2 79.6 G1-BitFit (B) 90.2 81.3 85.1 79.6 85.8 82.8 79.6 49.5 79.2 G1-LoRA (L) 91.4 81.9 86.2 80.8 86.4 83.9 80.8 49.6 80.0 G1-(P, L) 91.8 82.9 86.8 81.3 87.1 84.2 81.6 52.3 81.0 G1-(A, P) 91.3 81.9 86.4 81.1 85.6 83.7 80.7 52.8 80.1 G1-(A, L) 91.6 82.3 86.1 81.5 85.8 84.9 81.5 51.8 80.6 G1-(A, P, L) 91.1 81.7 85.8 81.2 86.4 84.2 80.9 52.3 80.4 G1-(P, B, L) 91.5 82.8 86.3 81.4 86.1 83.6 81.2 51.5 80.5 G1-(A, P, B) 91.3 82.3 86.7 80.8 86.8 84.3 80.7 51.8 80.5 G1-(A, B, L) 91.7 82.5 86.2 81.3 86.3 84.6 81.3 51.7 80.7 G1-(A, P, B, L) 91.6 82.3 86.2 81.1 86.6 84.2 81.1 51.1 80.5 G2-Adapter (A) 92.1 82.5 86.4 81.8 87.2 84.8 81.8 53.8 81.3 G2-Preﬁx (P) 91.8 83.1 87.2 81.6 86.2 84.4 81.1 52.8 81.0 G2-BitFit (B) 91.2 82.1 86.4 81.1 86.3 84.6 80.3 53.1 80.6 G2-LoRA (L) 92.6 82.9 87.5 81.3 87.4 85.1 81.9 52.2 81.4 G2-(P, L) 91.6 82.7 87.6 81.6 87.8 85.3 82.1 52.8 81.4 G2-(A, P) 92.1 83.3 87.5 81.9 87.4 85.5 81.8 53.1 81.5 G2-(A, L) 92.5 83.7 88.1 82.2 87.4 85.7 82.9 53.6 82.1 G2-(A, P, L) 92.3 83.4 87.4 81.6 87.1 85.3 81.4 53.2 81.4 G2-(P, B, L) 91.8 83.1 87.4 81.5 87.2 85.1 82.7 53.8 81.5 G2-(A, P, B) 91.5 82.6 87.8 81.3 86.5 85.2 82.1 54.2 81.4 G2-(A, B, L) 92.6 83.5 87.2 82 87.3 86.5 82.5 52.8 81.8 G2-(A, P, B, L) 92.8 83.2 87.6 81.6 87.5 85.5 82.4 51.2 81.5 G3-Adapter (A) 92.6 84.1 88.3 81.8 87.8 85.4 82.8 55.2 82.2 G3-Preﬁx (P) 92.1 83.3 87.6 81.4 87.1 85.4 82.6 53.5 81.6 G3-BitFit (B) 92.4 83.9 88.4 82.1 87.2 85.8 82.4 53.3 81.9 G3-LoRA (L) 93.1 84.3 87.7 82.4 87.8 86.2 83.1 54.3 82.3 G3-(P, L) 92.8 84.1 88.7 82.6 88.2 86.2 83.3 54.7 82.6 G3-(A, P) 93.1 83.8 89.1 82.3 88.1 85.8 82.6 55.1 82.5 G3-(A, L) 92.7 84.5 88.4 82.8 88.2 86.1 83.5 54.6 82.6 G3-(A, P, L) 92.8 84.6 88.1 82.5 87.7 85.5 83.2 53.8 82.3 G3-(P, B, L) 93.6 84.9 89.3 83.1 88.2 86.5 83.9 55.8 83.2 G3-(A, P, B) 93.3 83.9 88.5 82.2 88.4 86.2 83.5 55.3 82.6 G3-(A, B, L) 93.4 84.2 88.9 82.6 87.8 85.8 84.2 54.9 82.7 G3-(A, P, B, L) 92.2 84.4 88.7 82.3 88.5 86.2 84.2 54.2 82.5 G4-Adapter (A) 92.8 85.2 89.1 83.5 87.8 86.5 84.2 56.3 83.2 G4-Preﬁx (P) 92.8 84.6 89.5 82.6 87.4 86.5 83.8 55.8 82.8 G4-BitFit (B) 93.8 84.9 89.5 83.3 88.7 86.8 84.4 55.2 83.3 G4-LoRA (L) 93.3 84.7 89.3 82.7 88.3 86.2 82.7 54.7 82.7 G4-(P, L) 93.8 85.3 89.6 83.6 88.6 86.8 84.6 56.3 83.5 G4-(A, P) 93.8 84.9 89.8 84.3 88.5 86.6 84.8 56.7 83.6 G4-(A, L) 93.7 85.6 89.5 84.1 88.2 86.6 85.2 55.4 83.5 G4-(A, P, L) 94.2 85.2 89.6 83.9 88.2 86.4 84.9 55.9 83.5 G4-(P, B, L) 93.8 85.9 89.8 83.6 88.6 86.9 85.2 56.3 83.7 G4-(A, P, B) 94.4 85.7 90.1 84.8 88.9 87.2 85.3 57.3 84.2 G4-(A, B, L) 93.8 85.3 89.5 84.1 88.8 86.7 85.5 56.6 83.7 G4-(A, P, B, L) 94.1 85.4 89.7 84.4 88.5 86.5 85.2 56.8 83.8 17Table 16: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different layer grouping constraints to the S0 design space. Layer grouping is based on 8 groups. Layer Grouping SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg S0-models 76.9 70.1 72.5 73.3 63.6 71.7 73.8 24.3 65.7 Increasing 83.2 74 .1 76 .6 77 .1 67 .7 76.8 74.7 30.0 70.0 Uniform 83.6 73.4 78.0 77.9 68.2 76.4 78.6 34.2 71.3 Decreasing 80.3 71.6 77.4 75.5 67.0 75.3 77.2 26.4 68.9 Spindle 86.2 74.3 79.1 78.6 68.5 77.4 79.5 35.1 72.3 Bottleneck 83.2 73.1 75.8 77.6 67.9 75.3 78.2 31.4 70.3 Table 17: Performances of different tuning methods on the SuperGLUE datasets using the XLNet-base (upper part) and XLNet-large (lower part) pretrained backbone models, respectively. The results are averaged over 10 random runs. The S4-model and S4-3b-model perform signiﬁcantly better than the second-best PEFT methods in all the eight datasets at the signiﬁcance level p <0.05 (*) or even p <0.01 (**). Method BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC Average Adapter 72.8 71.3/78.0 64.0 67.0/24.5 71.0/71.8 76.2 65.0 60.8 66.2 Preﬁx 72.0 70.5/77.0 63.3 66.4/23.8 69.9/71.0 75.5 64.4 60.8 65.9 BitFit 71.8 70.0/76.2 62.8 65.8/22.6 69.4/70.6 74.5 64.8 60.6 65.2 LoRA 72.2 71.1/77.8 64.7 67.4/24.8 70.8/71.3 76.8 65.1 61.1 66.4 S4-model 73.8∗∗ 71.7/78.4∗ 65.9∗∗ 68.2/25.5∗∗ 71.1/72.0∗ 78.4∗∗ 65.8∗ 62.6∗ 67.5 Adapter 74.4 71.4/81.1 67.4 68.8/26.4 71.7/72.4 80.8 68.0 64.6 68.8 Preﬁx 72.4 70.0/78.3 66.9 68.8/25.8 70.9/71.2 78.8 66.9 64.0 67.7 BitFit 71.1 70.7/79.8 68.0 68.6/25.4 71.1/71.6 80.4 67.2 64.3 68.1 LoRA 74.1 72.1/80.9 67.9 69.1/26.8 72.0/72.8 81.0 67.8 64.4 69.0 S4-3b-model 76.8∗∗ 74.6/81.9∗∗ 68.6∗∗ 69.5/27.1∗ 72.4/73.3∗ 81.2∗ 68.2∗∗ 64.8∗ 69.7 Table 18: Total training time (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model with 8 A100 GPUs from S0 to S1. SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA 18 mins 22 mins 20 mins 40 mins 8 mins 12 mins 8 mins 6 mins 18",
      "meta_data": {
        "arxiv_id": "2301.01821v1",
        "authors": [
          "Jiaao Chen",
          "Aston Zhang",
          "Xingjian Shi",
          "Mu Li",
          "Alex Smola",
          "Diyi Yang"
        ],
        "published_date": "2023-01-04T21:00:18Z",
        "pdf_url": "https://arxiv.org/pdf/2301.01821v1.pdf"
      }
    },
    {
      "title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
      "abstract": "Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA\nand its variants have gained considerable popularity because of avoiding\nadditional inference costs. However, there still often exists an accuracy gap\nbetween these methods and full fine-tuning (FT). In this work, we first\nintroduce a novel weight decomposition analysis to investigate the inherent\ndifferences between FT and LoRA. Aiming to resemble the learning capacity of FT\nfrom the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA).\nDoRA decomposes the pre-trained weight into two components, magnitude and\ndirection, for fine-tuning, specifically employing LoRA for directional updates\nto efficiently minimize the number of trainable parameters. By employing \\ours,\nwe enhance both the learning capacity and training stability of LoRA while\navoiding any additional inference overhead. \\ours~consistently outperforms LoRA\non fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as\ncommonsense reasoning, visual instruction tuning, and image/video-text\nunderstanding. Code is available at https://github.com/NVlabs/DoRA.",
      "full_text": "DoRA: Weight-Decomposed Low-Rank Adaptation Shih-Yang Liu1 2 Chien-Yi Wang1 Hongxu Yin 1 Pavlo Molchanov1 Yu-Chiang Frank Wang1 Kwang-Ting Cheng 2 Min-Hung Chen 1 Abstract Among the widely used parameter-efficient fine- tuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decom- position analysis to investigate the inherent dif- ferences between FT and LoRA. Aiming to re- semble the learning capacity of FT from the findings, we propose Weight-Decomposed Low- Rank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, mag- nitude and direction, for fine-tuning, specifically employing LoRA for directional updates to ef- ficiently minimize the number of trainable pa- rameters. By employing DoRA, we enhance both the learning capacity and training stabil- ity of LoRA while avoiding any additional in- ference overhead. DoRA consistently outper- forms LoRA on fine-tuning LLaMA, LLaV A, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruc- tion tuning, and image/video-text understanding. Code is available at https://github.com/ NVlabs/DoRA. 1. Introduction Models that are pre-trained with extensive general domain datasets have demonstrated remarkable generalization abil- ities, significantly benefiting a wide array of applications, from natural language processing (NLP) tasks (Qin et al., 2023; Taori et al., 2023) to multi-modal tasks (Li et al., 2022; Liu et al., 2023a). To tailor these general models for spe- cific downstream tasks, full fine-tuning (FT) is commonly 1NVIDIA 2HKUST. Correspondence to: Shih-Yang Liu <shi- hyangl@nvidia.com, sliuau@connect.ust.hk>, Min-Hung Chen <minhungc@nvidia.com>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). Magnitude B  A  Pretrained Weight Merged Weight Pretrained Weight Pretrained Weight Adapt Frozen Trainable Magnitude Direction Direction Decompose (Initialize) Merge Figure 1.An overview of our proposed DoRA, which decomposes the pre-trained weight into magnitude and direction components for fine-tuning, especially with LoRA to efficiently update the direction component. Note that || · ||c denotes the vector-wise norm of a matrix across each column vector. employed, involving the retraining of all model parameters. Nevertheless, as the size of models and datasets expand in scale, the expense associated with fine-tuning the entire model becomes prohibitively large. To address this issue, parameter-efficient fine-tuning (PEFT) methods (Houlsby et al., 2019) have been introduced to fine- tune the pre-trained models with only a minimal number of parameters. Among these, LoRA (Hu et al., 2022), which does not change the model architecture, has become notably popular for its simplicity and efficacy. Nevertheless, there is still a capacity gap between LoRA and FT, which is often attributed to the limited number of trainable parameters without further exploration of other underlying causes (Hu et al., 2022; Kopiczko et al., 2024). Drawing on Weight Normalization (Salimans & Kingma, 2016), which achieves faster convergence via improving the conditioning of the gradient with weight reparameterization, we introduce a novel weight decomposition analysis that ini- tially reparameterizes model weights into magnitude and di- rectional components, subsequently examining the changes 1 arXiv:2402.09353v6  [cs.CL]  9 Jul 2024DoRA: Weight-Decomposed Low-Rank Adaptation in magnitude and direction introduced by LoRA and FT. Our analysis reveals that LoRA and FT exhibit markedly distinct patterns of updates, leading us to surmise that these variations mirror the learning capability of each method. Inspired by our findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA), which begins by decompos- ing the pre-trained weight into its magnitude and directional components, then fine-tunes both. Given the substantial size of the directional component in terms of parameters, we exploit LoRA for the directional adaptation to enable efficient fine-tuning, as illustrated in Figure.1. Moreover, by showing a learning behavior similar to FT both empir- ically and mathematically, suggesting a learning capacity closely resembling FT, we have validated DoRA across a wide variety of tasks, from NLP to Vision-Language, and over various backbones, including LLM and LVLM. The experimental results show that DoRA consistently outper- forms LoRA without sacrificing inference efficiency, such as commonsense reasoning (+3.7/+1.0 on LLaMA-7B/13B, +2.9 on LLaMA2-7B, and +4.4 on LLaMA3-8B), visual in- struction tuning (+0.6 on LLaV A-7B), and image/video-text understanding (+0.9/+1.9 on VL-BART). The summary of our contributions is as follows: • We introduce DoRA, a novel PEFT method that incor- porates weight decomposition, achieving a learning capacity closely resembling FT without any additional inference latency over LoRA. • We introduce a novel weight decomposition analysis to uncover the fundamental differences in the learning patterns of FT and different PEFT methods. • DoRA consistently surpasses LoRA on various tasks, from NLP to Vision-Language benchmarks and across various backbones, including LLM and LVLM. 2. Related Works Parameter-Efficient Fine-Tuning (PEFT)methods are de- signed to reduce the high expense of fine-tuning large-scale models. They achieve this by training a relatively small subset of parameters, compared to the total number of pa- rameters, for adapting to downstream tasks. Existing PEFT methods can be divided into three categories. The first category is referred to as Adapter-based methods, which involve introducing additional trainable modules into the original frozen backbone, such as (Houlsby et al., 2019; He et al., 2021; Karimi Mahabadi et al., 2021; mahabadi et al., 2021). For example, (Houlsby et al., 2019) proposes adding linear modules in sequence to the existing layer, whereas (He et al., 2021) advocates for integrating these modules in parallel with the original layer to enhance performance. The second category is Prompt-based methods. These methods add extra soft tokens (prompts) to the initial input and fo- cus solely on fine-tuning these trainable vectors, as seen in works like (Lester et al., 2021; Razdaibiedina et al., 2023; Wang et al., 2023). However, these approaches typically face challenges due to their sensitivity to initialization, af- fecting their overall effectiveness. These first two categories, whether altering the model’s input or architecture, result in increased inference latency compared to the baseline model. LoRA (Hu et al., 2022) and its variants are among the third category of PEFT, notable for not adding any extra inference burden. These methods apply low-rank matrices to approximate weight changes during fine-tuning and can merge with pre-trained weights prior to inference. For ex- ample, (Zhang et al., 2023) employs SVD decomposition and prunes less significant singular values for more efficient updates. (Hyeon-Woo et al., 2022) focuses on low-rank Hadamard product for federated learning. (Qiu et al., 2023; Liu et al., 2023b) exploit orthogonal factorization in fine- tuning diffusion models. (Renduchintala et al., 2023) uses weight tying to further reduce the trainable parameters. (Yeh et al., 2023) introduces a unified LoRA family framework for Stable diffusion. (Ponti et al., 2022) chooses different combinations of LoRAs from the inventory with a routing function for different tasks. (Kopiczko et al., 2024) imple- ments learnable scaling vectors to adjust a shared pair of frozen random matrices across layers. Our research also falls within this third category, and we validate the efficacy of our proposed method alongside LoRA and its variants through comprehensive experimentation. 3. Pattern Analysis of LoRA and FT 3.1. Low-Rank Adaptation (LoRA) Building upon the hypothesis that updates made during the fine-tuning exhibit a low “intrinsic rank”, LoRA (Hu et al., 2022) proposes using the product of two low-rank matri- ces to update the pre-trained weights incrementally. For a pre-trained weight matrix W0 ∈ Rd×k, LoRA models the weight update ∆W ∈ Rd×k utilizing a low-rank decompo- sition, expressed as BA, where B ∈ Rd×r and A ∈ Rr×k represent two low-rank matrices, withr ≪ min(d, k). Con- sequently, the fine-tuned weight W′ can be represented as: W′ = W0 + ∆W = W0 + BA (1) where W0 remains static during the fine-tuning process, and the underlined parameters are being trained. The ma- trix A is initialized with uniform Kaiming distribution (He et al., 2015), while B is initially set to zero, resulting in ∆W = BA being zero at the start of training. Notably, this decomposition of ∆W can be substituted with other LoRA variants, such as VeRA (Kopiczko et al., 2024). Ad- ditionally, based on Eq. (1), we can merge the learned ∆W with the pre-trained weight W0 and obtain W′ in advance 2DoRA: Weight-Decomposed Low-Rank Adaptation of deployment, and given that both W′ and W0 both fall within the dimensionality of Rd×k, LoRA and its related variants do not introduce any extra latency during the infer- ence compared to the original model. 3.2. Weight Decomposition Analysis The study presented in LoRA (Hu et al., 2022) suggests that LoRA can be considered a general approximation of full fine-tuning. By gradually increasing the rank r of LoRA to align with the rank of pre-trained weights, LoRA can attain a level of expressiveness akin to that of FT. Con- sequently, many previous studies have attributed the dis- crepancy in accuracy between LoRA and FT primarily to the limited number of trainable parameters, often without further analysis (Hu et al., 2022; Kopiczko et al., 2024). Drawing inspiration from Weight Normalization (Salimans & Kingma, 2016), which reparameterizes the weight matrix into magnitude and direction for accelerating optimization, we introduce an innovative weight decomposition analysis. Our analysis restructures the weight matrix into two sep- arate components, magnitude and direction, to reveal the inherent differences in LoRA and FT learning patterns. Analysis Method: This analysis examines the updates in both magnitude and direction of the LoRA and FT weights relative to the pre-trained weights to reveal the fundamental differences in the learning behaviors of both. The weight decomposition of W ∈ Rd×k can be formulated as: W = m V ||V ||c = ||W||c W ||W||c (2) where m ∈ R1×k is the magnitude vector, V ∈ Rd×k is the directional matrix, with ||·|| c being the vector-wise norm of a matrix across each column. This decomposition ensures that each column of V/||V ||c remains a unit vector, and the corresponding scalar in m defines the magnitude of each vector. For our weight decomposition analysis, we select the VL- BART model fine-tuned on four image-text tasks as outlined in (Sung et al., 2022) for a case study. Following (Sung et al., 2022), which applies LoRA only to the query/value weight matrix in the self-attention module. We decompose the pre- trained weight W0, the full fine-tuned weight WFT, and the merged LoRA weight WLoRA of query/value weight matrix using Eq. (2). The magnitude and directional variations between W0 and WFT can be defined as follows: ∆Mt FT = Pk n=1 |mn,t FT − mn 0 | k (3) ∆Dt FT = Pk n=1(1 − cos(V n,t FT , Wn 0 )) k (4) Here, ∆Mt FT and and ∆Dt FT represent the magnitude dif- ference and directional difference between W0 and WFT at t training step respectively, with cos(·, ·) being the cosine similarity function. Mn,t FT and Mn 0 are the nth scalars in their respective magnitude vectors, while V n,t FT and Wn 0 are the nth columns in V t FT and W0. The magnitude and direc- tional differences between WLoRA and W0 are calculated similarly, as per Eq. (3) and Eq. (4). We select checkpoints from four different training steps for analysis, comprising three intermediate steps and the final checkpoint from both FT and LoRA, and we perform weight decomposition anal- ysis on each of these checkpoints to determine the ∆M and ∆D throughout different layers. Analysis Results: Figure 2 (a) and (b) illustrate the alter- ations in the query weight matrix of FT and LoRA, with each point representing a ( ∆Dt, ∆Mt) pair from query weight matrices across different layers and training steps. Similarly, Figure 7 in the appendix displays the value weight matrix modifications. It is noticeable that LoRA exhibits a consistent positive slope trend across all the intermediate steps, signifying a proportional relationship between the changes in direction and magnitude. In contrast, the FT displays a more varied learning pattern with a relatively neg- ative slope. This distinction between FT and LoRA likely mirrors their respective learning capability. While LoRA tends to either increase or decrease the magnitude and direc- tion updates proportionally, it lacks the nuanced capability for more subtle adjustments. Specifically, LoRA does not show proficiency in executing slight directional changes alongside more significant magnitude alterations, or vice versa, a feature more characteristic of the FT method. We suspect that such limitation of LoRA might stem from the challenge of concurrent learning both magnitude and direc- tional adaptation, which could be overly complex for LoRA. Consequently, in this work, we aim to propose a variant of LoRA that exhibits a learning pattern more closely resem- bling that of FT, and can improve the learning capacity over LoRA. 4. Method 4.1. Weight-Decomposed Low-Rank Adaptation Drawing from the insights of our weight decomposition analysis, we introduce Weight- Decomposed L ow-Rank Adaptation (DoRA). DoRA initially decomposes the pre- trained weight into its magnitude and directional compo- nents and finetunes both of them. Because the directional component is large in terms of parameter numbers, we fur- ther decompose it with LoRA for efficient finetuning. Our intuitions are two-fold. Firstly, we believe that limiting LoRA to concentrate exclusively on directional adaptation while also allowing the magnitude component to be tun- able simplifies the task compared to the original approach, where LoRA is required to learn adjustments in both mag- 3DoRA: Weight-Decomposed Low-Rank Adaptation 0.014 0.016 0.018 0.020 0.022 D (a) 0.05 0.06 0.07 0.08 0.09 M FT layer 1 layer 2 layer 3 layer 4 layer 5 layer 6 Inter step 1 Inter step 2 Inter step 3 Final step 0.10 0.11 0.12 0.13 0.14 0.15 0.16 D (b) 0.25 0.30 0.35 0.40 0.45 LoRA Inter step 1 Inter step 2 Inter step 3 Final step 0.16 0.18 0.20 0.22 0.24 0.26 0.28 0.30 0.32 D (c) 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 DoRA Inter step 1 Inter step 2 Inter step 3 Final step Figure 2.Magnitude and direction updates of (a) FT, (b) LoRA, and (c) DoRA of the query matrices across different layers and intermediate steps. Different markers represent matrices of different training steps and different colors represent the matrices of each layer. nitude and direction. Secondly, the process of optimizing directional updates is made more stable through weight decomposition, which we delve into more thoroughly in Section.4.2. It is important to highlight that the main distinc- tion between DoRA and weight normalization (Salimans & Kingma, 2016) lies in their training approaches. Weight normalization trains both components from scratch, mak- ing the method sensitive to different initializations. Con- versely, DoRA avoids such initialization concerns since both components begin with pre-trained weights. We initialize DoRA with pre-trained weight W0 as outlined in Eq. (2), where m = ||W0||c and V = W0 after initialization. We then keep V frozen and m a trainable vector. The directional component is then updated through LoRA. DoRA can be formulated similar to Eq. (1) as: W′ = m V + ∆V ||V + ∆V ||c = m W0 + BA ||W0 + BA||c (5) where ∆V is the incremental directional update learned by multiplying two low-rank matrices B and A, and the underlined parameters denote the trainable parameters. The matrices B ∈ Rd×r and A ∈ Rr×k are initialized in line with LoRA’s strategy to ensure that W′ equals W0 before the finetuning. Furthermore, DoRA can be merged with the pre-trained weight before inference, thereby not introducing any additional latency. We visualize the magnitude and directional differences of the query weight matrix between the merged DoRA weight and W0 in the same setting as for FT and LoRA in Figure 2 (c) and leave the visualization of the value weight matrix in the appendix. From the regression line for (∆D, ∆M) of both DoRA and FT, we reveal that in contrast to LoRA’s pattern, DoRA, and FT are characterized by a distinct neg- ative slope. We reason that FT tends towards a negative slope because pre-trained weights already possess substan- tial knowledge suitable for various downstream tasks. There- fore, when provided with adequate learning capacity, having a larger magnitude or direction alteration alone is sufficient enough for downstream adaptation. We additionally com- pute the correlation between ∆D and ∆M for FT, LoRA, and DoRA, and we found that both FT and DoRA exhibit negative correlation values of -0.62 and -0.31, respectively. In contrast, LoRA shows a positive correlation with a value of 0.83. In conclusion, the fact that DoRA demonstrates the ability to make only substantial directional adjustments with relatively minimal changes in magnitude or the reverse while showing learning patterns closer to FT’s signifies its superior learning capacity over LoRA. 4.2. Gradient Analysis of DoRA In this section, we first derive the gradient of DoRA and illustrate how our proposed decomposition benefits the opti- mization of ∆V . Subsequently, we analyze from the gradi- ent’s perspective to explicate the learning pattern of DoRA, which tends to have a negative slope. From Eq. (5), we can obtain the gradient of Loss L with respect to m and V ′ = V + ∆V as: ∇V ′L = m ||V ′||c \u0012 I − V ′V ′T ||V ′||2c \u0013 ∇W′L (6) ∇mL = ∇W′L ·V ′ ||V ′||c (7) Eq. (6) reveals that the weight gradient ∇W′L is scaled by m/||V ′||c and is projected away from the current weight matrix. These two effects contribute to aligning the gra- dient’s covariance matrix more closely with the identity matrix, which is advantageous for optimization (Salimans & Kingma, 2016). Additionally, given that V ′ = V + ∆V , the gradient ∇V ′L is equivalent to ∇∆V L. Therefore, the optimization benefits derived from this decomposition are fully transferred to ∆V , enhancing the learning stability of LoRA. We can gain further insight into the learning pattern of DoRA by referring to Eq. (7). In the subsequent dis- cussion, we represent vectors using lower-case letters in- stead of the previous matrix form notation. Consider w′′ = w′ + ∆w as the parameter update for a weight vector, where ∆w ∝ ∇w′L. In two hypothetical update 4DoRA: Weight-Decomposed Low-Rank Adaptation scenarios, S1 and S2, S1 involves a smaller directional update (∆DS1), while S2 involves a larger one ( ∆DS2). Assuming ||∆wS1|| = ||∆wS2||, and at time 0, we have ∆v = 0 and v′ = v. From ∆DS1 < ∆DS2, it follows that |cos(∆wS1, w′)| > |cos(∆wS2, w′)|. Since ∆w ∝ ∇w′L, it implies |cos(∇S1 w′ L, w′)| > |cos(∇S2 w′ L, w′)|. From Sec 4.1, with v initialized as v0 and w′ = w0 at time 0, we get |cos(∇w′L, w′)| = |cos(∇w′L, v′)| = |cos(∇w′L, v)|. Using the cosine similarity equation with ∆v = 0: cos(∇w′L, v′) = cos(∇w′L, v) = ∇w′L ·v ||∇w′L||||v|| (8) denote m∗ as the magnitude scalar of vector w′ then Eq. (7) w.r.t m∗ can be rewritten to: ∇m∗L = ∇w′L ·v′ ||v′|| = ||∇w′L|| ·cos(∇w′L, v) (9) Given that ||∆wS1|| = ||∆wS2|| for S1 and S2, and ||∇S1 w′ L|| = ||∇S2 w′ L||. Therefore, with: ||∇S1 w′ L|| · |cos(∇S1 w′ L, v)| > ||∇S2 w′ L|| · |cos(∇S2 w′ L, v)| (10) it can be inferred that |∇S1 m∗L| > |∇S2 m∗L| which indicate that S1 has larger magnitude updates over S2 while having smaller directional alteration than that of S2. Our conclu- sion generally holds in practice, as evidenced by Figure 2 (c). Consequently, we have effectively shown how DoRA can be utilized to adjust the learning pattern, diverging from that of LoRA and aligning more closely with the pattern of FT. 4.3. Reduction of Training Overhead In Eq. (1), the gradients of W′ and ∆W are the same. How- ever, with DoRA, which redirects the low-rank adaptation towards the directional component, the gradient of the low- rank updates differs from that ofW′, as illustrated in Eq. (6). This divergence necessitates extra memory during backprop- agation. To address this, we suggest treating||V + ∆V ||c in Eq. (5) as a constant, thereby detaching it from the gradient graph. This means that while ||V + ∆V ||c dynamically reflects the updates of ∆V , it won’t receive any gradient during backpropagation. With this modification, the gra- dient w.r.t m remains unchanged, and ∇V ′L is redefined as: ∇V ′L = m C ∇W′L where C = ||V ′||c (11) This approach reduces the gradient graph memory consump- tion drastically without a noticeable difference in accuracy. We conduct an ablation study to evaluate the impact of the proposed modification on fine-tuning LLaMA-7B and VL- BART. The results indicate that the modification leads to a training memory reduction of approximately 24.4% in fine-tuning LLaMA and 12.4% in VL-BART. Furthermore, the accuracy of DoRA with the modification remains un- changed for VL-BART and shows a negligible difference of only 0.2 compared to DoRA without the modification on LLaMA. For a comprehensive comparison of training memory usage and accuracy differences, please see Table 7 in the appendix. Consequently, all subsequent experiments with DoRA incorporate this adjustment. 5. Experiments We conduct a variety of experiments to showcase the effi- cacy of DoRA on various tasks including language, image, and video domains. Firstly, we evaluate DoRA against sev- eral Parameter-Efficient Fine-Tuning (PEFT) methods by fine-tuning LLaMA-7B/13B, LLaMA2-7B, and LLaMA3- 8B on commonsense reasoning tasks. Subsequently, we extend from single modality to multimodality. We compare DoRA with LoRA across multi-task image/video-text under- standing tasks using VL-BART and visual instruction tuning with LLaV A-1.5-7B. Following this, we explore the com- patibility of DoRA with LoRA and VeRA (Kopiczko et al., 2024) for instruction-tuning on LLaMA-7B and LLaMA2- 7B. Furthermore, we perform a series of ablation studies to illustrate that DoRA surpasses LoRA in performance, irre- spective of the number of fine-tuning training samples and rank variations. Lastly, We analyze the tuning granularity of DoRA, and show that DoRA can achieve better accuracy than LoRA with fewer trainable parameters by selectively updating only the directional components of certain mod- ules. 5.1. Commonsense Reasoning We evaluate DoRA against LoRA and several baseline meth- ods which include Prompt learning (Prefix) (Li & Liang, 2021), Series adapter (Series) (Houlsby et al., 2019), and Parallel adapter (Parallel) (He et al., 2021) on LLaMA- 7B/13B (Touvron et al., 2023) for commonsense reasoning tasks. We also include ChatGPT’s accuracy obtained with gpt-3.5-turbo API using a zero-shot Chain of Thought (Ope- nAI, 2023; Wei et al., 2022). The commonsense reasoning tasks comprise 8 sub-tasks, each with a predefined training and testing set. We follow the setting of (Hu et al., 2023) and amalgamate the training datasets from all 8 tasks to create the final training dataset and conduct evaluations on the individual testing dataset for each task. To ensure a fair comparison, we initially fine-tuned models with DoRA following the LoRA config- uration, maintaining the same rank while adjusting only the learning rate. The marginal increase of 0.01% in the number of trainable parameters for DoRA over LoRA, as detailed in Table 1, arises from the inclusion of learnable magnitude components (parameter of size 1 × k). Then, we further halve the rank used in DoRA compared to LoRA 5DoRA: Weight-Decomposed Low-Rank Adaptation Table 1.Accuracy comparison of LLaMA 7B/13B, LLaMA2 7B, and LLaMA3 8B with various PEFT methods on eight commonsense reasoning datasets. Results of all the baseline methods on LLaMA 7B/13B are taken from (Hu et al., 2023). Results of LoRA on LLaMA2 7B and LLaMA3 8B are obtained using the hyperparameters described in (Hu et al., 2023). DoRA†: the adjusted version of DoRA with the rank halved. Model PEFT Method # Params (%) BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg. ChatGPT - - 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0 LLaMA-7B Prefix 0.11 64.3 76.8 73.9 42.1 72.1 72.9 54.0 60.6 64.6 Series 0.99 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8 Parallel 3.54 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.2 LoRA 0.83 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7 DoRA† (Ours) 0.43 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5 DoRA (Ours) 0.84 69.7 83.4 78.6 87.2 81.0 81.9 66.2 79.2 78.4 LLaMA-13B Prefix 0.03 65.3 75.4 72.1 55.2 68.6 79.5 62.9 68.0 68.4 Series 0.80 71.8 83 79.2 88.1 82.4 82.5 67.3 81.8 79.5 Parallel 2.89 72.5 84.9 79.8 92.1 84.7 84.2 71.2 82.4 81.4 LoRA 0.67 72.1 83.5 80.5 90.5 83.7 82.8 68.3 82.4 80.5 DoRA† (Ours) 0.35 72.5 85.3 79.9 90.1 82.9 82.7 69.7 83.6 80.8 DoRA (Ours) 0.68 72.4 84.9 81.5 92.4 84.2 84.2 69.6 82.8 81.5 LLaMA2-7B LoRA 0.83 69.8 79.9 79.5 83.6 82.6 79.8 64.7 81.0 77.6 DoRA† (Ours) 0.43 72.0 83.1 79.9 89.1 83.0 84.5 71.0 81.2 80.5 DoRA (Ours) 0.84 71.8 83.7 76.0 89.1 82.6 83.7 68.2 82.4 79.7 LLaMA3-8B LoRA 0.70 70.8 85.2 79.9 91.7 84.3 84.2 71.2 79.0 80.8 DoRA† (Ours) 0.35 74.5 88.8 80.3 95.5 84.7 90.1 79.1 87.2 85.0 DoRA (Ours) 0.71 74.6 89.3 79.9 95.5 85.6 90.5 80.4 85.8 85.2 and denote this adjusted configuration as DoRA†. See Table 8 for details on the hyperparameters used. Table 1 demonstrates that DoRA consistently surpasses all baseline methods across both LLaMA-7B/13B, LLaMA2- 7B and LLaMA3-8B. Notably, in the LLaMA-7B model, where LoRA exceeds the performance of other baselines, DoRA further enhances accuracy by 3.7%, outstripping ChatGPT’s accuracy levels. Conversely, for LLaMA-13B, where LoRA’s effectiveness is inferior to the Parallel adapter, DoRA achieves superior accuracy over LoRA by 1% and comparable accuracy to the Parallel adapter, with only a quarter of the trainable parameters required by the Parallel adapter and without adding any extra inference overhead as the Parallel adapter. Additionally, DoRA consistently surpasses LoRA on both LLaMA2-7B and LLaMA3-8B by 2.1% and 4.4%, respectively. Furthermore, DoRA† exceeds LoRA’s performance on LLaMA-7B by 2.8%, on LLaMA- 13B by 1%, on LLaMA2-7B by 2.9%, and on LLaMA3-8B by 4.2%, despite having only half as many trainable parame- ters as LoRA. This outcome suggests that the integration of DoRA enhances the learning capability of LoRA, thereby reducing the need for a higher rank to surpass LoRA in terms of accuracy. Additionally, in previous sections, we hypothesize that a negative correlation between the magnitude update and di- rectional update is more optimal than a positive correlation. This is because pre-trained weights already contain sub- stantial knowledge suitable for downstream tasks, and a larger magnitude or direction alteration alone is sufficient Figure 3.Magnitude (a) and direction (b) difference of LoRA/DoRA and the pre-trained weight of the query matrices across different layers. for downstream adaptation. To further validate our hypoth- esis, we used LLaMA2-7B fine-tuned with DoRA/LoRA on commonsense reasoning datasets as a case study. We visualized the magnitude (∆M) and directional difference (∆D) between the DoRA/LoRA weights and the pre-trained model weights across different modules and layers. In Fig- ure 3 (a) and (b), we observe that the DoRA fine-tuned weights show less deviation from the pre-trained weights in both magnitude and direction, while the differences for the LoRA fine-tuned weights are significantly larger. Coupled with the experimental results that DoRA significantly out- performs LoRA, we can conclude that our earlier hypothesis is valid: a robust foundation model does not require signif- icant alterations for effective downstream adaptation and having the ability to perform more fine-grained magnitude and directional update explains the superiority of DoRA over LoRA. We leave the visualization of the value and key 6DoRA: Weight-Decomposed Low-Rank Adaptation weight matrices in the appendix. 5.2. Image/Video-Text Understanding Table 2.The multi-task evaluation results on VQA, GQA, NVLR2 and COCO Caption with the VL-BART backbone. Method #Params (%) VQAv2 GQA NVLR2 COCO Cap Avg. FT 100 66.9 56.7 73.7 112.0 77.3 LoRA 5.93 65.2 53.6 71.9 115.3 76.5 DoRA (Ours) 5.96 65.8 54.7 73.1 115.9 77.4 Table 3.The multi-task evaluation results on TVQA, How2QA, TVC, and YC2C with the VL-BART backbone. Method #Params (%) TVQA How2QA TVC YC2C Avg. FT 100 76.3 73.9 45.7 154 87.5 LoRA 5.17 75.5 72.9 44.6 140.9 83.5 DoRA (Ours) 5.19 76.3 74.1 45.8 145.4 85.4 Having shown that DoRA can consistently achieve bet- ter accuracy on fine-tuning LLM, we would like to see if DoRA can remain competitive on multi-modality fine- tuning tasks. We compare DoRA with LoRA and full fine- tuning on VL-BART which comprises a vision encoder (CLIP-ResNet101 (Radford et al., 2021)) and an encoder- decoder language model ( BARTBase (Lewis et al., 2020)) across four different image-text tasks: VQAv2 (Goyal et al., 2017) and GQA (Hudson & Manning, 2019) for visual ques- tion answering, NLVR2 (Suhr et al., 2019) for visual reason- ing, and MSCOCO (Chen et al., 2015) for image captioning, and four different video-text tasks from the V ALUE (Li et al., 2021) Benchmark: TVQA (Lei et al., 2018) and How2QA (Li et al., 2020) for video question answering, TVC (Lei et al., 2020) and YC2C (Zhou et al., 2018) for video captioning. We follow the same framework as (Sung et al., 2022) and fine-tuned VL-BART within a multi-task framework for both image/video-text tasks. We adopt the same setup as that of LoRA outlined in (Sung et al., 2022) when ap- plying DoRA. See Table 9 for the complete hyperparam- eters. The result of LoRA and FT for both image/video- text tasks is directly quoted from (Sung et al., 2022). We can see that DoRA uniformly surpasses LoRA in accuracy while maintaining a similar count of trainable parameters in both Table 2 and Table 3. In particular, DoRA exceeds LoRA’s performance by nearly 1% in image-text under- standing tasks, reaching the accuracy level of FT. Moreover, DoRA achieves roughly 2% higher accuracy than LoRA in video-text understanding tasks. 5.3. Visual Instruction Tuning We further scale up the model size and compare DoRA to LoRA and FT on the visual instruction tuning tasks with Table 4.Visual instruction tuning evaluation results for LLaV A- 1.5-7B on a wide range of seven vision-language tasks. We directly use checkpoints from (Liu et al., 2023a) to reproduce their results. Method # Params(%) Avg. FT 100 66.5 LoRA 4.61 66.9 DoRA (Ours) 4.63 67.6 LLaV A-1.5-7B (Liu et al., 2023a) which is composed of a language model, Vicuna-1.5-7B (Peng et al., 2023), and a vision encoder, CLIP ViT-L/336px (Radford et al., 2021). The training datasets contain several datasets from VQA (Goyal et al., 2017; Hudson & Manning, 2019; Marino et al., 2019; Schwenk et al., 2022), OCR (Mishra et al., 2019; Sidorov et al., 2020), region-level VQA (Kazemzadeh et al., 2014; Krishna et al., 2017; Mao et al., 2016), visual conversation (Liu et al., 2023a), and language conversation data. We follow the setting of (Liu et al., 2023a) to filter the training data and construct the tunning prompt format. For a fair comparison, DoRA follows the same configuration as the LoRA configuration provided by (Liu et al., 2023a). The fine-tuned models are then evaluated on seven vision- language benchmarks: VQAv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019), VisWiz (Gurari et al., 2018) SQA (Lu et al., 2022), VQAT (Singh et al., 2019), POPE (Li et al., 2023), and MMBench (Liu et al., 2023c). From Table 4, we can observe that the average accuracy of LoRA already surpasses FT, which could imply that FT might be experiencing issues with overfitting. Given that DoRA is designed to enhance LoRA’s performance to more closely resemble that of FT, in scenarios where FT is infe- rior to LoRA, DoRA’s improvement over LoRA might not be as pronounced as observed in other experiments where FT usually outperforms LoRA. Nonetheless, DoRA still demonstrates superior performance over both LoRA and FT, with an average improvement of 0.7% over LoRA and 1.1% over FT. See Table 10 for the hyperparameters setting and Table 12 for the score of each evaluation benchmark. 5.4. Compatibility of DoRA with other LoRA variants Recall from Equation.(1) that ∆W can be adapted by differ- ent LoRA variants. With DoRA, the concept of incremental directional update ∆V introduced in Equation.(5) can like- wise be replaced with alternative LoRA variants. In this section, we select VeRA (Kopiczko et al., 2024) as a case study to explore DoRA’s compatibility with other LoRA variants. VeRA suggests freezing a unique pair of random low-rank matrices to be shared across all layers, employ- ing only minimal layer-specific trainable scaling vectors to capture each layer’s incremental updates. This approach allows VeRA to reduce trainable parameters significantly 7DoRA: Weight-Decomposed Low-Rank Adaptation Table 5.Average scores on MT-Bench assigned by GPT-4 to the answers generated by fine-tuned LLaMA-7B/LLaMA2-7B. Model PEFT Method # Params (%) Score LLaMA-7B LoRA 2.31 5.1 DoRA (Ours) 2.33 5.5 VeRA 0.02 4.3 DV oRA (Ours) 0.04 5.0 LLaMA2-7B LoRA 2.31 5.7 DoRA (Ours) 2.33 6.0 VeRA 0.02 5.5 DV oRA (Ours) 0.04 6.0 by 10x compared to LoRA, with only a minimal impact on accuracy. We apply VeRA for the directional update in DoRA and name such combination DV oRA. We assess the effectiveness of both DV oRA and DoRA compared to VeRA and LoRA across LLaMA-7B and LLaMA2-7B, focusing on instruction tuning with the 10K subset of cleaned Alpaca dataset (Taori et al., 2023). We utilize the official imple- mentation of VeRA to obtain the results of VeRA and LoRA and fine-tune the model with DV oRA and DoRA using the identical training settings as VeRA and LoRA (see Table 11 in the appendix for more details). The performance of the fine-tuned models is then evaluated on the MT-Bench bench- mark (Zheng et al., 2023) by generating model responses to a pre-defined set of 80 multi-turn questions. These re- sponses are then evaluated by GPT-4, which reviews each answer and assigns a numerical score out of 10. Table 5 presents the average scores for DV oRA, DoRA, VeRA, and LoRA, demonstrating that our proposed method exhibits consistent improvements over VeRA and LoRA for both LLaMA-7B and LLaMA2-7B. This effectively show- cases the compatibility of DoRA with VeRA. In particular, DV oRA merges the advantageous qualities of DoRA and VeRA, attaining scores that are on par with or even sur- pass those of LoRA, yet with significantly fewer parame- ters. For example, DV oRA outperforms VeRA by 0.7/0.5 points and achieves the same level of accuracy as LoRA on LLaMA-7B and DoRA on LLaMA2-7B, respectively. Ad- ditionally, we present a selection of questions chosen from MT-Bench, accompanied by the responses from LLaMA2- 7B fine-tuned using DV oRA and VeRA in the appendix (Table 13 and 14) where we can observe that the answers given by DV oRA tend to be more precise and structural. Next, to further assess DoRA’s ability to remain competitive under varying amounts of training data, considering that in practical situations, access to extensive fine-tuning datasets is frequently limited. We compare DoRA to LoRA and DV oRA to VeRA for fine-tuning LLaMA2-7B/LLaMA-7B with a range of instruction-tuning sample sizes, specifically 1000, 4000, 7000, 10000, with 10000 being the setting of 1000 4000 7000 10000 Number of instruction tuning training samples 5.0 5.2 5.4 5.6 5.8 6.0Score LLaMA2-7B DoRA LoRA DV oRA VeRA Figure 4.Performance of fine-tuned LLaMA2-7B on MT-Bench using different numbers of Alpaca training samples. (Kopiczko et al., 2024). We visualize the average perfor- mance of each method on LLaMA2-7B in Figure 4, and on LLaMA-7B in Figure 9 in the appendix. The result shows that DoRA and DV oRA consistently outperform LoRA and VeRA across all training sample sizes. For instance, with 7000 training samples, DoRA and DV oRA surpass LoRA and VeRA by margins of 0.3 and 0.33, respectively. Even when the sample size is reduced to 1000, DoRA and DV oRA maintain their lead with advantages of 0.29 and 0.22 over LoRA and VeRA, respectively. This demonstrates that our methods persistently enhance performance over LoRA and VeRA, regardless of the training sample volume. 5.5. Robustness of DoRA towards different rank settings 4 8 16 32 64 rank r 40 45 50 55 60 65 70 75 80Avg. Accuracy (+ 37.2%) (+ 22.4%) LLaMA-7B DoRA LoRA Figure 5.Average accuracy of LoRA and DoRA for varying ranks for LLaMA-7B on the commonsense reasoning tasks. This section explores the impact of various rank configura- tions on DoRA and LoRA by adjustingr within the set {4, 8, 16, 32, 64} and assessing the performance of the fine-tuned LLaMA-7B on commonsense reasoning tasks as outlined in Sec 5.1. The average accuracies of LoRA and DoRA across different ranks are depicted in Figure 5, with detailed num- bers presented in Table 15. From Figure 5, we can observe that DoRA consistently surpasses LoRA across all rank con- figurations. Notably, the performance gap widens for ranks 8DoRA: Weight-Decomposed Low-Rank Adaptation Table 6.Accuracy comparison of LLaMA 7B/13B with two differ- ent tuning granularity of DoRA. Columns m and V designate the modules with tunable magnitude and directional components, re- spectively. Each module is represented by its first letter as follows: (Q)uery, (K)ey, (V)alue, (O)utput, (G)ate, (U)p, (D)own. Model PEFT Method # Params (%) m V Avg. LLaMA-7B LoRA 0.83 - - 74.7 DoRA (Ours) 0.84 QKVUD QKVUD 78.1 DoRA (Ours) 0.39 QKVOGUD QKV 77.5 LLaMA-13B LoRA 0.67 - - 80.5 DoRA (Ours) 0.68 QKVUD QKVUD 81.5 DoRA (Ours) 0.31 QKVOGUD QKV 81.3 below 8, where LoRA’s average accuracies drop to 40.74% for r = 8 and 39.49% for r = 4. In contrast, DoRA retains a notable accuracy of 77.96% for r = 8 and 61.89% for r = 4, demonstrating its resilience and consistently superior performance over LoRA regardless of the rank setting. 5.6. Tuning Granularity Analysis The visualization in Figure 2 indicates that significant changes in magnitude often result in relatively smaller di- rectional changes. Given this observation and the fact that directional updates account for most of the trainable param- eters, it prompts an investigation into whether it is possible to decrease the number of trainable parameters by updating only the magnitude components of specific modules while continuing to update both the magnitude and directional components for the remaining linear modules. Our findings indicate that, in contrast to the original con- figuration suggested for LoRA in (Hu et al., 2023), which requires updates to both the Multi-head Attention and MLP layers for optimal performance, DoRA can already achieve superior accuracy by updating only the directional and mag- nitude components of the multi-head layers and the magni- tude of the MLP layers. Specifically, as shown in Table 6, by updating the directional and magnitude components of the QKV modules and only the magnitude of the rest of the layers, DoRA surpasses LoRA by 2.8% on LLaMA-7B and 0.8% on LLaMA-13B, while utilizing only less than half of the trainable parameters compared to LoRA. 6. Broader Impacts 6.1. QDoRA: Enhancements to QLoRA While finetuning LLMs with PEFT significantly reduces training memory overhead, a considerable amount of GPU memory is still required to initially load the model weights onto the GPUs. To further decrease the memory demands of finetuning, QLoRA (Dettmers et al., 2023) suggests quan- tizing the pretrained model to 4-bit and finetuning LoRA on top of the frozen low-bit backbone. With our porposed 0.0 0.1 0.2 0.3 0.4 0.5 Exact match score (Eval size: 500) Zero-shot Five-shot Full Finetune QLoRA QDoRA 0.23 0.27 0.51 0.32 0.56 0.07 0.08 0.26 0.12 0.31 100k Orca-Math finetuning results LLaMA2-7B LLaMA3-8B Figure 6.Accuracy comparison of LLaMA2-7B/LLaMA3-8B with QDoRA, QLoRA and FT on Orca-Math (Mitra et al., 2024). DoRA, which narrows the gap between LoRA and FT, it is natural to also explore whether DoRA can enhance the accuracy of LoRA within the QLoRA framework. Recently, (Kerem Turgutlu, 2024) launch a project that substitutes the LoRA component in QLoRA with DoRA, dubbing it QDoRA, and incorporate the training pipeline with Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023) to enable model splitting and parallel training across multiple GPUs. They conducted experiments on fine-tuning LLaMA2- 7B/LLaMA3-8B using the Orca-Math(Mitra et al., 2024) dataset with QDoRA, QLoRA, and FT. The training set included 100k samples, with 500 reserved for evaluation using the exact match score as the metric. In addition to the fine-tuned models, they also reported results from zero-shot, few-shot, and FT with post-training quantization (PTQ), where the FT model is quantized to the BnB NF4 format after training. According to Figure 6, QDoRA not only significantly surpasses QLoRA by 0.19/0.23 on LLaMA2- 7B and LLaMA3-8B, but it also slightly outperforms FT on both models, while using considerably less memory. This in- dicates that QDoRA can effectively combines the parameter efficiency of QLoRA with the more granular optimization of full finetuning. These initial findings suggest that QDoRA holds considerable promise and could hugely benefit the opensoure community by substantially lowering the GPU memory requirements for fine-tuning large language mod- els. 6.2. Text-to-Image Generation Recently, as diffusion models have expanded in size, LoRA has become a popular method for efficiently fine-tuning large stable diffusion models. In this section, we aim to explore whether DoRA’s advantages over LoRA extend to the task of text-to-image generation. We follow the training pipeline of DreamBooth (Ruiz et al., 2023) for fine-tuning 9DoRA: Weight-Decomposed Low-Rank Adaptation SDXL (Podell et al., 2023), utilizing the advanced train- ing scripts developed by HuggingFace. The hyperparameter settings for LoRA and DoRA are kept the same, and we fine- tune the model using two challenging datasets: 3D icons and Lego sets. The sample seeds for generating the images are kept the same for LoRA and DoRA for fair comparison. The generated images are shown in Figure 10 and 11 in the appendix. The results indicate that DoRA achieves signif- icantly better personalization than LoRA when using the same training settings, and more accurately reflects the train- ing targets. For example, in Figure 10, the first sub-figure of DoRA’s output features a unique round square around the image, which is a feature common to all the training targets. In contrast, this feature is absent in all the LoRA outputs. A similar observation could be found with the Lego training targets, where only the DoRA outputs consistently incorporate the Lego logo in the generated images. 7. Conclusion In this work, we first conduct a novel weight decomposi- tion analysis to reveal the distinct learning patterns between LoRA and FT. Building on these insights, we introduce DoRA, a fine-tuning method that is compatible with LoRA and its variants and exhibits a closer resemblance to FT’s learning behavior. DoRA consistently outperforms LoRA across various fine-tuning tasks and model architectures. Specifically, DoRA improves upon LoRA in commonsense reasoning and visual instruction tuning tasks. Furthermore, DoRA also shows compatibility with VeRA on the Alpaca instruction tuning task. Moreover, DoRA can be considered as a costless alternative to LoRA, as its decomposed mag- nitude and direction components can be merged back into the pre-trained weight after the training, ensuring that there is no extra inference overhead. For future work, we wish to explore the generalizability of DoRA in domains beyond language and vision, particularly in the field of audio. Acknowledgements We extend our gratitude to Benjamin Bossan, Younes Belkada, and Sourab Mangrulkar from Hugging Face for their assistance in integrating DoRA into the PEFT pack- age, thus making our work more accessible to the broader public. We thank Kerem Turgutlu, Jonathan Whitaker, and Jeremy Howard from Answer.AI for their work on the imple- mentation and experiments of QDoRA/FSDP, which makes fine-tuning of large language models with DoRA on con- sumer GPUs a lot more feasible. We also thank Sebastian Raschka for his well-written tutorial on DoRA which offers a thorough overview of the background knowledge neces- sary to comprehend DoRA. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Chen, X., Fang, H., Lin, T.-Y ., Vedantam, R., Gupta, S., Doll´ar, P., and Zitnick, C. L. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. In Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 10088–10115. Cur- ran Associates, Inc., 2023. Goyal, Y ., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the v in vqa matter: Elevating the role of image understanding in visual question answer- ing. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6904–6913, 2017. Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., and Bigham, J. P. Vizwiz grand challenge: Answering visual questions from blind people. In Pro- ceedings of the IEEE conference on computer vision and pattern recognition, pp. 3608–3617, 2018. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., and Neubig, G. Towards a unified view of parameter-efficient trans- fer learning. In International Conference on Learning Representations, 2021. He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on 10DoRA: Weight-Decomposed Low-Rank Adaptation imagenet classification. In Proceedings of the IEEE Inter- national Conference on Computer Vision, pp. 1026–1034, 2015. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning , pp. 2790–2799, 2019. Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. Hu, Z., Wang, L., Lan, Y ., Xu, W., Lim, E.-P., Bing, L., Xu, X., Poria, S., and Lee, R. LLM-adapters: An adapter family for parameter-efficient fine-tuning of large lan- guage models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. Hudson, D. A. and Manning, C. D. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 6700– 6709, 2019. Hyeon-Woo, N., Ye-Bin, M., and Oh, T.-H. Fedpara: Low- rank hadamard product for communication-efficient fed- erated learning. In International Conference on Learning Representations, 2022. Karimi Mahabadi, R., Ruder, S., Dehghani, M., and Hen- derson, J. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. In Proceedings of the 59th Annual Meeting of the Association for Com- putational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 565–576, 2021. Kazemzadeh, S., Ordonez, V ., Matten, M., and Berg, T. ReferItGame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 787–798, 2014. Kerem Turgutlu, Jonathan Whitaker, J. H. Ef- ficient finetuning of llama 3 with fsdp qdora. https://www.answer.ai/posts/ 2024-04-26-fsdp-qdora-llama3.html , 2024. Kopiczko, D. J., Blankevoort, T., and Asano, Y . M. Vera: Vector-based random matrix adaptation. In International Conference on Learning Representations, 2024. Krishna, R., Zhu, Y ., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y ., Li, L.-J., Shamma, D. A., et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. In- ternational journal of computer vision, pp. 32–73, 2017. Lei, J., Yu, L., Bansal, M., and Berg, T. TVQA: Localized, compositional video question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1369–1379, 2018. Lei, J., Yu, L., Berg, T. L., and Bansal, M. Tvr: A large-scale dataset for video-subtitle moment retrieval. In European Conference on Computer Vision, pp. 447–463, 2020. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, 2021. Lewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mo- hamed, A., Levy, O., Stoyanov, V ., and Zettlemoyer, L. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehen- sion. In Proceedings of the 58th Annual Meeting of the As- sociation for Computational Linguistics, pp. 7871–7880, 2020. Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Confer- ence on Machine Learning, pp. 12888–12900, 2022. Li, L., Chen, Y .-C., Cheng, Y ., Gan, Z., Yu, L., and Liu, J. HERO: Hierarchical encoder for Video+Language omni- representation pre-training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2046–2065, 2020. Li, L., Lei, J., Gan, Z., Yu, L., Chen, Y .-C., Pillai, R., Cheng, Y ., Zhou, L., Wang, X. E., Wang, W. Y ., et al. Value: A multi-task benchmark for video-and-language understand- ing evaluation. In Thirty-fifth Conference on Neural In- formation Processing Systems Datasets and Benchmarks Track (Round 1), 2021. Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582– 4597, 2021. Li, Y ., Du, Y ., Zhou, K., Wang, J., Zhao, X., and Wen, J.-R. Evaluating object hallucination in large vision-language 11DoRA: Weight-Decomposed Low-Rank Adaptation models. In Proceedings of the 2023 Conference on Em- pirical Methods in Natural Language Processing , pp. 292–305, 2023. Liu, H., Li, C., Wu, Q., and Lee, Y . J. Visual instruction tun- ing. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. Liu, W., Qiu, Z., Feng, Y ., Xiu, Y ., Xue, Y ., Yu, L., Feng, H., Liu, Z., Heo, J., Peng, S., et al. Parameter-efficient orthogonal finetuning via butterfly factorization. arXiv preprint arXiv:2311.06243, 2023b. Liu, Y ., Duan, H., Zhang, Y ., Li, B., Zhang, S., Zhao, W., Yuan, Y ., Wang, J., He, C., Liu, Z., et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023c. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Infor- mation Processing Systems, pp. 2507–2521, 2022. mahabadi, R. K., Henderson, J., and Ruder, S. Compacter: Efficient low-rank hypercomplex adapter layers. In Ad- vances in Neural Information Processing Systems, 2021. Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A. L., and Murphy, K. Generation and comprehension of unam- biguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 11–20, 2016. Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. Ok-vqa: A visual question answering benchmark requir- ing external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pp. 3195–3204, 2019. Mishra, A., Shekhar, S., Singh, A. K., and Chakraborty, A. Ocr-vqa: Visual question answering by reading text in images. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pp. 947–952, 2019. Mitra, A., Khanpour, H., Rosset, C., and Awadallah, A. Orca-math: Unlocking the potential of slms in grade school math, 2024. OpenAI. Welcome to the openai platform, 2023. URL https://platform.openai.com/docs/ introduction. Peng, B., Li, C., He, P., Galley, M., and Gao, J. Instruc- tion tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., M ¨uller, J., Penna, J., and Rombach, R. Sdxl: Im- proving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Ponti, E. M., Sordoni, A., Bengio, Y ., and Reddy, S. Com- bining modular skills in multitask learning.arXiv preprint arXiv:2202.13914, 2022. Qin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., and Yang, D. Is chatgpt a general-purpose natural language processing task solver? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1339–1384, 2023. Qiu, Z., Liu, W., Feng, H., Xue, Y ., Feng, Y ., Liu, Z., Zhang, D., Weller, A., and Sch¨olkopf, B. Controlling text- to-image diffusion by orthogonal finetuning. In Thirty- seventh Conference on Neural Information Processing Systems, 2023. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748–8763, 2021. Razdaibiedina, A., Mao, Y ., Khabsa, M., Lewis, M., Hou, R., Ba, J., and Almahairi, A. Residual prompt tuning: improving prompt tuning with residual reparameteriza- tion. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 6740–6757, 2023. Renduchintala, A., Konuk, T., and Kuchaiev, O. Tied-lora: Enhacing parameter efficiency of lora with weight tying. arXiv preprint arXiv:2311.09578, 2023. Ruiz, N., Li, Y ., Jampani, V ., Pritch, Y ., Rubinstein, M., and Aberman, K. Dreambooth: Fine tuning text-to-image dif- fusion models for subject-driven generation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22500–22510, 2023. Salimans, T. and Kingma, D. P. Weight normalization: a simple reparameterization to accelerate training of deep neural networks. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pp. 901–909, 2016. Schwenk, D., Khandelwal, A., Clark, C., Marino, K., and Mottaghi, R. A-okvqa: A benchmark for visual ques- tion answering using world knowledge. In European Conference on Computer Vision, pp. 146–162, 2022. Sidorov, O., Hu, R., Rohrbach, M., and Singh, A. Textcaps: a dataset for image captioning with reading comprehen- sion. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceed- ings, Part II 16, pp. 742–758, 2020. 12DoRA: Weight-Decomposed Low-Rank Adaptation Singh, A., Natarajan, V ., Shah, M., Jiang, Y ., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8317–8326, 2019. Suhr, A., Zhou, S., Zhang, A., Zhang, I., Bai, H., and Artzi, Y . A corpus for reasoning about natural language grounded in photographs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6418–6428, 2019. Sung, Y .-L., Cho, J., and Bansal, M. Vl-adapter: Parameter- efficient transfer learning for vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition , pp. 5227–5237, 2022. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y ., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stan- ford alpaca: An instruction-following llama model, 2023. URL https://github.com/tatsu-lab/ stanford_alpaca. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan- guage models. arXiv preprint arXiv:2302.13971, 2023. Wang, Y ., Wu, J., Dabral, T., Zhang, J., Brown, G., Lu, C.-T., Liu, F., Liang, Y ., Pang, B., Bendersky, M., et al. Non- intrusive adaptation: Input-centric parameter-efficient fine-tuning for versatile multimodal modeling. arXiv preprint arXiv:2310.12100, 2023. Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le, Q. V ., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, 2022. Yeh, S.-Y ., Hsieh, Y .-G., Gao, Z., Yang, B. B., Oh, G., and Gong, Y . Navigating text-to-image customization: From lycoris fine-tuning to model evaluation. arXiv preprint arXiv:2309.14859, 2023. Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y ., Chen, W., and Zhao, T. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh Interna- tional Conference on Learning Representations, 2023. Zhao, Y ., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Zheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing, E., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. Zhou, L., Xu, C., and Corso, J. Towards automatic learning of procedures from web instructional videos. In Proceed- ings of the AAAI Conference on Artificial Intelligence , 2018. 13DoRA: Weight-Decomposed Low-Rank Adaptation A. Appendix A.1. Weight decomposition analysis on the value weight matrix In this section, we illustrate the changes in magnitude and direction within the value weight matrix for FT, LoRA, and DoRA across different training steps and layers, as shown in Figure 7. This reveals patterns similar to those seen in the query weight matrix depicted in Figure 2, indicating that DoRA is capable of displaying learning behaviors that closely mirror those of FT across various modules. 0.02 0.03 0.04 0.05 0.06 0.07 0.08 D (a) 0.020 0.025 0.030 0.035 0.040 0.045 0.050 0.055 M FT layer 1 layer 2 layer 3 layer 4 layer 5 layer 6 Inter step 1 Inter step 2 Inter step 3 Final step 0.150 0.175 0.200 0.225 0.250 0.275 0.300 0.325 D (b) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 LoRA Inter step 1 Inter step 2 Inter step 3 Final step 0.20 0.25 0.30 0.35 0.40 0.45 0.50 D (c) 0.4 0.6 0.8 1.0 DoRA Inter step 1 Inter step 2 Inter step 3 Final step Figure 7.Magnitude and Directional changes of FT (a), LoRA (b), and DoRA (c) of the V weight matrices across different layers and intermediate steps. A.2. Ablation study for the modification to reduce DoRA training cost Table 7 presents the GPU cost and the average accuracy of DoRA with and without the proposed modification for commonsense reasoning tasks and image-text understanding tasks. The results indicate that the modification leads to a training memory reduction of approximately 24.4% in fine-tuning LLaMA and 12.4% in VL-BART. Furthermore, the accuracy of DoRA with the modification remains unchanged for VL-BART and shows a negligible difference of only 0.2 compared to DoRA without the modification on LLaMA. Table 7.GPU cost and accuracy of DoRA with or without the modification on the commonsense reasoning tasks and image-text understanding tasks. Model PEFT Method Accumulation steps Batch Size GPU Memory Cost (GB) # Params (%) Avg. LLaMA-7B DoRA w/o modification 4 16 37.3 0.84 78.3 DoRA 28.2 (-24.4%) 0.84 78.1 VL-BART DoRA w/o modification - 300 23.4 5.96 77.3 DoRA 20.5 (-12.4%) 5.96 77.4 14DoRA: Weight-Decomposed Low-Rank Adaptation A.3. Hyperparameters Table 8.Hyperparameter configurations of DoRA for LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B on the commonsense reasoning tasks. Hyperparameters (DoRA) LLaMA-7B LLaMA-13B LLaMA2-7B LLaMA3-8B Rank r 16 32 16 32 16 32 16 32 α 32 64 32 64 32 64 32 64 Dropout 0.05 Optimizer AdamW LR 2e-4 1e-4 3e-4 2e-4 2e-4 2e-4 1e-4 1e-4 LR Scheduler Linear Batch size 16 Warmup Steps 100 Epochs 3 Where Q,K,V ,Up,Down Table 9.Hyperparameter configurations of DoRA for fine-tuning VL-Bart on image/video-text tasks. Hyperparameters (DoRA) image-text video-text Rank r 128 α 128 Dropout 0.0 Optimizer AdamW LR 1e-3 3e-4 LR Scheduler Linear Batch size 300 40 Warmup ratio 0.1 Epochs 20 7 Where Q,K Table 10.Hyperparameter configurations of DoRA and LoRA for fine-tuning LLaV A-1.5-7B with visual instruction tuning datasets. Hyperparameters DoRA LoRA Rank r 128 α 256 Dropout 0.05 Optimizer AdamW LR 2e-4 LR Scheduler Cosine decay Batch size 16 Warmup ratio 0.03 Epochs 1 Where Q,K,V ,O,Up,Down,Gate 15DoRA: Weight-Decomposed Low-Rank Adaptation Table 11.Hyperparameter configurations of DoRA and DV oRA for fine-tuning LLaMA-7B and LLaMA2-7B with cleaned Alpaca dataset. Hyperparameters (DoRA) LLaMA-7B LLaMA2-7B Rank r 64 Dropout 0.0 Optimizer AdamW LR 4e-4 LR Scheduler Cosine Batch size 4 Accumulation Steps 4 Warmup ratio 0.1 Epochs 1 Where Q,K,V ,O,Up,Down,Gate Hyperparameters (DVoRA) LLaMA-7B LLaMA2-7B Rank r 1024 Dropout 0.0 Optimizer AdamW LR 4e-3 LR Scheduler Cosine Batch size 4 Accumulation Steps 4 Warmup ratio 0.1 Epochs 1 Where Q,K,V ,O,Up,Down,Gate 16DoRA: Weight-Decomposed Low-Rank Adaptation A.4. Magnitude and Direction difference between DoRA/LoRA fine-tuned weight and the pre-triained weight of LLaMA2-7B for the commonsesne reasoning tasks Figure 8 depicts the magnitude and direction differences in the weights of the query, key, and value matrices between LoRA/DoRA fine-tuned models and the pre-trained model across various layers of LLaMA2-7B for the commonsense reasoning tasks. The figure shows that the DoRA fine-tuned weights deviate less from the pre-trained weights in both magnitude and direction, supporting our hypothesis that a robust foundation model does not need substantial changes for effective downstream adaptation. (a) (b) (d) (c) (f) (e) Figure 8.Magnitude and direction differences in the weights of the query, key, and value matrices between LoRA/DoRA fine-tuned models and the pre-trained model across various layers of LLaMA2-7B for the commonsense reasoning tasks. 17DoRA: Weight-Decomposed Low-Rank Adaptation A.5. Visual instruction tuning evaluation result Table 12 presents the evaluation result of LLaV A-1.5-7B fine-tuned using DoRA, LoRA, and FT with visual instruction tuning data. The evaluation is conducted on an array of seven different vision-language benchmarks. From the table, we can see that DoRA consistently outmatch FT, and can achieve an average accuracy better than LoRA. Table 12.Visual instruction tuning evaluation result of DoRA, LoRA, and FT for LLaV A-1.5-7B on a wide range of 7 vision-language tasks. Method # Params(%) VQAv2 GQA VisWiz SQA VQA T POPE MMBench Avg. FT 100 78.5 61.9 50.0 66.8 58.2 85.9 64.3 66.5 LoRA 4.61 79.1 62.9 47.8 68.4 58.2 86.4 66.1 66.9 DoRA (Ours) 4.63 78.6 62.9 52.2 69.9 57 87.2 66.1 67.6 18DoRA: Weight-Decomposed Low-Rank Adaptation A.6. Instruction-tuning evaluation examples We sample two questions from the MT-Bench question set and present the respective responses from DV oRA and VeRA, including the scores and comments provided by GPT-4. From these examples, we can observe that responses from DV oRA are generally more accurate and well-organized compared to those from VeRA. For instance, in response to Question 24, DV oRA leverages a well-known superhero story that is familiar to children, whereas VeRA’s response begins with an uncertain tone, potentially making it less engaging than DV oRA’s reply. Table 13.Question 24 from MT-Bench alongside with response from LLaMA2-7B fine-tuned with DV oRA and VeRA, and the evaluation provided by GPT-4. Question 24 As a superhero, how would you explain your origin story to a curious child? VeRA Well, I’m not really sure where I come from. All I know is that one day, I woke up with special powers and abilities that allowed me to help people. I decided to use my powers to fight crime and protect the innocent, and that’s how I became a superhero DV oRA As a superhero, I have the ability to use my super powers to protect people and fight against evil. My origin story begins when I was just a regular kid like you, but one day, I was bitten by a radioactive spider, and it gave me incredible powers. I could climb walls, shoot webs, and see in the dark. I decided to use my powers to help people, and that’s how I became your friendly neighborhood Spider-Man. GPT-4 Evaluation Score: 5 7. Assistant 1’s response starts with an uncertain tone, which might not be very engaging for a child. The explanation is quite vague and doesn’t offer a clear or imaginative origin story that could captivate a child’s interest. It does, however, maintain the theme of using powers for good, which is a positive message. Assistant 2’s response is more detailed and uses a familiar superhero origin story (Spider-Man’s) that many children can relate to and understand. It’s engaging and provides a clear narrative of how an ordinary person can become a superhero. However, it directly references Spider-Man, which is not an original origin story and could be misleading if the assistant is supposed to be a unique superhero. Despite this, the response is more likely to resonate with a child due to its clarity and reliability. Both responses could be improved by providing an original and imaginative origin story that doesn’t rely on existing superhero narratives. They should also ensure that the story is simple enough for a child to understand while being engaging and inspiring. 19DoRA: Weight-Decomposed Low-Rank Adaptation Table 14.Question 33 from MT-Bench alongside with response from LLaMA2-7B fine-tuned with DV oRA and VeRA, and the evaluation provided by GPT-4. Question 33 Why might someone choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app? VeRA There are several reasons why someone might choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app. Some people may prefer the physical act of navigating with a paper map, as it can be more tactile and interactive. Others may prefer to have a hard copy of the map in case the GPS or app fails or the battery dies. Additionally, some people may not have access to a GPS device or smartphone, or may not want to rely on technology for navigation. Finally, some people may simply prefer the security and peace of mind that comes with knowing exactly where they are and how to get to their destination DV oRA There are several reasons why someone might choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app. Here are some of them: 1. Navigation accuracy: GPS devices and smartphone apps rely on satellite signals to pinpoint your location, but this technology is not foolproof. GPS signals can be blocked by tall buildings, tunnels, or other obstacles, resulting in inaccurate or missing information. Paper maps, on the other hand, are based on accurate, up-to-date information that is not subject to these limitations. 2. Cost: GPS devices and smartphone apps can be expensive to purchase and maintain. Paper maps are free to use and do not require an ongoing subscription or data plan. 3. Personal preference: Some people prefer the tactile experience of using a paper map, which can be easier to read and navigate than a screen. Others enjoy the challenge of navigating without technology, which can help them learn more about the area they are visiting. 4. Battery life: GPS devices and smartphone apps can drain batteries quickly, especially when used for navigation. Paper maps do not require power, so they can be used for extended periods without recharging. 5. Offline use: GPS devices and smartphone apps rely on an internet connection to function properly. If there is no signal, the device will not be able to provide accurate information. Paper maps, on the other hand, can be used offline, making them ideal for use in remote or unpopulated areas. In summary, there are several reasons why someone might choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app. These include navigation accuracy, cost, personal preference, battery life, and offline use. GPT-4 Evaluation Score: 6 8. Assistant 1’s response is somewhat brief and lacks specific details that could be helpful to the user. It mentions the tactile nature of paper maps, the possibility of technology failure, lack of access to technology, and a general preference for security and peace of mind. However, it does not delve into the practical considerations such as GPS signal limitations, cost, battery life, or offline use, which are important factors in deciding between paper maps and digital navigation tools. Assistant 2’s response is more comprehensive and provides a structured list of reasons, including navigation accuracy, cost, personal preference, battery life, and offline use. Each point is explained clearly, giving the user a better understanding of the potential advantages of paper maps over GPS devices or smartphone apps. The response is relevant, accurate, and detailed, making it more helpful to the user. Both assistants provided relevant and accurate information, but Assistant 2’s answer is more detailed and structured, which is why it receives a higher score. The scores reflect the level of detail and the usefulness of the information provided to the user, with Assistant 2 offering a more complete answer to the question 20DoRA: Weight-Decomposed Low-Rank Adaptation A.7. Robustness towards different numbers of instruction tunning examples Figure 9 depicts the score of DoRA, LoRA, DV oRA, and VeRA for fine-tuning LLaMA-7B with varying instruction-tuning sample sizes, including 1000, 4000, 7000, and 10000. From the figure, we can observe that similar to that of the result on LLaMA2-7B, both DoRA and DV oRA consistently outperform LoRA and VeRA across the entire range of training sample sizes. 1000 4000 7000 10000 Number of instruction tuning training samples 3.0 3.5 4.0 4.5 5.0 5.5Score LLaMA-7B DoRA LoRA DV oRA VeRA Figure 9.Performance of LLaMA-7B fine-tuned with LoRA, DoRA, VeRA, and DV oRA on MT-Bench using different numbers of training samples from Alpaca dataset (Taori et al., 2023). 21DoRA: Weight-Decomposed Low-Rank Adaptation A.8. Robustness of DoRA towards different rank Table 15 shows a comparison of the average accuracy between LoRA and DoRA method across various rank settings for commonsense reasoning tasks. DoRA consistently outperforms LoRA at all rank settings, with the performance gap widening as the rank decreases. This suggests that our method effectively enhances the learning capacity of LoRA, enabling it to achieve better accuracy with fewer trainable parameters. Table 15.Accuracy comparison of LoRA and DoRA with varying ranks for LLaMA-7B on the commonsense reasoning tasks. PEFT Method rank r # Params (%) BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg. LoRA 4 0.10 2.3 46.1 18.3 19.7 55.2 65.4 51.9 57 39.5 8 0.21 31.3 57.0 44.0 11.8 43.3 45.7 39.2 53.8 40.7 16 0.42 69.9 77.8 75.1 72.1 55.8 77.1 62.2 78.0 70.9 32 0.83 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7 64 1.64 66.7 79.1 75.7 17.6 78.8 73.3 59.6 75.2 65.8 DoRA (Ours) 4 0.11 51.3 42.2 77.8 25.4 78.8 78.7 62.5 78.6 61.9 8 0.22 69.9 81.8 79.7 85.2 80.1 81.5 65.7 79.8 77.9 16 0.43 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5 32 0.84 69.7 83.4 78.6 87.2 81.0 81.9 66.2 79.2 78.4 64 1.65 69.9 81.4 79.1 40.7 80.0 80.9 65.5 79.4 72.1 22DoRA: Weight-Decomposed Low-Rank Adaptation A.9. Text-to-Image Generation Figures 10 and 11 show the images produced by SDXL fine-tuned with DoRAand LoRA via DreamBooth (Ruiz et al., 2023) personalization techniques on two distinct training sets: 3D Icon 1 and Lego2. The results reveal that DoRA can achieve considerably better personalization than LoRA with identical training configurations, more closely matching the training target. Figure 10.Images generated with SDXL finetuned with LoRA and DoRA on the 3D Icon training sets. Figure 11.Images generated with SDXL finetuned with LoRA and DoRA on the Lego training sets. 1https://huggingface.co/datasets/linoyts/3d_icon 2https://huggingface.co/datasets/merve/lego_sets_latest 23",
      "meta_data": {
        "arxiv_id": "2402.09353v6",
        "authors": [
          "Shih-Yang Liu",
          "Chien-Yi Wang",
          "Hongxu Yin",
          "Pavlo Molchanov",
          "Yu-Chiang Frank Wang",
          "Kwang-Ting Cheng",
          "Min-Hung Chen"
        ],
        "published_date": "2024-02-14T17:59:34Z",
        "pdf_url": "https://arxiv.org/pdf/2402.09353v6.pdf"
      }
    },
    {
      "title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
      "abstract": "Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA\nand its variants have gained considerable popularity because of avoiding\nadditional inference costs. However, there still often exists an accuracy gap\nbetween these methods and full fine-tuning (FT). In this work, we first\nintroduce a novel weight decomposition analysis to investigate the inherent\ndifferences between FT and LoRA. Aiming to resemble the learning capacity of FT\nfrom the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA).\nDoRA decomposes the pre-trained weight into two components, magnitude and\ndirection, for fine-tuning, specifically employing LoRA for directional updates\nto efficiently minimize the number of trainable parameters. By employing \\ours,\nwe enhance both the learning capacity and training stability of LoRA while\navoiding any additional inference overhead. \\ours~consistently outperforms LoRA\non fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as\ncommonsense reasoning, visual instruction tuning, and image/video-text\nunderstanding. Code is available at https://github.com/NVlabs/DoRA.",
      "full_text": "DoRA: Weight-Decomposed Low-Rank Adaptation Shih-Yang Liu1 2 Chien-Yi Wang1 Hongxu Yin 1 Pavlo Molchanov1 Yu-Chiang Frank Wang1 Kwang-Ting Cheng 2 Min-Hung Chen 1 Abstract Among the widely used parameter-efficient fine- tuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decom- position analysis to investigate the inherent dif- ferences between FT and LoRA. Aiming to re- semble the learning capacity of FT from the findings, we propose Weight-Decomposed Low- Rank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, mag- nitude and direction, for fine-tuning, specifically employing LoRA for directional updates to ef- ficiently minimize the number of trainable pa- rameters. By employing DoRA, we enhance both the learning capacity and training stabil- ity of LoRA while avoiding any additional in- ference overhead. DoRA consistently outper- forms LoRA on fine-tuning LLaMA, LLaV A, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruc- tion tuning, and image/video-text understanding. Code is available at https://github.com/ NVlabs/DoRA. 1. Introduction Models that are pre-trained with extensive general domain datasets have demonstrated remarkable generalization abil- ities, significantly benefiting a wide array of applications, from natural language processing (NLP) tasks (Qin et al., 2023; Taori et al., 2023) to multi-modal tasks (Li et al., 2022; Liu et al., 2023a). To tailor these general models for spe- cific downstream tasks, full fine-tuning (FT) is commonly 1NVIDIA 2HKUST. Correspondence to: Shih-Yang Liu <shi- hyangl@nvidia.com, sliuau@connect.ust.hk>, Min-Hung Chen <minhungc@nvidia.com>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). Magnitude B  A  Pretrained Weight Merged Weight Pretrained Weight Pretrained Weight Adapt Frozen Trainable Magnitude Direction Direction Decompose (Initialize) Merge Figure 1.An overview of our proposed DoRA, which decomposes the pre-trained weight into magnitude and direction components for fine-tuning, especially with LoRA to efficiently update the direction component. Note that || · ||c denotes the vector-wise norm of a matrix across each column vector. employed, involving the retraining of all model parameters. Nevertheless, as the size of models and datasets expand in scale, the expense associated with fine-tuning the entire model becomes prohibitively large. To address this issue, parameter-efficient fine-tuning (PEFT) methods (Houlsby et al., 2019) have been introduced to fine- tune the pre-trained models with only a minimal number of parameters. Among these, LoRA (Hu et al., 2022), which does not change the model architecture, has become notably popular for its simplicity and efficacy. Nevertheless, there is still a capacity gap between LoRA and FT, which is often attributed to the limited number of trainable parameters without further exploration of other underlying causes (Hu et al., 2022; Kopiczko et al., 2024). Drawing on Weight Normalization (Salimans & Kingma, 2016), which achieves faster convergence via improving the conditioning of the gradient with weight reparameterization, we introduce a novel weight decomposition analysis that ini- tially reparameterizes model weights into magnitude and di- rectional components, subsequently examining the changes 1 arXiv:2402.09353v6  [cs.CL]  9 Jul 2024DoRA: Weight-Decomposed Low-Rank Adaptation in magnitude and direction introduced by LoRA and FT. Our analysis reveals that LoRA and FT exhibit markedly distinct patterns of updates, leading us to surmise that these variations mirror the learning capability of each method. Inspired by our findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA), which begins by decompos- ing the pre-trained weight into its magnitude and directional components, then fine-tunes both. Given the substantial size of the directional component in terms of parameters, we exploit LoRA for the directional adaptation to enable efficient fine-tuning, as illustrated in Figure.1. Moreover, by showing a learning behavior similar to FT both empir- ically and mathematically, suggesting a learning capacity closely resembling FT, we have validated DoRA across a wide variety of tasks, from NLP to Vision-Language, and over various backbones, including LLM and LVLM. The experimental results show that DoRA consistently outper- forms LoRA without sacrificing inference efficiency, such as commonsense reasoning (+3.7/+1.0 on LLaMA-7B/13B, +2.9 on LLaMA2-7B, and +4.4 on LLaMA3-8B), visual in- struction tuning (+0.6 on LLaV A-7B), and image/video-text understanding (+0.9/+1.9 on VL-BART). The summary of our contributions is as follows: • We introduce DoRA, a novel PEFT method that incor- porates weight decomposition, achieving a learning capacity closely resembling FT without any additional inference latency over LoRA. • We introduce a novel weight decomposition analysis to uncover the fundamental differences in the learning patterns of FT and different PEFT methods. • DoRA consistently surpasses LoRA on various tasks, from NLP to Vision-Language benchmarks and across various backbones, including LLM and LVLM. 2. Related Works Parameter-Efficient Fine-Tuning (PEFT)methods are de- signed to reduce the high expense of fine-tuning large-scale models. They achieve this by training a relatively small subset of parameters, compared to the total number of pa- rameters, for adapting to downstream tasks. Existing PEFT methods can be divided into three categories. The first category is referred to as Adapter-based methods, which involve introducing additional trainable modules into the original frozen backbone, such as (Houlsby et al., 2019; He et al., 2021; Karimi Mahabadi et al., 2021; mahabadi et al., 2021). For example, (Houlsby et al., 2019) proposes adding linear modules in sequence to the existing layer, whereas (He et al., 2021) advocates for integrating these modules in parallel with the original layer to enhance performance. The second category is Prompt-based methods. These methods add extra soft tokens (prompts) to the initial input and fo- cus solely on fine-tuning these trainable vectors, as seen in works like (Lester et al., 2021; Razdaibiedina et al., 2023; Wang et al., 2023). However, these approaches typically face challenges due to their sensitivity to initialization, af- fecting their overall effectiveness. These first two categories, whether altering the model’s input or architecture, result in increased inference latency compared to the baseline model. LoRA (Hu et al., 2022) and its variants are among the third category of PEFT, notable for not adding any extra inference burden. These methods apply low-rank matrices to approximate weight changes during fine-tuning and can merge with pre-trained weights prior to inference. For ex- ample, (Zhang et al., 2023) employs SVD decomposition and prunes less significant singular values for more efficient updates. (Hyeon-Woo et al., 2022) focuses on low-rank Hadamard product for federated learning. (Qiu et al., 2023; Liu et al., 2023b) exploit orthogonal factorization in fine- tuning diffusion models. (Renduchintala et al., 2023) uses weight tying to further reduce the trainable parameters. (Yeh et al., 2023) introduces a unified LoRA family framework for Stable diffusion. (Ponti et al., 2022) chooses different combinations of LoRAs from the inventory with a routing function for different tasks. (Kopiczko et al., 2024) imple- ments learnable scaling vectors to adjust a shared pair of frozen random matrices across layers. Our research also falls within this third category, and we validate the efficacy of our proposed method alongside LoRA and its variants through comprehensive experimentation. 3. Pattern Analysis of LoRA and FT 3.1. Low-Rank Adaptation (LoRA) Building upon the hypothesis that updates made during the fine-tuning exhibit a low “intrinsic rank”, LoRA (Hu et al., 2022) proposes using the product of two low-rank matri- ces to update the pre-trained weights incrementally. For a pre-trained weight matrix W0 ∈ Rd×k, LoRA models the weight update ∆W ∈ Rd×k utilizing a low-rank decompo- sition, expressed as BA, where B ∈ Rd×r and A ∈ Rr×k represent two low-rank matrices, withr ≪ min(d, k). Con- sequently, the fine-tuned weight W′ can be represented as: W′ = W0 + ∆W = W0 + BA (1) where W0 remains static during the fine-tuning process, and the underlined parameters are being trained. The ma- trix A is initialized with uniform Kaiming distribution (He et al., 2015), while B is initially set to zero, resulting in ∆W = BA being zero at the start of training. Notably, this decomposition of ∆W can be substituted with other LoRA variants, such as VeRA (Kopiczko et al., 2024). Ad- ditionally, based on Eq. (1), we can merge the learned ∆W with the pre-trained weight W0 and obtain W′ in advance 2DoRA: Weight-Decomposed Low-Rank Adaptation of deployment, and given that both W′ and W0 both fall within the dimensionality of Rd×k, LoRA and its related variants do not introduce any extra latency during the infer- ence compared to the original model. 3.2. Weight Decomposition Analysis The study presented in LoRA (Hu et al., 2022) suggests that LoRA can be considered a general approximation of full fine-tuning. By gradually increasing the rank r of LoRA to align with the rank of pre-trained weights, LoRA can attain a level of expressiveness akin to that of FT. Con- sequently, many previous studies have attributed the dis- crepancy in accuracy between LoRA and FT primarily to the limited number of trainable parameters, often without further analysis (Hu et al., 2022; Kopiczko et al., 2024). Drawing inspiration from Weight Normalization (Salimans & Kingma, 2016), which reparameterizes the weight matrix into magnitude and direction for accelerating optimization, we introduce an innovative weight decomposition analysis. Our analysis restructures the weight matrix into two sep- arate components, magnitude and direction, to reveal the inherent differences in LoRA and FT learning patterns. Analysis Method: This analysis examines the updates in both magnitude and direction of the LoRA and FT weights relative to the pre-trained weights to reveal the fundamental differences in the learning behaviors of both. The weight decomposition of W ∈ Rd×k can be formulated as: W = m V ||V ||c = ||W||c W ||W||c (2) where m ∈ R1×k is the magnitude vector, V ∈ Rd×k is the directional matrix, with ||·|| c being the vector-wise norm of a matrix across each column. This decomposition ensures that each column of V/||V ||c remains a unit vector, and the corresponding scalar in m defines the magnitude of each vector. For our weight decomposition analysis, we select the VL- BART model fine-tuned on four image-text tasks as outlined in (Sung et al., 2022) for a case study. Following (Sung et al., 2022), which applies LoRA only to the query/value weight matrix in the self-attention module. We decompose the pre- trained weight W0, the full fine-tuned weight WFT, and the merged LoRA weight WLoRA of query/value weight matrix using Eq. (2). The magnitude and directional variations between W0 and WFT can be defined as follows: ∆Mt FT = Pk n=1 |mn,t FT − mn 0 | k (3) ∆Dt FT = Pk n=1(1 − cos(V n,t FT , Wn 0 )) k (4) Here, ∆Mt FT and and ∆Dt FT represent the magnitude dif- ference and directional difference between W0 and WFT at t training step respectively, with cos(·, ·) being the cosine similarity function. Mn,t FT and Mn 0 are the nth scalars in their respective magnitude vectors, while V n,t FT and Wn 0 are the nth columns in V t FT and W0. The magnitude and direc- tional differences between WLoRA and W0 are calculated similarly, as per Eq. (3) and Eq. (4). We select checkpoints from four different training steps for analysis, comprising three intermediate steps and the final checkpoint from both FT and LoRA, and we perform weight decomposition anal- ysis on each of these checkpoints to determine the ∆M and ∆D throughout different layers. Analysis Results: Figure 2 (a) and (b) illustrate the alter- ations in the query weight matrix of FT and LoRA, with each point representing a ( ∆Dt, ∆Mt) pair from query weight matrices across different layers and training steps. Similarly, Figure 7 in the appendix displays the value weight matrix modifications. It is noticeable that LoRA exhibits a consistent positive slope trend across all the intermediate steps, signifying a proportional relationship between the changes in direction and magnitude. In contrast, the FT displays a more varied learning pattern with a relatively neg- ative slope. This distinction between FT and LoRA likely mirrors their respective learning capability. While LoRA tends to either increase or decrease the magnitude and direc- tion updates proportionally, it lacks the nuanced capability for more subtle adjustments. Specifically, LoRA does not show proficiency in executing slight directional changes alongside more significant magnitude alterations, or vice versa, a feature more characteristic of the FT method. We suspect that such limitation of LoRA might stem from the challenge of concurrent learning both magnitude and direc- tional adaptation, which could be overly complex for LoRA. Consequently, in this work, we aim to propose a variant of LoRA that exhibits a learning pattern more closely resem- bling that of FT, and can improve the learning capacity over LoRA. 4. Method 4.1. Weight-Decomposed Low-Rank Adaptation Drawing from the insights of our weight decomposition analysis, we introduce Weight- Decomposed L ow-Rank Adaptation (DoRA). DoRA initially decomposes the pre- trained weight into its magnitude and directional compo- nents and finetunes both of them. Because the directional component is large in terms of parameter numbers, we fur- ther decompose it with LoRA for efficient finetuning. Our intuitions are two-fold. Firstly, we believe that limiting LoRA to concentrate exclusively on directional adaptation while also allowing the magnitude component to be tun- able simplifies the task compared to the original approach, where LoRA is required to learn adjustments in both mag- 3DoRA: Weight-Decomposed Low-Rank Adaptation 0.014 0.016 0.018 0.020 0.022 D (a) 0.05 0.06 0.07 0.08 0.09 M FT layer 1 layer 2 layer 3 layer 4 layer 5 layer 6 Inter step 1 Inter step 2 Inter step 3 Final step 0.10 0.11 0.12 0.13 0.14 0.15 0.16 D (b) 0.25 0.30 0.35 0.40 0.45 LoRA Inter step 1 Inter step 2 Inter step 3 Final step 0.16 0.18 0.20 0.22 0.24 0.26 0.28 0.30 0.32 D (c) 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 DoRA Inter step 1 Inter step 2 Inter step 3 Final step Figure 2.Magnitude and direction updates of (a) FT, (b) LoRA, and (c) DoRA of the query matrices across different layers and intermediate steps. Different markers represent matrices of different training steps and different colors represent the matrices of each layer. nitude and direction. Secondly, the process of optimizing directional updates is made more stable through weight decomposition, which we delve into more thoroughly in Section.4.2. It is important to highlight that the main distinc- tion between DoRA and weight normalization (Salimans & Kingma, 2016) lies in their training approaches. Weight normalization trains both components from scratch, mak- ing the method sensitive to different initializations. Con- versely, DoRA avoids such initialization concerns since both components begin with pre-trained weights. We initialize DoRA with pre-trained weight W0 as outlined in Eq. (2), where m = ||W0||c and V = W0 after initialization. We then keep V frozen and m a trainable vector. The directional component is then updated through LoRA. DoRA can be formulated similar to Eq. (1) as: W′ = m V + ∆V ||V + ∆V ||c = m W0 + BA ||W0 + BA||c (5) where ∆V is the incremental directional update learned by multiplying two low-rank matrices B and A, and the underlined parameters denote the trainable parameters. The matrices B ∈ Rd×r and A ∈ Rr×k are initialized in line with LoRA’s strategy to ensure that W′ equals W0 before the finetuning. Furthermore, DoRA can be merged with the pre-trained weight before inference, thereby not introducing any additional latency. We visualize the magnitude and directional differences of the query weight matrix between the merged DoRA weight and W0 in the same setting as for FT and LoRA in Figure 2 (c) and leave the visualization of the value weight matrix in the appendix. From the regression line for (∆D, ∆M) of both DoRA and FT, we reveal that in contrast to LoRA’s pattern, DoRA, and FT are characterized by a distinct neg- ative slope. We reason that FT tends towards a negative slope because pre-trained weights already possess substan- tial knowledge suitable for various downstream tasks. There- fore, when provided with adequate learning capacity, having a larger magnitude or direction alteration alone is sufficient enough for downstream adaptation. We additionally com- pute the correlation between ∆D and ∆M for FT, LoRA, and DoRA, and we found that both FT and DoRA exhibit negative correlation values of -0.62 and -0.31, respectively. In contrast, LoRA shows a positive correlation with a value of 0.83. In conclusion, the fact that DoRA demonstrates the ability to make only substantial directional adjustments with relatively minimal changes in magnitude or the reverse while showing learning patterns closer to FT’s signifies its superior learning capacity over LoRA. 4.2. Gradient Analysis of DoRA In this section, we first derive the gradient of DoRA and illustrate how our proposed decomposition benefits the opti- mization of ∆V . Subsequently, we analyze from the gradi- ent’s perspective to explicate the learning pattern of DoRA, which tends to have a negative slope. From Eq. (5), we can obtain the gradient of Loss L with respect to m and V ′ = V + ∆V as: ∇V ′L = m ||V ′||c \u0012 I − V ′V ′T ||V ′||2c \u0013 ∇W′L (6) ∇mL = ∇W′L ·V ′ ||V ′||c (7) Eq. (6) reveals that the weight gradient ∇W′L is scaled by m/||V ′||c and is projected away from the current weight matrix. These two effects contribute to aligning the gra- dient’s covariance matrix more closely with the identity matrix, which is advantageous for optimization (Salimans & Kingma, 2016). Additionally, given that V ′ = V + ∆V , the gradient ∇V ′L is equivalent to ∇∆V L. Therefore, the optimization benefits derived from this decomposition are fully transferred to ∆V , enhancing the learning stability of LoRA. We can gain further insight into the learning pattern of DoRA by referring to Eq. (7). In the subsequent dis- cussion, we represent vectors using lower-case letters in- stead of the previous matrix form notation. Consider w′′ = w′ + ∆w as the parameter update for a weight vector, where ∆w ∝ ∇w′L. In two hypothetical update 4DoRA: Weight-Decomposed Low-Rank Adaptation scenarios, S1 and S2, S1 involves a smaller directional update (∆DS1), while S2 involves a larger one ( ∆DS2). Assuming ||∆wS1|| = ||∆wS2||, and at time 0, we have ∆v = 0 and v′ = v. From ∆DS1 < ∆DS2, it follows that |cos(∆wS1, w′)| > |cos(∆wS2, w′)|. Since ∆w ∝ ∇w′L, it implies |cos(∇S1 w′ L, w′)| > |cos(∇S2 w′ L, w′)|. From Sec 4.1, with v initialized as v0 and w′ = w0 at time 0, we get |cos(∇w′L, w′)| = |cos(∇w′L, v′)| = |cos(∇w′L, v)|. Using the cosine similarity equation with ∆v = 0: cos(∇w′L, v′) = cos(∇w′L, v) = ∇w′L ·v ||∇w′L||||v|| (8) denote m∗ as the magnitude scalar of vector w′ then Eq. (7) w.r.t m∗ can be rewritten to: ∇m∗L = ∇w′L ·v′ ||v′|| = ||∇w′L|| ·cos(∇w′L, v) (9) Given that ||∆wS1|| = ||∆wS2|| for S1 and S2, and ||∇S1 w′ L|| = ||∇S2 w′ L||. Therefore, with: ||∇S1 w′ L|| · |cos(∇S1 w′ L, v)| > ||∇S2 w′ L|| · |cos(∇S2 w′ L, v)| (10) it can be inferred that |∇S1 m∗L| > |∇S2 m∗L| which indicate that S1 has larger magnitude updates over S2 while having smaller directional alteration than that of S2. Our conclu- sion generally holds in practice, as evidenced by Figure 2 (c). Consequently, we have effectively shown how DoRA can be utilized to adjust the learning pattern, diverging from that of LoRA and aligning more closely with the pattern of FT. 4.3. Reduction of Training Overhead In Eq. (1), the gradients of W′ and ∆W are the same. How- ever, with DoRA, which redirects the low-rank adaptation towards the directional component, the gradient of the low- rank updates differs from that ofW′, as illustrated in Eq. (6). This divergence necessitates extra memory during backprop- agation. To address this, we suggest treating||V + ∆V ||c in Eq. (5) as a constant, thereby detaching it from the gradient graph. This means that while ||V + ∆V ||c dynamically reflects the updates of ∆V , it won’t receive any gradient during backpropagation. With this modification, the gra- dient w.r.t m remains unchanged, and ∇V ′L is redefined as: ∇V ′L = m C ∇W′L where C = ||V ′||c (11) This approach reduces the gradient graph memory consump- tion drastically without a noticeable difference in accuracy. We conduct an ablation study to evaluate the impact of the proposed modification on fine-tuning LLaMA-7B and VL- BART. The results indicate that the modification leads to a training memory reduction of approximately 24.4% in fine-tuning LLaMA and 12.4% in VL-BART. Furthermore, the accuracy of DoRA with the modification remains un- changed for VL-BART and shows a negligible difference of only 0.2 compared to DoRA without the modification on LLaMA. For a comprehensive comparison of training memory usage and accuracy differences, please see Table 7 in the appendix. Consequently, all subsequent experiments with DoRA incorporate this adjustment. 5. Experiments We conduct a variety of experiments to showcase the effi- cacy of DoRA on various tasks including language, image, and video domains. Firstly, we evaluate DoRA against sev- eral Parameter-Efficient Fine-Tuning (PEFT) methods by fine-tuning LLaMA-7B/13B, LLaMA2-7B, and LLaMA3- 8B on commonsense reasoning tasks. Subsequently, we extend from single modality to multimodality. We compare DoRA with LoRA across multi-task image/video-text under- standing tasks using VL-BART and visual instruction tuning with LLaV A-1.5-7B. Following this, we explore the com- patibility of DoRA with LoRA and VeRA (Kopiczko et al., 2024) for instruction-tuning on LLaMA-7B and LLaMA2- 7B. Furthermore, we perform a series of ablation studies to illustrate that DoRA surpasses LoRA in performance, irre- spective of the number of fine-tuning training samples and rank variations. Lastly, We analyze the tuning granularity of DoRA, and show that DoRA can achieve better accuracy than LoRA with fewer trainable parameters by selectively updating only the directional components of certain mod- ules. 5.1. Commonsense Reasoning We evaluate DoRA against LoRA and several baseline meth- ods which include Prompt learning (Prefix) (Li & Liang, 2021), Series adapter (Series) (Houlsby et al., 2019), and Parallel adapter (Parallel) (He et al., 2021) on LLaMA- 7B/13B (Touvron et al., 2023) for commonsense reasoning tasks. We also include ChatGPT’s accuracy obtained with gpt-3.5-turbo API using a zero-shot Chain of Thought (Ope- nAI, 2023; Wei et al., 2022). The commonsense reasoning tasks comprise 8 sub-tasks, each with a predefined training and testing set. We follow the setting of (Hu et al., 2023) and amalgamate the training datasets from all 8 tasks to create the final training dataset and conduct evaluations on the individual testing dataset for each task. To ensure a fair comparison, we initially fine-tuned models with DoRA following the LoRA config- uration, maintaining the same rank while adjusting only the learning rate. The marginal increase of 0.01% in the number of trainable parameters for DoRA over LoRA, as detailed in Table 1, arises from the inclusion of learnable magnitude components (parameter of size 1 × k). Then, we further halve the rank used in DoRA compared to LoRA 5DoRA: Weight-Decomposed Low-Rank Adaptation Table 1.Accuracy comparison of LLaMA 7B/13B, LLaMA2 7B, and LLaMA3 8B with various PEFT methods on eight commonsense reasoning datasets. Results of all the baseline methods on LLaMA 7B/13B are taken from (Hu et al., 2023). Results of LoRA on LLaMA2 7B and LLaMA3 8B are obtained using the hyperparameters described in (Hu et al., 2023). DoRA†: the adjusted version of DoRA with the rank halved. Model PEFT Method # Params (%) BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg. ChatGPT - - 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0 LLaMA-7B Prefix 0.11 64.3 76.8 73.9 42.1 72.1 72.9 54.0 60.6 64.6 Series 0.99 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8 Parallel 3.54 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.2 LoRA 0.83 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7 DoRA† (Ours) 0.43 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5 DoRA (Ours) 0.84 69.7 83.4 78.6 87.2 81.0 81.9 66.2 79.2 78.4 LLaMA-13B Prefix 0.03 65.3 75.4 72.1 55.2 68.6 79.5 62.9 68.0 68.4 Series 0.80 71.8 83 79.2 88.1 82.4 82.5 67.3 81.8 79.5 Parallel 2.89 72.5 84.9 79.8 92.1 84.7 84.2 71.2 82.4 81.4 LoRA 0.67 72.1 83.5 80.5 90.5 83.7 82.8 68.3 82.4 80.5 DoRA† (Ours) 0.35 72.5 85.3 79.9 90.1 82.9 82.7 69.7 83.6 80.8 DoRA (Ours) 0.68 72.4 84.9 81.5 92.4 84.2 84.2 69.6 82.8 81.5 LLaMA2-7B LoRA 0.83 69.8 79.9 79.5 83.6 82.6 79.8 64.7 81.0 77.6 DoRA† (Ours) 0.43 72.0 83.1 79.9 89.1 83.0 84.5 71.0 81.2 80.5 DoRA (Ours) 0.84 71.8 83.7 76.0 89.1 82.6 83.7 68.2 82.4 79.7 LLaMA3-8B LoRA 0.70 70.8 85.2 79.9 91.7 84.3 84.2 71.2 79.0 80.8 DoRA† (Ours) 0.35 74.5 88.8 80.3 95.5 84.7 90.1 79.1 87.2 85.0 DoRA (Ours) 0.71 74.6 89.3 79.9 95.5 85.6 90.5 80.4 85.8 85.2 and denote this adjusted configuration as DoRA†. See Table 8 for details on the hyperparameters used. Table 1 demonstrates that DoRA consistently surpasses all baseline methods across both LLaMA-7B/13B, LLaMA2- 7B and LLaMA3-8B. Notably, in the LLaMA-7B model, where LoRA exceeds the performance of other baselines, DoRA further enhances accuracy by 3.7%, outstripping ChatGPT’s accuracy levels. Conversely, for LLaMA-13B, where LoRA’s effectiveness is inferior to the Parallel adapter, DoRA achieves superior accuracy over LoRA by 1% and comparable accuracy to the Parallel adapter, with only a quarter of the trainable parameters required by the Parallel adapter and without adding any extra inference overhead as the Parallel adapter. Additionally, DoRA consistently surpasses LoRA on both LLaMA2-7B and LLaMA3-8B by 2.1% and 4.4%, respectively. Furthermore, DoRA† exceeds LoRA’s performance on LLaMA-7B by 2.8%, on LLaMA- 13B by 1%, on LLaMA2-7B by 2.9%, and on LLaMA3-8B by 4.2%, despite having only half as many trainable parame- ters as LoRA. This outcome suggests that the integration of DoRA enhances the learning capability of LoRA, thereby reducing the need for a higher rank to surpass LoRA in terms of accuracy. Additionally, in previous sections, we hypothesize that a negative correlation between the magnitude update and di- rectional update is more optimal than a positive correlation. This is because pre-trained weights already contain sub- stantial knowledge suitable for downstream tasks, and a larger magnitude or direction alteration alone is sufficient Figure 3.Magnitude (a) and direction (b) difference of LoRA/DoRA and the pre-trained weight of the query matrices across different layers. for downstream adaptation. To further validate our hypoth- esis, we used LLaMA2-7B fine-tuned with DoRA/LoRA on commonsense reasoning datasets as a case study. We visualized the magnitude (∆M) and directional difference (∆D) between the DoRA/LoRA weights and the pre-trained model weights across different modules and layers. In Fig- ure 3 (a) and (b), we observe that the DoRA fine-tuned weights show less deviation from the pre-trained weights in both magnitude and direction, while the differences for the LoRA fine-tuned weights are significantly larger. Coupled with the experimental results that DoRA significantly out- performs LoRA, we can conclude that our earlier hypothesis is valid: a robust foundation model does not require signif- icant alterations for effective downstream adaptation and having the ability to perform more fine-grained magnitude and directional update explains the superiority of DoRA over LoRA. We leave the visualization of the value and key 6DoRA: Weight-Decomposed Low-Rank Adaptation weight matrices in the appendix. 5.2. Image/Video-Text Understanding Table 2.The multi-task evaluation results on VQA, GQA, NVLR2 and COCO Caption with the VL-BART backbone. Method #Params (%) VQAv2 GQA NVLR2 COCO Cap Avg. FT 100 66.9 56.7 73.7 112.0 77.3 LoRA 5.93 65.2 53.6 71.9 115.3 76.5 DoRA (Ours) 5.96 65.8 54.7 73.1 115.9 77.4 Table 3.The multi-task evaluation results on TVQA, How2QA, TVC, and YC2C with the VL-BART backbone. Method #Params (%) TVQA How2QA TVC YC2C Avg. FT 100 76.3 73.9 45.7 154 87.5 LoRA 5.17 75.5 72.9 44.6 140.9 83.5 DoRA (Ours) 5.19 76.3 74.1 45.8 145.4 85.4 Having shown that DoRA can consistently achieve bet- ter accuracy on fine-tuning LLM, we would like to see if DoRA can remain competitive on multi-modality fine- tuning tasks. We compare DoRA with LoRA and full fine- tuning on VL-BART which comprises a vision encoder (CLIP-ResNet101 (Radford et al., 2021)) and an encoder- decoder language model ( BARTBase (Lewis et al., 2020)) across four different image-text tasks: VQAv2 (Goyal et al., 2017) and GQA (Hudson & Manning, 2019) for visual ques- tion answering, NLVR2 (Suhr et al., 2019) for visual reason- ing, and MSCOCO (Chen et al., 2015) for image captioning, and four different video-text tasks from the V ALUE (Li et al., 2021) Benchmark: TVQA (Lei et al., 2018) and How2QA (Li et al., 2020) for video question answering, TVC (Lei et al., 2020) and YC2C (Zhou et al., 2018) for video captioning. We follow the same framework as (Sung et al., 2022) and fine-tuned VL-BART within a multi-task framework for both image/video-text tasks. We adopt the same setup as that of LoRA outlined in (Sung et al., 2022) when ap- plying DoRA. See Table 9 for the complete hyperparam- eters. The result of LoRA and FT for both image/video- text tasks is directly quoted from (Sung et al., 2022). We can see that DoRA uniformly surpasses LoRA in accuracy while maintaining a similar count of trainable parameters in both Table 2 and Table 3. In particular, DoRA exceeds LoRA’s performance by nearly 1% in image-text under- standing tasks, reaching the accuracy level of FT. Moreover, DoRA achieves roughly 2% higher accuracy than LoRA in video-text understanding tasks. 5.3. Visual Instruction Tuning We further scale up the model size and compare DoRA to LoRA and FT on the visual instruction tuning tasks with Table 4.Visual instruction tuning evaluation results for LLaV A- 1.5-7B on a wide range of seven vision-language tasks. We directly use checkpoints from (Liu et al., 2023a) to reproduce their results. Method # Params(%) Avg. FT 100 66.5 LoRA 4.61 66.9 DoRA (Ours) 4.63 67.6 LLaV A-1.5-7B (Liu et al., 2023a) which is composed of a language model, Vicuna-1.5-7B (Peng et al., 2023), and a vision encoder, CLIP ViT-L/336px (Radford et al., 2021). The training datasets contain several datasets from VQA (Goyal et al., 2017; Hudson & Manning, 2019; Marino et al., 2019; Schwenk et al., 2022), OCR (Mishra et al., 2019; Sidorov et al., 2020), region-level VQA (Kazemzadeh et al., 2014; Krishna et al., 2017; Mao et al., 2016), visual conversation (Liu et al., 2023a), and language conversation data. We follow the setting of (Liu et al., 2023a) to filter the training data and construct the tunning prompt format. For a fair comparison, DoRA follows the same configuration as the LoRA configuration provided by (Liu et al., 2023a). The fine-tuned models are then evaluated on seven vision- language benchmarks: VQAv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019), VisWiz (Gurari et al., 2018) SQA (Lu et al., 2022), VQAT (Singh et al., 2019), POPE (Li et al., 2023), and MMBench (Liu et al., 2023c). From Table 4, we can observe that the average accuracy of LoRA already surpasses FT, which could imply that FT might be experiencing issues with overfitting. Given that DoRA is designed to enhance LoRA’s performance to more closely resemble that of FT, in scenarios where FT is infe- rior to LoRA, DoRA’s improvement over LoRA might not be as pronounced as observed in other experiments where FT usually outperforms LoRA. Nonetheless, DoRA still demonstrates superior performance over both LoRA and FT, with an average improvement of 0.7% over LoRA and 1.1% over FT. See Table 10 for the hyperparameters setting and Table 12 for the score of each evaluation benchmark. 5.4. Compatibility of DoRA with other LoRA variants Recall from Equation.(1) that ∆W can be adapted by differ- ent LoRA variants. With DoRA, the concept of incremental directional update ∆V introduced in Equation.(5) can like- wise be replaced with alternative LoRA variants. In this section, we select VeRA (Kopiczko et al., 2024) as a case study to explore DoRA’s compatibility with other LoRA variants. VeRA suggests freezing a unique pair of random low-rank matrices to be shared across all layers, employ- ing only minimal layer-specific trainable scaling vectors to capture each layer’s incremental updates. This approach allows VeRA to reduce trainable parameters significantly 7DoRA: Weight-Decomposed Low-Rank Adaptation Table 5.Average scores on MT-Bench assigned by GPT-4 to the answers generated by fine-tuned LLaMA-7B/LLaMA2-7B. Model PEFT Method # Params (%) Score LLaMA-7B LoRA 2.31 5.1 DoRA (Ours) 2.33 5.5 VeRA 0.02 4.3 DV oRA (Ours) 0.04 5.0 LLaMA2-7B LoRA 2.31 5.7 DoRA (Ours) 2.33 6.0 VeRA 0.02 5.5 DV oRA (Ours) 0.04 6.0 by 10x compared to LoRA, with only a minimal impact on accuracy. We apply VeRA for the directional update in DoRA and name such combination DV oRA. We assess the effectiveness of both DV oRA and DoRA compared to VeRA and LoRA across LLaMA-7B and LLaMA2-7B, focusing on instruction tuning with the 10K subset of cleaned Alpaca dataset (Taori et al., 2023). We utilize the official imple- mentation of VeRA to obtain the results of VeRA and LoRA and fine-tune the model with DV oRA and DoRA using the identical training settings as VeRA and LoRA (see Table 11 in the appendix for more details). The performance of the fine-tuned models is then evaluated on the MT-Bench bench- mark (Zheng et al., 2023) by generating model responses to a pre-defined set of 80 multi-turn questions. These re- sponses are then evaluated by GPT-4, which reviews each answer and assigns a numerical score out of 10. Table 5 presents the average scores for DV oRA, DoRA, VeRA, and LoRA, demonstrating that our proposed method exhibits consistent improvements over VeRA and LoRA for both LLaMA-7B and LLaMA2-7B. This effectively show- cases the compatibility of DoRA with VeRA. In particular, DV oRA merges the advantageous qualities of DoRA and VeRA, attaining scores that are on par with or even sur- pass those of LoRA, yet with significantly fewer parame- ters. For example, DV oRA outperforms VeRA by 0.7/0.5 points and achieves the same level of accuracy as LoRA on LLaMA-7B and DoRA on LLaMA2-7B, respectively. Ad- ditionally, we present a selection of questions chosen from MT-Bench, accompanied by the responses from LLaMA2- 7B fine-tuned using DV oRA and VeRA in the appendix (Table 13 and 14) where we can observe that the answers given by DV oRA tend to be more precise and structural. Next, to further assess DoRA’s ability to remain competitive under varying amounts of training data, considering that in practical situations, access to extensive fine-tuning datasets is frequently limited. We compare DoRA to LoRA and DV oRA to VeRA for fine-tuning LLaMA2-7B/LLaMA-7B with a range of instruction-tuning sample sizes, specifically 1000, 4000, 7000, 10000, with 10000 being the setting of 1000 4000 7000 10000 Number of instruction tuning training samples 5.0 5.2 5.4 5.6 5.8 6.0Score LLaMA2-7B DoRA LoRA DV oRA VeRA Figure 4.Performance of fine-tuned LLaMA2-7B on MT-Bench using different numbers of Alpaca training samples. (Kopiczko et al., 2024). We visualize the average perfor- mance of each method on LLaMA2-7B in Figure 4, and on LLaMA-7B in Figure 9 in the appendix. The result shows that DoRA and DV oRA consistently outperform LoRA and VeRA across all training sample sizes. For instance, with 7000 training samples, DoRA and DV oRA surpass LoRA and VeRA by margins of 0.3 and 0.33, respectively. Even when the sample size is reduced to 1000, DoRA and DV oRA maintain their lead with advantages of 0.29 and 0.22 over LoRA and VeRA, respectively. This demonstrates that our methods persistently enhance performance over LoRA and VeRA, regardless of the training sample volume. 5.5. Robustness of DoRA towards different rank settings 4 8 16 32 64 rank r 40 45 50 55 60 65 70 75 80Avg. Accuracy (+ 37.2%) (+ 22.4%) LLaMA-7B DoRA LoRA Figure 5.Average accuracy of LoRA and DoRA for varying ranks for LLaMA-7B on the commonsense reasoning tasks. This section explores the impact of various rank configura- tions on DoRA and LoRA by adjustingr within the set {4, 8, 16, 32, 64} and assessing the performance of the fine-tuned LLaMA-7B on commonsense reasoning tasks as outlined in Sec 5.1. The average accuracies of LoRA and DoRA across different ranks are depicted in Figure 5, with detailed num- bers presented in Table 15. From Figure 5, we can observe that DoRA consistently surpasses LoRA across all rank con- figurations. Notably, the performance gap widens for ranks 8DoRA: Weight-Decomposed Low-Rank Adaptation Table 6.Accuracy comparison of LLaMA 7B/13B with two differ- ent tuning granularity of DoRA. Columns m and V designate the modules with tunable magnitude and directional components, re- spectively. Each module is represented by its first letter as follows: (Q)uery, (K)ey, (V)alue, (O)utput, (G)ate, (U)p, (D)own. Model PEFT Method # Params (%) m V Avg. LLaMA-7B LoRA 0.83 - - 74.7 DoRA (Ours) 0.84 QKVUD QKVUD 78.1 DoRA (Ours) 0.39 QKVOGUD QKV 77.5 LLaMA-13B LoRA 0.67 - - 80.5 DoRA (Ours) 0.68 QKVUD QKVUD 81.5 DoRA (Ours) 0.31 QKVOGUD QKV 81.3 below 8, where LoRA’s average accuracies drop to 40.74% for r = 8 and 39.49% for r = 4. In contrast, DoRA retains a notable accuracy of 77.96% for r = 8 and 61.89% for r = 4, demonstrating its resilience and consistently superior performance over LoRA regardless of the rank setting. 5.6. Tuning Granularity Analysis The visualization in Figure 2 indicates that significant changes in magnitude often result in relatively smaller di- rectional changes. Given this observation and the fact that directional updates account for most of the trainable param- eters, it prompts an investigation into whether it is possible to decrease the number of trainable parameters by updating only the magnitude components of specific modules while continuing to update both the magnitude and directional components for the remaining linear modules. Our findings indicate that, in contrast to the original con- figuration suggested for LoRA in (Hu et al., 2023), which requires updates to both the Multi-head Attention and MLP layers for optimal performance, DoRA can already achieve superior accuracy by updating only the directional and mag- nitude components of the multi-head layers and the magni- tude of the MLP layers. Specifically, as shown in Table 6, by updating the directional and magnitude components of the QKV modules and only the magnitude of the rest of the layers, DoRA surpasses LoRA by 2.8% on LLaMA-7B and 0.8% on LLaMA-13B, while utilizing only less than half of the trainable parameters compared to LoRA. 6. Broader Impacts 6.1. QDoRA: Enhancements to QLoRA While finetuning LLMs with PEFT significantly reduces training memory overhead, a considerable amount of GPU memory is still required to initially load the model weights onto the GPUs. To further decrease the memory demands of finetuning, QLoRA (Dettmers et al., 2023) suggests quan- tizing the pretrained model to 4-bit and finetuning LoRA on top of the frozen low-bit backbone. With our porposed 0.0 0.1 0.2 0.3 0.4 0.5 Exact match score (Eval size: 500) Zero-shot Five-shot Full Finetune QLoRA QDoRA 0.23 0.27 0.51 0.32 0.56 0.07 0.08 0.26 0.12 0.31 100k Orca-Math finetuning results LLaMA2-7B LLaMA3-8B Figure 6.Accuracy comparison of LLaMA2-7B/LLaMA3-8B with QDoRA, QLoRA and FT on Orca-Math (Mitra et al., 2024). DoRA, which narrows the gap between LoRA and FT, it is natural to also explore whether DoRA can enhance the accuracy of LoRA within the QLoRA framework. Recently, (Kerem Turgutlu, 2024) launch a project that substitutes the LoRA component in QLoRA with DoRA, dubbing it QDoRA, and incorporate the training pipeline with Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023) to enable model splitting and parallel training across multiple GPUs. They conducted experiments on fine-tuning LLaMA2- 7B/LLaMA3-8B using the Orca-Math(Mitra et al., 2024) dataset with QDoRA, QLoRA, and FT. The training set included 100k samples, with 500 reserved for evaluation using the exact match score as the metric. In addition to the fine-tuned models, they also reported results from zero-shot, few-shot, and FT with post-training quantization (PTQ), where the FT model is quantized to the BnB NF4 format after training. According to Figure 6, QDoRA not only significantly surpasses QLoRA by 0.19/0.23 on LLaMA2- 7B and LLaMA3-8B, but it also slightly outperforms FT on both models, while using considerably less memory. This in- dicates that QDoRA can effectively combines the parameter efficiency of QLoRA with the more granular optimization of full finetuning. These initial findings suggest that QDoRA holds considerable promise and could hugely benefit the opensoure community by substantially lowering the GPU memory requirements for fine-tuning large language mod- els. 6.2. Text-to-Image Generation Recently, as diffusion models have expanded in size, LoRA has become a popular method for efficiently fine-tuning large stable diffusion models. In this section, we aim to explore whether DoRA’s advantages over LoRA extend to the task of text-to-image generation. We follow the training pipeline of DreamBooth (Ruiz et al., 2023) for fine-tuning 9DoRA: Weight-Decomposed Low-Rank Adaptation SDXL (Podell et al., 2023), utilizing the advanced train- ing scripts developed by HuggingFace. The hyperparameter settings for LoRA and DoRA are kept the same, and we fine- tune the model using two challenging datasets: 3D icons and Lego sets. The sample seeds for generating the images are kept the same for LoRA and DoRA for fair comparison. The generated images are shown in Figure 10 and 11 in the appendix. The results indicate that DoRA achieves signif- icantly better personalization than LoRA when using the same training settings, and more accurately reflects the train- ing targets. For example, in Figure 10, the first sub-figure of DoRA’s output features a unique round square around the image, which is a feature common to all the training targets. In contrast, this feature is absent in all the LoRA outputs. A similar observation could be found with the Lego training targets, where only the DoRA outputs consistently incorporate the Lego logo in the generated images. 7. Conclusion In this work, we first conduct a novel weight decomposi- tion analysis to reveal the distinct learning patterns between LoRA and FT. Building on these insights, we introduce DoRA, a fine-tuning method that is compatible with LoRA and its variants and exhibits a closer resemblance to FT’s learning behavior. DoRA consistently outperforms LoRA across various fine-tuning tasks and model architectures. Specifically, DoRA improves upon LoRA in commonsense reasoning and visual instruction tuning tasks. Furthermore, DoRA also shows compatibility with VeRA on the Alpaca instruction tuning task. Moreover, DoRA can be considered as a costless alternative to LoRA, as its decomposed mag- nitude and direction components can be merged back into the pre-trained weight after the training, ensuring that there is no extra inference overhead. For future work, we wish to explore the generalizability of DoRA in domains beyond language and vision, particularly in the field of audio. Acknowledgements We extend our gratitude to Benjamin Bossan, Younes Belkada, and Sourab Mangrulkar from Hugging Face for their assistance in integrating DoRA into the PEFT pack- age, thus making our work more accessible to the broader public. We thank Kerem Turgutlu, Jonathan Whitaker, and Jeremy Howard from Answer.AI for their work on the imple- mentation and experiments of QDoRA/FSDP, which makes fine-tuning of large language models with DoRA on con- sumer GPUs a lot more feasible. We also thank Sebastian Raschka for his well-written tutorial on DoRA which offers a thorough overview of the background knowledge neces- sary to comprehend DoRA. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Chen, X., Fang, H., Lin, T.-Y ., Vedantam, R., Gupta, S., Doll´ar, P., and Zitnick, C. L. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. In Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 10088–10115. Cur- ran Associates, Inc., 2023. Goyal, Y ., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the v in vqa matter: Elevating the role of image understanding in visual question answer- ing. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6904–6913, 2017. Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., and Bigham, J. P. Vizwiz grand challenge: Answering visual questions from blind people. In Pro- ceedings of the IEEE conference on computer vision and pattern recognition, pp. 3608–3617, 2018. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., and Neubig, G. Towards a unified view of parameter-efficient trans- fer learning. In International Conference on Learning Representations, 2021. He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on 10DoRA: Weight-Decomposed Low-Rank Adaptation imagenet classification. In Proceedings of the IEEE Inter- national Conference on Computer Vision, pp. 1026–1034, 2015. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning , pp. 2790–2799, 2019. Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. Hu, Z., Wang, L., Lan, Y ., Xu, W., Lim, E.-P., Bing, L., Xu, X., Poria, S., and Lee, R. LLM-adapters: An adapter family for parameter-efficient fine-tuning of large lan- guage models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. Hudson, D. A. and Manning, C. D. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 6700– 6709, 2019. Hyeon-Woo, N., Ye-Bin, M., and Oh, T.-H. Fedpara: Low- rank hadamard product for communication-efficient fed- erated learning. In International Conference on Learning Representations, 2022. Karimi Mahabadi, R., Ruder, S., Dehghani, M., and Hen- derson, J. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. In Proceedings of the 59th Annual Meeting of the Association for Com- putational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 565–576, 2021. Kazemzadeh, S., Ordonez, V ., Matten, M., and Berg, T. ReferItGame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 787–798, 2014. Kerem Turgutlu, Jonathan Whitaker, J. H. Ef- ficient finetuning of llama 3 with fsdp qdora. https://www.answer.ai/posts/ 2024-04-26-fsdp-qdora-llama3.html , 2024. Kopiczko, D. J., Blankevoort, T., and Asano, Y . M. Vera: Vector-based random matrix adaptation. In International Conference on Learning Representations, 2024. Krishna, R., Zhu, Y ., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y ., Li, L.-J., Shamma, D. A., et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. In- ternational journal of computer vision, pp. 32–73, 2017. Lei, J., Yu, L., Bansal, M., and Berg, T. TVQA: Localized, compositional video question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1369–1379, 2018. Lei, J., Yu, L., Berg, T. L., and Bansal, M. Tvr: A large-scale dataset for video-subtitle moment retrieval. In European Conference on Computer Vision, pp. 447–463, 2020. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, 2021. Lewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mo- hamed, A., Levy, O., Stoyanov, V ., and Zettlemoyer, L. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehen- sion. In Proceedings of the 58th Annual Meeting of the As- sociation for Computational Linguistics, pp. 7871–7880, 2020. Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Confer- ence on Machine Learning, pp. 12888–12900, 2022. Li, L., Chen, Y .-C., Cheng, Y ., Gan, Z., Yu, L., and Liu, J. HERO: Hierarchical encoder for Video+Language omni- representation pre-training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2046–2065, 2020. Li, L., Lei, J., Gan, Z., Yu, L., Chen, Y .-C., Pillai, R., Cheng, Y ., Zhou, L., Wang, X. E., Wang, W. Y ., et al. Value: A multi-task benchmark for video-and-language understand- ing evaluation. In Thirty-fifth Conference on Neural In- formation Processing Systems Datasets and Benchmarks Track (Round 1), 2021. Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582– 4597, 2021. Li, Y ., Du, Y ., Zhou, K., Wang, J., Zhao, X., and Wen, J.-R. Evaluating object hallucination in large vision-language 11DoRA: Weight-Decomposed Low-Rank Adaptation models. In Proceedings of the 2023 Conference on Em- pirical Methods in Natural Language Processing , pp. 292–305, 2023. Liu, H., Li, C., Wu, Q., and Lee, Y . J. Visual instruction tun- ing. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. Liu, W., Qiu, Z., Feng, Y ., Xiu, Y ., Xue, Y ., Yu, L., Feng, H., Liu, Z., Heo, J., Peng, S., et al. Parameter-efficient orthogonal finetuning via butterfly factorization. arXiv preprint arXiv:2311.06243, 2023b. Liu, Y ., Duan, H., Zhang, Y ., Li, B., Zhang, S., Zhao, W., Yuan, Y ., Wang, J., He, C., Liu, Z., et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023c. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Infor- mation Processing Systems, pp. 2507–2521, 2022. mahabadi, R. K., Henderson, J., and Ruder, S. Compacter: Efficient low-rank hypercomplex adapter layers. In Ad- vances in Neural Information Processing Systems, 2021. Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A. L., and Murphy, K. Generation and comprehension of unam- biguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 11–20, 2016. Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. Ok-vqa: A visual question answering benchmark requir- ing external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pp. 3195–3204, 2019. Mishra, A., Shekhar, S., Singh, A. K., and Chakraborty, A. Ocr-vqa: Visual question answering by reading text in images. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pp. 947–952, 2019. Mitra, A., Khanpour, H., Rosset, C., and Awadallah, A. Orca-math: Unlocking the potential of slms in grade school math, 2024. OpenAI. Welcome to the openai platform, 2023. URL https://platform.openai.com/docs/ introduction. Peng, B., Li, C., He, P., Galley, M., and Gao, J. Instruc- tion tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., M ¨uller, J., Penna, J., and Rombach, R. Sdxl: Im- proving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Ponti, E. M., Sordoni, A., Bengio, Y ., and Reddy, S. Com- bining modular skills in multitask learning.arXiv preprint arXiv:2202.13914, 2022. Qin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., and Yang, D. Is chatgpt a general-purpose natural language processing task solver? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1339–1384, 2023. Qiu, Z., Liu, W., Feng, H., Xue, Y ., Feng, Y ., Liu, Z., Zhang, D., Weller, A., and Sch¨olkopf, B. Controlling text- to-image diffusion by orthogonal finetuning. In Thirty- seventh Conference on Neural Information Processing Systems, 2023. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748–8763, 2021. Razdaibiedina, A., Mao, Y ., Khabsa, M., Lewis, M., Hou, R., Ba, J., and Almahairi, A. Residual prompt tuning: improving prompt tuning with residual reparameteriza- tion. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 6740–6757, 2023. Renduchintala, A., Konuk, T., and Kuchaiev, O. Tied-lora: Enhacing parameter efficiency of lora with weight tying. arXiv preprint arXiv:2311.09578, 2023. Ruiz, N., Li, Y ., Jampani, V ., Pritch, Y ., Rubinstein, M., and Aberman, K. Dreambooth: Fine tuning text-to-image dif- fusion models for subject-driven generation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22500–22510, 2023. Salimans, T. and Kingma, D. P. Weight normalization: a simple reparameterization to accelerate training of deep neural networks. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pp. 901–909, 2016. Schwenk, D., Khandelwal, A., Clark, C., Marino, K., and Mottaghi, R. A-okvqa: A benchmark for visual ques- tion answering using world knowledge. In European Conference on Computer Vision, pp. 146–162, 2022. Sidorov, O., Hu, R., Rohrbach, M., and Singh, A. Textcaps: a dataset for image captioning with reading comprehen- sion. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceed- ings, Part II 16, pp. 742–758, 2020. 12DoRA: Weight-Decomposed Low-Rank Adaptation Singh, A., Natarajan, V ., Shah, M., Jiang, Y ., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8317–8326, 2019. Suhr, A., Zhou, S., Zhang, A., Zhang, I., Bai, H., and Artzi, Y . A corpus for reasoning about natural language grounded in photographs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6418–6428, 2019. Sung, Y .-L., Cho, J., and Bansal, M. Vl-adapter: Parameter- efficient transfer learning for vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition , pp. 5227–5237, 2022. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y ., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stan- ford alpaca: An instruction-following llama model, 2023. URL https://github.com/tatsu-lab/ stanford_alpaca. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan- guage models. arXiv preprint arXiv:2302.13971, 2023. Wang, Y ., Wu, J., Dabral, T., Zhang, J., Brown, G., Lu, C.-T., Liu, F., Liang, Y ., Pang, B., Bendersky, M., et al. Non- intrusive adaptation: Input-centric parameter-efficient fine-tuning for versatile multimodal modeling. arXiv preprint arXiv:2310.12100, 2023. Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le, Q. V ., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, 2022. Yeh, S.-Y ., Hsieh, Y .-G., Gao, Z., Yang, B. B., Oh, G., and Gong, Y . Navigating text-to-image customization: From lycoris fine-tuning to model evaluation. arXiv preprint arXiv:2309.14859, 2023. Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y ., Chen, W., and Zhao, T. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh Interna- tional Conference on Learning Representations, 2023. Zhao, Y ., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Zheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing, E., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. Zhou, L., Xu, C., and Corso, J. Towards automatic learning of procedures from web instructional videos. In Proceed- ings of the AAAI Conference on Artificial Intelligence , 2018. 13DoRA: Weight-Decomposed Low-Rank Adaptation A. Appendix A.1. Weight decomposition analysis on the value weight matrix In this section, we illustrate the changes in magnitude and direction within the value weight matrix for FT, LoRA, and DoRA across different training steps and layers, as shown in Figure 7. This reveals patterns similar to those seen in the query weight matrix depicted in Figure 2, indicating that DoRA is capable of displaying learning behaviors that closely mirror those of FT across various modules. 0.02 0.03 0.04 0.05 0.06 0.07 0.08 D (a) 0.020 0.025 0.030 0.035 0.040 0.045 0.050 0.055 M FT layer 1 layer 2 layer 3 layer 4 layer 5 layer 6 Inter step 1 Inter step 2 Inter step 3 Final step 0.150 0.175 0.200 0.225 0.250 0.275 0.300 0.325 D (b) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 LoRA Inter step 1 Inter step 2 Inter step 3 Final step 0.20 0.25 0.30 0.35 0.40 0.45 0.50 D (c) 0.4 0.6 0.8 1.0 DoRA Inter step 1 Inter step 2 Inter step 3 Final step Figure 7.Magnitude and Directional changes of FT (a), LoRA (b), and DoRA (c) of the V weight matrices across different layers and intermediate steps. A.2. Ablation study for the modification to reduce DoRA training cost Table 7 presents the GPU cost and the average accuracy of DoRA with and without the proposed modification for commonsense reasoning tasks and image-text understanding tasks. The results indicate that the modification leads to a training memory reduction of approximately 24.4% in fine-tuning LLaMA and 12.4% in VL-BART. Furthermore, the accuracy of DoRA with the modification remains unchanged for VL-BART and shows a negligible difference of only 0.2 compared to DoRA without the modification on LLaMA. Table 7.GPU cost and accuracy of DoRA with or without the modification on the commonsense reasoning tasks and image-text understanding tasks. Model PEFT Method Accumulation steps Batch Size GPU Memory Cost (GB) # Params (%) Avg. LLaMA-7B DoRA w/o modification 4 16 37.3 0.84 78.3 DoRA 28.2 (-24.4%) 0.84 78.1 VL-BART DoRA w/o modification - 300 23.4 5.96 77.3 DoRA 20.5 (-12.4%) 5.96 77.4 14DoRA: Weight-Decomposed Low-Rank Adaptation A.3. Hyperparameters Table 8.Hyperparameter configurations of DoRA for LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B on the commonsense reasoning tasks. Hyperparameters (DoRA) LLaMA-7B LLaMA-13B LLaMA2-7B LLaMA3-8B Rank r 16 32 16 32 16 32 16 32 α 32 64 32 64 32 64 32 64 Dropout 0.05 Optimizer AdamW LR 2e-4 1e-4 3e-4 2e-4 2e-4 2e-4 1e-4 1e-4 LR Scheduler Linear Batch size 16 Warmup Steps 100 Epochs 3 Where Q,K,V ,Up,Down Table 9.Hyperparameter configurations of DoRA for fine-tuning VL-Bart on image/video-text tasks. Hyperparameters (DoRA) image-text video-text Rank r 128 α 128 Dropout 0.0 Optimizer AdamW LR 1e-3 3e-4 LR Scheduler Linear Batch size 300 40 Warmup ratio 0.1 Epochs 20 7 Where Q,K Table 10.Hyperparameter configurations of DoRA and LoRA for fine-tuning LLaV A-1.5-7B with visual instruction tuning datasets. Hyperparameters DoRA LoRA Rank r 128 α 256 Dropout 0.05 Optimizer AdamW LR 2e-4 LR Scheduler Cosine decay Batch size 16 Warmup ratio 0.03 Epochs 1 Where Q,K,V ,O,Up,Down,Gate 15DoRA: Weight-Decomposed Low-Rank Adaptation Table 11.Hyperparameter configurations of DoRA and DV oRA for fine-tuning LLaMA-7B and LLaMA2-7B with cleaned Alpaca dataset. Hyperparameters (DoRA) LLaMA-7B LLaMA2-7B Rank r 64 Dropout 0.0 Optimizer AdamW LR 4e-4 LR Scheduler Cosine Batch size 4 Accumulation Steps 4 Warmup ratio 0.1 Epochs 1 Where Q,K,V ,O,Up,Down,Gate Hyperparameters (DVoRA) LLaMA-7B LLaMA2-7B Rank r 1024 Dropout 0.0 Optimizer AdamW LR 4e-3 LR Scheduler Cosine Batch size 4 Accumulation Steps 4 Warmup ratio 0.1 Epochs 1 Where Q,K,V ,O,Up,Down,Gate 16DoRA: Weight-Decomposed Low-Rank Adaptation A.4. Magnitude and Direction difference between DoRA/LoRA fine-tuned weight and the pre-triained weight of LLaMA2-7B for the commonsesne reasoning tasks Figure 8 depicts the magnitude and direction differences in the weights of the query, key, and value matrices between LoRA/DoRA fine-tuned models and the pre-trained model across various layers of LLaMA2-7B for the commonsense reasoning tasks. The figure shows that the DoRA fine-tuned weights deviate less from the pre-trained weights in both magnitude and direction, supporting our hypothesis that a robust foundation model does not need substantial changes for effective downstream adaptation. (a) (b) (d) (c) (f) (e) Figure 8.Magnitude and direction differences in the weights of the query, key, and value matrices between LoRA/DoRA fine-tuned models and the pre-trained model across various layers of LLaMA2-7B for the commonsense reasoning tasks. 17DoRA: Weight-Decomposed Low-Rank Adaptation A.5. Visual instruction tuning evaluation result Table 12 presents the evaluation result of LLaV A-1.5-7B fine-tuned using DoRA, LoRA, and FT with visual instruction tuning data. The evaluation is conducted on an array of seven different vision-language benchmarks. From the table, we can see that DoRA consistently outmatch FT, and can achieve an average accuracy better than LoRA. Table 12.Visual instruction tuning evaluation result of DoRA, LoRA, and FT for LLaV A-1.5-7B on a wide range of 7 vision-language tasks. Method # Params(%) VQAv2 GQA VisWiz SQA VQA T POPE MMBench Avg. FT 100 78.5 61.9 50.0 66.8 58.2 85.9 64.3 66.5 LoRA 4.61 79.1 62.9 47.8 68.4 58.2 86.4 66.1 66.9 DoRA (Ours) 4.63 78.6 62.9 52.2 69.9 57 87.2 66.1 67.6 18DoRA: Weight-Decomposed Low-Rank Adaptation A.6. Instruction-tuning evaluation examples We sample two questions from the MT-Bench question set and present the respective responses from DV oRA and VeRA, including the scores and comments provided by GPT-4. From these examples, we can observe that responses from DV oRA are generally more accurate and well-organized compared to those from VeRA. For instance, in response to Question 24, DV oRA leverages a well-known superhero story that is familiar to children, whereas VeRA’s response begins with an uncertain tone, potentially making it less engaging than DV oRA’s reply. Table 13.Question 24 from MT-Bench alongside with response from LLaMA2-7B fine-tuned with DV oRA and VeRA, and the evaluation provided by GPT-4. Question 24 As a superhero, how would you explain your origin story to a curious child? VeRA Well, I’m not really sure where I come from. All I know is that one day, I woke up with special powers and abilities that allowed me to help people. I decided to use my powers to fight crime and protect the innocent, and that’s how I became a superhero DV oRA As a superhero, I have the ability to use my super powers to protect people and fight against evil. My origin story begins when I was just a regular kid like you, but one day, I was bitten by a radioactive spider, and it gave me incredible powers. I could climb walls, shoot webs, and see in the dark. I decided to use my powers to help people, and that’s how I became your friendly neighborhood Spider-Man. GPT-4 Evaluation Score: 5 7. Assistant 1’s response starts with an uncertain tone, which might not be very engaging for a child. The explanation is quite vague and doesn’t offer a clear or imaginative origin story that could captivate a child’s interest. It does, however, maintain the theme of using powers for good, which is a positive message. Assistant 2’s response is more detailed and uses a familiar superhero origin story (Spider-Man’s) that many children can relate to and understand. It’s engaging and provides a clear narrative of how an ordinary person can become a superhero. However, it directly references Spider-Man, which is not an original origin story and could be misleading if the assistant is supposed to be a unique superhero. Despite this, the response is more likely to resonate with a child due to its clarity and reliability. Both responses could be improved by providing an original and imaginative origin story that doesn’t rely on existing superhero narratives. They should also ensure that the story is simple enough for a child to understand while being engaging and inspiring. 19DoRA: Weight-Decomposed Low-Rank Adaptation Table 14.Question 33 from MT-Bench alongside with response from LLaMA2-7B fine-tuned with DV oRA and VeRA, and the evaluation provided by GPT-4. Question 33 Why might someone choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app? VeRA There are several reasons why someone might choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app. Some people may prefer the physical act of navigating with a paper map, as it can be more tactile and interactive. Others may prefer to have a hard copy of the map in case the GPS or app fails or the battery dies. Additionally, some people may not have access to a GPS device or smartphone, or may not want to rely on technology for navigation. Finally, some people may simply prefer the security and peace of mind that comes with knowing exactly where they are and how to get to their destination DV oRA There are several reasons why someone might choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app. Here are some of them: 1. Navigation accuracy: GPS devices and smartphone apps rely on satellite signals to pinpoint your location, but this technology is not foolproof. GPS signals can be blocked by tall buildings, tunnels, or other obstacles, resulting in inaccurate or missing information. Paper maps, on the other hand, are based on accurate, up-to-date information that is not subject to these limitations. 2. Cost: GPS devices and smartphone apps can be expensive to purchase and maintain. Paper maps are free to use and do not require an ongoing subscription or data plan. 3. Personal preference: Some people prefer the tactile experience of using a paper map, which can be easier to read and navigate than a screen. Others enjoy the challenge of navigating without technology, which can help them learn more about the area they are visiting. 4. Battery life: GPS devices and smartphone apps can drain batteries quickly, especially when used for navigation. Paper maps do not require power, so they can be used for extended periods without recharging. 5. Offline use: GPS devices and smartphone apps rely on an internet connection to function properly. If there is no signal, the device will not be able to provide accurate information. Paper maps, on the other hand, can be used offline, making them ideal for use in remote or unpopulated areas. In summary, there are several reasons why someone might choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app. These include navigation accuracy, cost, personal preference, battery life, and offline use. GPT-4 Evaluation Score: 6 8. Assistant 1’s response is somewhat brief and lacks specific details that could be helpful to the user. It mentions the tactile nature of paper maps, the possibility of technology failure, lack of access to technology, and a general preference for security and peace of mind. However, it does not delve into the practical considerations such as GPS signal limitations, cost, battery life, or offline use, which are important factors in deciding between paper maps and digital navigation tools. Assistant 2’s response is more comprehensive and provides a structured list of reasons, including navigation accuracy, cost, personal preference, battery life, and offline use. Each point is explained clearly, giving the user a better understanding of the potential advantages of paper maps over GPS devices or smartphone apps. The response is relevant, accurate, and detailed, making it more helpful to the user. Both assistants provided relevant and accurate information, but Assistant 2’s answer is more detailed and structured, which is why it receives a higher score. The scores reflect the level of detail and the usefulness of the information provided to the user, with Assistant 2 offering a more complete answer to the question 20DoRA: Weight-Decomposed Low-Rank Adaptation A.7. Robustness towards different numbers of instruction tunning examples Figure 9 depicts the score of DoRA, LoRA, DV oRA, and VeRA for fine-tuning LLaMA-7B with varying instruction-tuning sample sizes, including 1000, 4000, 7000, and 10000. From the figure, we can observe that similar to that of the result on LLaMA2-7B, both DoRA and DV oRA consistently outperform LoRA and VeRA across the entire range of training sample sizes. 1000 4000 7000 10000 Number of instruction tuning training samples 3.0 3.5 4.0 4.5 5.0 5.5Score LLaMA-7B DoRA LoRA DV oRA VeRA Figure 9.Performance of LLaMA-7B fine-tuned with LoRA, DoRA, VeRA, and DV oRA on MT-Bench using different numbers of training samples from Alpaca dataset (Taori et al., 2023). 21DoRA: Weight-Decomposed Low-Rank Adaptation A.8. Robustness of DoRA towards different rank Table 15 shows a comparison of the average accuracy between LoRA and DoRA method across various rank settings for commonsense reasoning tasks. DoRA consistently outperforms LoRA at all rank settings, with the performance gap widening as the rank decreases. This suggests that our method effectively enhances the learning capacity of LoRA, enabling it to achieve better accuracy with fewer trainable parameters. Table 15.Accuracy comparison of LoRA and DoRA with varying ranks for LLaMA-7B on the commonsense reasoning tasks. PEFT Method rank r # Params (%) BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg. LoRA 4 0.10 2.3 46.1 18.3 19.7 55.2 65.4 51.9 57 39.5 8 0.21 31.3 57.0 44.0 11.8 43.3 45.7 39.2 53.8 40.7 16 0.42 69.9 77.8 75.1 72.1 55.8 77.1 62.2 78.0 70.9 32 0.83 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7 64 1.64 66.7 79.1 75.7 17.6 78.8 73.3 59.6 75.2 65.8 DoRA (Ours) 4 0.11 51.3 42.2 77.8 25.4 78.8 78.7 62.5 78.6 61.9 8 0.22 69.9 81.8 79.7 85.2 80.1 81.5 65.7 79.8 77.9 16 0.43 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5 32 0.84 69.7 83.4 78.6 87.2 81.0 81.9 66.2 79.2 78.4 64 1.65 69.9 81.4 79.1 40.7 80.0 80.9 65.5 79.4 72.1 22DoRA: Weight-Decomposed Low-Rank Adaptation A.9. Text-to-Image Generation Figures 10 and 11 show the images produced by SDXL fine-tuned with DoRAand LoRA via DreamBooth (Ruiz et al., 2023) personalization techniques on two distinct training sets: 3D Icon 1 and Lego2. The results reveal that DoRA can achieve considerably better personalization than LoRA with identical training configurations, more closely matching the training target. Figure 10.Images generated with SDXL finetuned with LoRA and DoRA on the 3D Icon training sets. Figure 11.Images generated with SDXL finetuned with LoRA and DoRA on the Lego training sets. 1https://huggingface.co/datasets/linoyts/3d_icon 2https://huggingface.co/datasets/merve/lego_sets_latest 23",
      "meta_data": {
        "arxiv_id": "2402.09353v6",
        "authors": [
          "Shih-Yang Liu",
          "Chien-Yi Wang",
          "Hongxu Yin",
          "Pavlo Molchanov",
          "Yu-Chiang Frank Wang",
          "Kwang-Ting Cheng",
          "Min-Hung Chen"
        ],
        "published_date": "2024-02-14T17:59:34Z",
        "pdf_url": "https://arxiv.org/pdf/2402.09353v6.pdf"
      }
    },
    {
      "title": "Spectral Adapter: Fine-Tuning in Spectral Space",
      "abstract": "Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for\npretrained deep neural networks have captured widespread interest. In this\nwork, we study the enhancement of current PEFT methods by incorporating the\nspectral information of pretrained weight matrices into the fine-tuning\nprocedure. We investigate two spectral adaptation mechanisms, namely additive\ntuning and orthogonal rotation of the top singular vectors, both are done via\nfirst carrying out Singular Value Decomposition (SVD) of pretrained weights and\nthen fine-tuning the top spectral space. We provide a theoretical analysis of\nspectral fine-tuning and show that our approach improves the rank capacity of\nlow-rank adapters given a fixed trainable parameter budget. We show through\nextensive experiments that the proposed fine-tuning model enables better\nparameter efficiency and tuning performance as well as benefits multi-adapter\nfusion.",
      "full_text": "Spectral Adapter: Fine-Tuning in Spectral Space Fangzhao Zhang Electrical Engineering Stanford University zfzhao@stanford.edu Mert Pilanci Electrical Engineering Stanford University pilanci@stanford.edu Abstract Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for pre- trained deep neural networks have captured widespread interest. In this work, we study the enhancement of current PEFT methods by incorporating the spectral information of pretrained weight matrices into the fine-tuning procedure. We investigate two spectral adaptation mechanisms, namely additive tuning and or- thogonal rotation of the top singular vectors, both are done via first carrying out Singular Value Decomposition (SVD) of pretrained weights and then fine-tuning the top spectral space. We provide a theoretical analysis of spectral fine-tuning and show that our approach improves the rank capacity of low-rank adapters given a fixed trainable parameter budget. We show through extensive experiments that the proposed fine-tuning model enables better parameter efficiency and tun- ing performance as well as benefits multi-adapter fusion. Code is released at https://github.com/pilancilab/spectral_adapter. 1 Introduction Size of language and vision model undergoes a drastic explosion in recent days and results in billions of parameters up to date. While fine-tuning has been used a lot for adapting pretrained large models to various downstream tasks, fine-tuning tasks become increasingly hard with current size of pretrained models due to the huge demand of computing resource. Meanwhile, exchange and storing of fine- tuned models are also expensive given their enormous size. To alleviate these rising problems for fine-tuning large pretrained models, a recent line of research has digged into the Parameter-Efficient Fine-Tuning (PEFT) model family and harnessed great attention. A high-level philosophy behind those PEFT methods is to train a reduced number of parameters compared to full fine-tuning, which instantly saves computing resource and enables light-weight fine-tuned model exchange. Among all PEFT methods, Low-Rank Adaptation (LoRA) [ 20] model is a huge success attributed to its simplicity and effectiveness. Specifically, LoRA proposes to tune an additive trainable low-rank matrix and brings zero inference latency after merging the adapter into pretrained model weights. Since its emergence, numerous variants of LoRA have been developed. For instance, AdaLoRA [65], IncreLoRA [62], and DyLoRA [ 54] propose to dynamically adjust LoRA rank distribution for improving tuning efficiency, QLoRA [10] combines LoRA with model quantization to further save computing resource, LoRA+ [ 16] and PrecLoRA [ 61] study the optimization landscape of LoRA training, and more recent variant DoRA [32] decomposes pretrained weights into magnitude and direction components and applies LoRA for direction tuning, see Apppendix A for a more comprehensive review of different LoRA variants. Other PEFT methods such as Orthogonal Fine- Tuning (OFT) proposes to multiply pretrained weights by tunable orthogonal matrices for preservation of hypersphere energy between pretrained neurons. Though these different PEFT methods focus on improving fine-tuning efficiency with reduced parameters, rare attention has been paid to utilize pretrained model weights’ information beyond its magnitude in the fine-tuning procedure. Prior research in statistical machine learning such as [36] has studied the Empirical Spectral Distribu- tion (ESD) of deep models’ weight matrices and found that the ESDs for larger model weights are arXiv:2405.13952v2  [cs.LG]  4 Nov 2024Figure 1: Training loss of fine-tuning Llama3 8B model with Orca Math dataset [38] and evaluation score on GSM8K benchmark [7]. We follow experimental setup in [53], see Appendix F.1 for details. All methods except full fine-tuning maintain approximately 0.23% trainable parameters. usually more structured and contain indicative information to distinguish between different training stages. More recent work such as [3] investigates the \"dark matter\" effect of bottom spectral space of model weights and recognizes its critical role in attention sink phenomenon observed in [57]. Both work contributes to decrypting spectral information of model weights and sheds light on building insightful understanding of the connection between weight matrices’ spectral information and model performance. In this work, we explore further the value of model weights’ spectral pattern and unravel its effectiveness in enhancing fine-tuning tasks. We showcase via extensive empirical observation that integration of spectral information of pretrained model weights improves current PEFT methods’ parameter efficiency, tuning effect, and arises as a natural solution to multi-adapter fusion problems. Moreover, the suggested fine-tuning model maintains better practicality compared to prior spectral tuning models, which will be investigated further below. Though any technique for weight fine-tuning can be directly applied to fine-tune singular vector matrices of pretrained model weights, we investigate two specific forms of such extension, namely additive tuning and orthogonal rotating the top singular vector space, which we address as Spectral AdapterA and Spectral AdapterR respectively in later content. The spectral adaptation mechanisms being considered are formally depicted in Section 2. As a warmup, to show that incorporating spectral information is indeed helpful, Figure 1 displays the training loss of fine-tuning Llama3 8B model on HuggingFace Orca Math dataset and validation score on GSM8K benchmark, from which it can be clearly observed that Spectral AdapterA performs superior to recent variants of PEFT methods and behaves closest to full fine-tuning, here we follow experimental setup in [53], see Appendix F.1 for details and more investigation. In below, we first introduce the fine-tuning model being studied in Section 2 and we then provide some theoretic insights in Section 3. After that, we detail the advantage of our spectral adapter in enhancing fine-tuning result, improving model’s parameter efficiency, and helping with multi-adapter fusion as well as address any concern with respect to practicality issues in Section 4. Conclusion and future work is discussed in Section 5. For sake of page limitation, literature review is deferred to Appendix A. To summarize, the proposed spectral adaptation mechanism demonstrates the first attempt to fine-tune spectral space of pretrained model weights in a parameter-efficient and storage-economic way which improves current PEFT methods from aspects involving tuning results, parameter efficiency, and multi-adapter fusion. We hope this work serves as a building block and motivates further and deeper insightful investigation for exploring spectral structure of pretrained model weights, which becomes increasingly meaningful especially in current large model regime. 2 Spectral Adapter: Incorporating Spectral Information into Fine-Tuning Motivated by the intrinsic low-rank of weight shifts in fine-tuning procedure studied in [1], LoRA [20] proposes to add a low-rank factorized trainable matrix to pretrained model weights and tune only these additive parameters for downstream task adaptation, which usually injects far fewer trainable parameters compared to full fine-tuning and results in light-weight tuned adapters. LoRA serves as an outstanding representative of PEFT family and is now widely-used for different fine-tuning tasks. 2Figure 2: Compared to LoRA which proposes to add low-rank trainable matrices to pretrained weights, we study two types of spectral adapters: Spectral AdapterA considers additively tuning the top columns of singular vector matrices and Spectral AdapterR considers orthogonally rotating the top columns of singular vector matrices. Inspired by the parameter efficiency of LoRA and the close connection between matrix rank and its spectral representation, here we study two spectral fine-tuning mechanisms, both are completed via first carrying out Singular Value Decomposition (SVD) of pretrained model weights and then fine- tuning the top columns of singular vector matrices obtained via the SVD. More precisely, consider a pretrained weight matrix with its spectral representation of form W =USV T , we define additive spectral adapter as Spectral AdapterA(W) ∶=[U1 +AU U2]S[V1 +AV V2], and correspondingly the rotational version Spectral AdapterR(W) ∶=[U1RU U2]S[V1RV V2], where U1, V1 denote the top- r columns of U and V and U2, V2 denote the rest of the columns. A =(AU , AV ) consists of trainable matrices of shape same as (U1, V1) and R =(RU , RV ) consists of two trainable orthogonal matrices of shape r by r such that RT U RU =RT V RV =I. As we show in later sections, the orthogonality constraint is efficiently handled with the Cayley parameterization, see Section 4.3 for details. The proposed fine-tuning model architecture can be visualized from Figure 2. Here Spectral AdapterA more resembles LoRA as it is of additive form while Spectral AdapterR more resembles prior Orthogonal Fine-Tuning (OFT) method which we compare further in Section 4. To ensure zero initialization as often done for PEFT methods, we initialize AU and AV both at zero. For rotational spectral adapter, we initialize RU and RV as identity matrices. A more thorough literature review suggests that prior work considering tuning model weights’ spectral representation (FSGAN[ 47], SVDiff [ 15]) has been proposed for alleviating overfitting when fine-tuning different vision models. These methods only look at tuning the singular values of flattened CNN weights and thus have fixed amount of trainable parameters. Moreover, these methods require storing all U, Sand V during training while only the diagonal vector of S is tuned, which nearly doubles the storage requirement compared to pretraining when fine-tuning on downstream tasks. Contrarily, we consider incorporating spectral information in generic fine-tuning procedure for different layers (flattened CNN weights, dense linear weights, etc.) and our method enables flexible parameter budget choices by varying values of r. Methodology-wise, we consider tuning the top-r columns of U and V by additive and rotational tuning, both requiring only these top columns to be stored additionally and the left part can be merged into a single weight matrix. See Section 4.4 for more investigation on practicality of the proposed method. 3 Theoretical Insights After introducing the model architecture of spectral adapter we consider, the main question now remains whether tuning the spectral representation of pretrained weights is indeed an improvement over existing PEFT methods. Before we step into our empirical observations, we first provide some theoretical insights for the proposed spectral adaptation mechanism. In this section, we show advantage of our spectral adapter method compared to LoRA from two theoretic perspectives by 3analyzing both the rank capacity of the adapters (Section 3.1) and the subspace alignment of pretrained weight matrices (Section 3.2). Specifically, we will see that Spectral AdapterA has larger rank capacity than LoRA adapter, which indicates the tuned weight has more adaptation freedom and thus is more desirable. Moreover, the dominant spectral direction of pretrained weight matrix identifies more ideal neuron alignment under the setting we consider in Section 3.2, which justifies the robustness of tuning top singular vectors in our spectral adapter. In Appendix D, we show that Spectral AdapterA is approximately equivalent to DoRA [32] for vector-form weights. 3.1 Adapter Rank Capacity For any pretrained weight matrixW, suppose that the adapter is given by the parameterizationfθ(W) where θ represents trainable weights. For instance with LoRA adapter, fθ(W) =W +ABT , where θ ={A, B} is trainable. We define the rank capacity of an adapter fθ(W) as follows: R(fθ; W) ∶= max θ rank(fθ(W))−min θ rank(fθ(W)), which describes the range of matrix ranks the tuned weight can achieve given a specific adapter form. Then, the following lemma shows that Spectral AdapterA has twice the rank capacity of LoRA adapter under an equal number of trainable parameters. Lemma 3.1. Suppose that W ∈Rn×m is an arbitrary full row-rank matrix and n ≤m without loss of generality. Consider rank-r LoRA and rank-r additive spectral adapter, which have an equal number of trainable parameters. We have R(LoRA; W) =r, R(Spectral AdapterA; W) =2r. See Appendix B for proof. Therefore when pretrained model weight matrix is close to full row-rank, as what has been observed in [20], Spectral AdapterA has nearly double rank capacity compared to LoRA adapter. Furthermore, some prior work explicitly imposes low-rank constraint when training original NNs [50, 43, 66, 22, 68, 24, 9]. Using LoRA adapter to fine-tune such pretrained model weights would destroy their rank constraints while applying spectral adapter preserves the constraints. Next we proceed to show that top spectral space of pretrained weight matrices is more aligned with ideal neuron direction under a simple setting via subspace decomposition analysis of pretrained model weights. This observation corroborates our choice of tuning top singular vectors in our proposed spectral adaptation mechanism. Empirically, we observe that tuning top directions performs superior to tuning bottom ones, see Appendix F.3 and F.5.1 for related experiments. 3.2 Weight Subspace Alignment Figure 3: Top singu- lar vector of pretrained weight recognizes more ideal neuron direction. Il- lustration plot for Section 3.2. Consider two-layer ReLU network with m hidden nodes and univariate output. For squared loss objective, we can write out the training problem explicitly as min W(1),W(2) ∥(XW (1))+W(2) −y∥2 2 +β(∥W(1)∥2 F +∥W(2)∥2 2), where X ∈ Rn×d is the data matrix, (W(1) ∈ Rd×m, W(2) ∈ Rm) are first and second layer weights respectively and y ∈Rn is the label vector. For better visualization, we take d = 3. Consider the case that all data points lie on xy−plane, which mimics the usual observation that data points occupy a low-dimensional manifold. Then we can decompose each first layer neuron W(1) j ∈ Rd into W(1) j = wj1 +wj2 where wj1 ∈ R(X), wj2 ⊥ R(X). With simple algebra, for non-zero weight decay which is often the default setting for current deep learning optimizers, one can derive wj2 =0 and thus W(1) j =wj1 ∈R(X). Therefore all optimal neurons lie also in xy−plane. However, due to optimization errors, some of the trained neurons might be slightly deviated from xy−plane, as illustrated in Figure 3, where ui indicates pretrained neuron directions, though most of them lie in xy−plane, some might deviate (i.e., u4). u⋆ indicates the top singular vector direction of pretrained weight W(1) which here recognizes the xy−plane orientation, and thus fine-tuning u⋆ is noiseless and is expected to be more robust. 44 Empirical Results: The Impact of Spectral Information We experiment our proposed spectral adapter with fine-tuning large language models and diffusion models and compare against various recent PEFT methods. From language model experiments, we observe that Spectral Adapter A performs superior to various PEFT baselines and harnesses higher scores on different benchmarks, which again verifies the effectiveness of incorporating spectral information into the fine-tuning procedure, see Section 4.1 for details. For diffusion model experiments, we will see that the advantage of spectral adapter comes in two-fold: Spectral AdapterA offers a natural solution to existing problems in multi-adapter fusion procedure and Spectral AdapterR manifests finer-grained parameter budgets as well as better parameter efficiency, see Section 4.2 and 4.3 respectively. For a fair comparison with all baselines, we use their official implementation and follow hyperparameter setting in their original reports as long as available. See each individual section for corresponding experimental details. All experiments are done with NVIDIA RTX A6000 GPU. 4.1 Language Model Fine-Tuning: Enhancing Fine-Tuning Results with Spectral Adapter A For large language model experiments, we present experimental results for fine-tuning DeBERTaV3- base model (185M) and Mistral model (7B) on GLUE and GSM8K tasks respectively. Our Spectral AdapterA method achieves superior tuning results compared to various recent PEFT methods in most experiments. DeBERTaV3-base Experiment. Table 1 shows fine-tuning results of DeBERTaV3-base model on GLUE benchmarks with various PEFT methods. For a fair comparison, we use official implemen- tations for LoRA, DoRA, OFT and AdaLoRA in HuggingFace PEFT library, with hyperparameter setting for LoRA [20] and AdaLoRA [65] following their original reports. We use same hyperpa- rameter setting as LoRA for DoRA and follow the setting used in BOFT [33], a variant of OFT, for OFT experiments. We abbreviate Spectral AdapterA as SpectralA for presentation simplicity and we tune hyperparameters for Spectral AdapterA. See Appendix F.2 for hyperparameter details and F.3 for loss/validation plot comparison. We fine-tune all q, k, vmatrices in attention layers. Our Spectral AdapterA achieves highest average score and best scores for most tasks with fewest trainable parameters. Method # Param GLUE MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg. LoRAr=24 0.72% 88.87 95.06 87.00 65.84 91.87 91.45 81.22 90.43 86.47 DoRAr=24 0.73% 88.91 95.29 88.72 65.84 92.01 91.51 80.14 90.10 86.57 OFTr=4 0.72% 89.16 95.06 87.74 66.75 93.28 91.33 78.70 89.72 86.47 AdaLoRAr=24 1.07% 89.44 94.95 89.70 63.06 93.17 91.48 83.75 91.22 87.10 SpectralA r=24 0.72% 89.79 95.75 90.19 69.44 93.35 91.65 83.39 90.64 88.03 Table 1: Accuracy comparison of fine-tuning DeBERTaV3-base with various PEFT methods on GLUE benchmarks. SpectralA is abbreviation for Spectral AdapterA. See Section 4.1 for experimental details. Mistral 7B Experiment. We experiment our Spectral Adapter A with Mistral 7B model [23] fine-tuned for GSM8K task [ 7]. Since all baseline model reports include no fine- tuning tasks with the Mistral family, we use official implementations of all baseline meth- ods for comparison and we fix learning rate to be 2.5e − 5 for all methods following [ 51]. Method #Param GSM8K Pre-Trained − 37.91 ±1.34 LoRAr=8 0.16% 44.81 ±1.37 DoRAr=8 0.17% 43.82 ±1.37 SpectralA r=8 0.16% 49.73 ±1.38 Table 2: Accuracy comparison of fine-tuning Mis- tral 7B model with different PEFT methods on GSM8K benchmark. See Section 4.1 for experi- mental details. We take r = 8 for LoRA, DoRA and Spectral AdapterA to maintain approximately same num- ber of trainable parameters for all methods. Ta- ble 2 presents the accuracy comparison where SpectralA stands for Spectral Adapter A. From the result, we observe that our Spectral AdapterA scores higher than both LoRA and DoRA by a large margin and increases the pretrained model baseline significantly, which verifies the effective- ness of the proposed spectral adaptation mecha- nism. See Appendix F.4 for more about experi- mental details. Note for a different learning rate, DoRA performs better than LoRA while still worse than our method, see also Appendix F.4 for details. 54.2 Diffusion Model Fusion: Improving Multi-Object Fine-Tuning with Spectral Adapter A Figure 4: Distributing different concept tunings along different spectral space helps with identity preservation in multi-adapter fusion, see Section 4.2 for details. Multi-adapter fusion is a current bottleneck in diffusion model fine-tuning tasks with LoRA adapters. Simply adding different LoRA adapters tuned for distinct objects will result in problems involving identity loss and concept binding [12]. To tackle this toughness, different methods emerge such as Gradient Fusion [12] and Orthogonal Adaptation [42]. Specifically, Orthogonal Adaptation method proposes to fix LoRA parameter B to have orthogonal basis and train A solely. Experiments there show that merging LoRA weights with such orthogonal basis helps preserving individual object characteristics compared to its non-orthogonal counterpart. In Orthogonal Adaptation [ 42], the authors maintain B by manually keeping large orthogonal matrices for different layer sizes and sample r columns from corresponding orthogonal matrix to form B for each LoRA adapter. With knowledge from random matrix theory, such sampled matrices are likely to have orthogonal basis. Notably, our Spectral AdapterA naturally operates on orthogonal singular vectors and thus introduces an elegant solution to multi-adapter fusion problems by distributing different concept tunings along different columns of singular vector matrices, which maps to wireless communications where the signals are distributed over non-overlapping frequencies. A subtlety here lies in the choice of column space for different fine-tuning tasks: (1) Sample-based methods can be adopted if data privacy is considered and different tuning tasks are done independently. In Appendix F.5, we show that tuning top columns manifests better generation quality compared to both tuning bottom columns and sampling random orthogonal basis as what has been done in Orthogonal Adaptation [42]. Thus there is a trade-off between high-quality generation and concept collapsing, i.e., sampling from top singular vectors is more encouraged while column overlapping between concepts happens more often compared to sampling from the whole set. (2) On the other hand, if fine-tuning tasks are not isolated and can collaborate on the column scheduling, then more deliberate tuning scheduling can be adopted, for example in a two-concept tuning task with r =4, the first concept can allocate first to fourth columns and the second concept then claims fifth to eighth columns. Figure 4 demonstrates steps for the same method for three-concept tuning task. Since we expect fine-tuned weights to stay close to original weights, though both row space and column space are tuned in spectral adapter, this adaptation mechanism approximates orthogonal-basis tuning for different objects and thus we expect it helps improving identity preservation for multi-adapter fusion. In this section, we investigate this effect via extensive diffusion model experiments. Our experiments follow [42] and build on [12] which studies multi-LoRA fusion. We experiment with multi-object tuning and face generation tasks. Due to space limitation, we present some multi-object tuning results below and we leave the rest to Appendix F.5. For all tasks, we compare against baselines including Gradient Fusion [12], Orthogonal Adaptation [42], and FedAvg [37]. We start with a simple review for these baseline methods. Baseline Review To merge different LoRA adapters, say we have a set of LoRA parameters{∆θ1, . . . ,∆θn} where ∆θi = AiBT i and pretrained parameter θ0, FedAvg [ 37] proposes to merge them in to a single parameter by taking a weighted average as θmerged =θ0 +∑i λi∆θi, where λi is the weight attached to parameter ∆θi and is usually taken to satisfy ∑i λi = 1, i.e., θmerged is a convex combination of individual adapters. Gradient Fusion [12] instead considers solving an auxiliary optimization problem of form θmerged =argminθ ∑n i=1 ∥(θ0 +∆θi)Xi −θXi∥2 F where Xi represents the input activation of the i-th concept. Orthogonal Adaptation [42] follows FedAvg method and replaces original LoRA 6Figure 5: Generation results of Chilloutmix diffusion model [8] with different fused adapters tuned on three custom animal concepts. See Section 4.2 for details. parameters with orthogonal-based LoRA adapters. For our method, to merge different spectral adapters, let θ0 = U0S0V T 0 denote the spectral representation of pretrained model weight. Given a set of spectral adapters {(Ui, Vi), . . . ,(Un, Vn)} with zero-padding to make the shape the same as (U0, V0), we follow FedAvg and compute θmerged = (U0 +∑i λiUi)S0(V0 +∑i λiVi)T . In the following experiments, we take λi =1/n as in [42] for all FedAvg, Orthogonal Adaptation, and our Spectral AdapterA fusion. Notably, all FedAvg, Orthogonal Adaptation, and our Spectral AdapterA fusion can be done approximately instantly while Gradient Fusion usually takes around 10 ∼ 15 minutes for solving its auxiliary optimization problems for all concept adapters. Multi-Object Generation We follow default training setting in [ 12] and fine-tune the Chilloutmix diffusion model [ 8] on three custom animal concepts, see original animals in \"reference\" in Figure 5. For better spatial alignment, we adopt T2I-Adapter [39] with sketch condition and we set guidance equal to one, see also \"reference\" in Figure 5 for the sketch condition being used. LoRA rank r =8 is adopted. For baseline comparisons, we use original code for Gradient Fusion [ 12] and Orthogonal Adaptation [42]. We adapt code of Gradient Fusion for FedAvg method since there is no official implementation available. Custom animal name is replaced with special token < Vanimal> for fine-tuning. For our Spectral AdapterA, we follow the method depicted in Figure 4 and tune first, second, and third top eighth columns of singular vector matrices for different animal concepts. Figure 5 shows the generation results with different methods for selected prompts. Notably, baseline methods sometimes fail to capture the custom animal concepts while Spectral AdapterA recognizes all custom animals and generates visually satisfactory images. For better measurement, we also compute the alignment scores for each generated image with both reference images and prompt texts. It can be witnessed that our method achieves better alignment scores compared to baselines. See Appendix F.7 for details on alignment score computation. 4.3 Diffusion Model Expressiveness: Improving Parameter Efficiency with Spectral AdapterR Spectral AdapterR is closely connected to prior Orthogonal Fine-Tuning (OFT ) [45] method which proposes to multiply the pretrained model weights by trainable orthogonal matrices in the fine- tuning procedure. Motivation behind OFT is to preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. Unlike OFT which orthogonally rotates neurons, Spectral Adapter R multiplies the top- r columns of singular vector space U and V by orthogonal trainable matrices. For our implementation, several options are available for maintaining a trainable orthogonal matrix such as adding an orthogonality penalty in the objective function considered in [65] or via Cayley parameterization considered in [ 45]. We follow [ 45] and adopt Cayley parameterization which is supported by Pytorch [44]. Specifically, the orthogonal matrix R is 7constructed via R =(I +Q)(I −Q)−1 with a skew-symmetric matrix Q maintained as (A −AT )/2 where A is our trainable parameter. Compared to adding an auxiliary orthogonality penalty, this parametrization is exact and thus the SVD form is preserved after tuning with Spectral AdapterR and can be adopted directly for subsequent fine-tuning tasks, which we state formally as a lemma below: Lemma 4.1. With the Cayley parametrization, Spectral AdapterR is an exact rotation operation and thus preserves the structure of the SVD of the fine-tuned weight. Subsequent fine-tunings can be applied consequently without recomputing the SVD each time. See Appendix C for the proof of above lemma. Unlike LoRA which requires number of trainable parameters to scale with weight size, when tuning top-r columns of U an V , Spectral AdapterR only requires two trainable matrices of size r ×r and thus can be more parameter-efficient especially for large pretrained weight. For common weight size such as W ∈ R1024×1024, LoRA with only r = 1 introduces same number of trainable parameters as Spectral AdapterR with r =32. For a thorough analysis on parameter efficiency improvement brought by Spectral AdapterR, we here also compare with different variants of LoRA which are proposed for trainable parameter savings. We review all baselines in detail below. Baseline Review We compare our Spectral Adapter R with LoRA [ 20], SVDiff [ 15], LiDB [ 48], OFT [ 45], and VeRA [25]. Though the other methods are proposed for vision model tuning, VeRA is originally proposed for LLM tuning and we extend it here to diffusion model tuning due to its parameter efficiency. Consider a pretrained weight W ∈Rn×n, SVDiff originally proposes to tune all singular values of flattened CNN weights, here we extend it to tune all singular values of text encoder and U-Net weights for our comparison, thus trainable parameter attached to W will be of size n and is nonadjustable. LiDB stands for Lightweight Dreambooth and proposes to cut down trainable parameter budget by introducing auxiliary frozen matrixAaux ∈Rn×a and Baux ∈Rb×n, then it mimics LoRA but uses AauxABT Baux in replace of ABT with trainable (A ∈ Ra×r, B∈ Rb×r). Thus with a, b< n, LiDB requires (a +b)r < 2nr trainable parameters. In below, we use a = 50, b= 100 as default in [48]. OFT multiplies the weight matrix by a trainable orthogonal matrix via Cayley parametrization discussed above, thus its complete version requires n2 trainable parameters. For parameter efficiency, OFT proposes to use block-diagonal trainable matrix with all diagonal blocks being orthogonal. Thus with r diagonal blocks, the number of trainable parameter will be r ×(n/r)2. Method Granularity #Param Auxiliary Param LoRA / ∞ 2nr∝n noSVDiff / 1 n∝n noLiDB / ∞ (a+b)r∝r yes OFT / #factors ofn1 (n/r)2 ∝nr no VeRA / ∞ n+r∝n yes Spectral AdapterR , n 2r2 ∝r no 1 Ceiling operation is ignored for this count. Table 3: Baseline methods comparison for parameter effi- ciency. Granularity indicates number of trainable parameter budgets available. See Section 4.3 for details. Further reduction of trainable parame- ter is achieved via sharing the diagonal blocks, which demands only (n/r)2 parameters. In below comparison, we use this shared block-diagonal version for best parameter efficiency of OFT. VeRA proposes to use ΛaAΛbBT in replace of ABT where Λa and Λb are diagonal matrices of size n ×n and r ×r respectively. Thus the total num- ber of trainable parameters by VeRA is (n +r) ∝n. Table 3 compares dif- ferent properties across all methods, where n represents weight size and r represents rank for all methods except for OFT, where r denotes number of diagonal blocks. Parameter Efficiency We fine-tune the Chilloumix diffusion model [8] with various PEFT methods on custom vase concept and present the generation results for prompt \"a <Vvase> on a table\" in Figure 6 for various trainable parameter budgets, where grey dash denotes that the corresponding parameter budget is unobtainable with a given adapter no matter how the hyperparameter is chosen and empty entry without grey dash 8Figure 6: Generation results for prompt “a <Vvase> on a table” after fine-tuning Chilloutmix diffusion model [8] on custom vase images with different PEFT methods. See Section 4.3 for details. represents that there is a way to achieve the corresponding parameter budget though the generation result is skipped for better visualization. We follow default LoRA implementation in [12] for LoRA baseline and adjust it for all other methods. From Figure 6, it can be observed that LoRA, OFT, and LiDB start to generate vase close to custom vase with at least 200k trainable parameters. SVDiff and VeRA are unable to generate ideal vase images even if scaled to large parameter budget. On the contrary, Spectral AdapterR starts to recognize the custom vase concept with only 20k trainable parameters and has finer-grained parameter choices compared to other methods, i.e., notably Spectral AdapterR can have as few as1k parameters while other methods start with at least tens of thousands of trainable parameters. In a word, Spectral AdapterR enjoys finer-grained parameter budget choices and manifests better visual quality with fewer parameters, thus achieves enhanced parameter efficiency compared to various other PEFT methods. Figure 7: Generation results for prompt “a yellow <Vchair>” after fine-tuning Chilloutmix diffusion model [8] on custom chair images with different PEFT methods. Spectral R is abbreviation for Spectral AdapterR. See Section 4.3 for details. Figure 7 above presents generation results of Chilloutmix diffusion model [8] tuned on custom chair concept with different methods under various parameter budgets. The prompt used is \"a yellow <Vchair>\". See \"reference\" in Figure 7 for original chair images. From the generation results, it can be observed that LoRA generates reasonable chairs for all rank r =1, 2, 3 though it already induces 273k parameters even if rank is set to 1. OFT and VeRA start to recognize custom chair with >100k parameters. SVDiff has a single fixed trainable parameter budget of size around 100k. LiDB forms a competitive candidate and generates satisfactory images with smallest trainable parameter budget among all baseline methods. However, our Spectral AdapterR still generates images better aligned to 9reference images with as few as 20k trainable parameters and has finer-grained parameter budget choices compared to LiDB. See Appendix F.6 for hyperparameter setting and Appendix F.7 for alignment score computation details. 4.4 Final Note: A Closer Look at SVD Cost Figure 8: Runtime and GPU storage cost plot. See Section 4.4 for details. To alleviate the concerns with respect to online training cost and show that our pro- posed method is very practical, we provide runtime and GPU storage cost bar plot in Figure 8, which shows runtime and GPU storage cost for LoRA and for our Spec- tral AdapterA when used for fine-tuning diffusion model in Section 4.2 and Mistral 7B model in Section 4.1. Here we adopt rank r = 8 for both LoRA and Spectral AdapterA. It can be observed that our Spec- tral Adapter A introduces negligible run- time and storage overhead for current large model size. Modern numerical tools such as randomized SVD [13] can also be exploited for further runtime reduction and the SVD procedure can be paral- lelized when multiple machines are available. See Appendix E for further investigation. 5 Conclusion and Limitations In this work, we investigate the incorporation of spectral information of pretrained model weights into current PEFT models by introducing a spectral adaptation mechanism which updates only the top singular vectors of pretrained weights. We investigate the additive and rotational variants of such spectral adaptation mechanism. Theoretically, we show the motivation of tuning top singular vectors by comparing the rank capacity of different fine-tuning models and carrying out weight decomposition of pretrained model layers. Empirically, we verify the superiority of our proposed spectral adaptation method compared to various recent PEFT methods from different aspects via extensive experiments. To our best knowledge, this is the first work considering incorporating spectral information as a practical generic paradigm for fine-tuning tasks and enhances fine-tuning results, parameter efficiency, as well as benefits multi-adapter fusion of existing PEFT methods. For future work, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Other PEFT methods such as AdaLoRA [65] can also be dynamically combined with spectral adaptation. A limitation of the current work remains in the choice of tuning top spectral space. Though its validity has been theoretically verified under simple settings, further investigation on tuning different columns of singular vector matrices is critical to understanding the role of spectral information in fine-tuning procedure. Besides, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Moreover, the time consumption of singular value decomposition procedure increases as model grows larger and thus faster singular value decomposition method also benefits. 106 Acknowledgement This work was supported in part by the National Science Foundation (NSF) under Grant DMS- 2134248; in part by the NSF CAREER Award under Grant CCF-2236829; in part by the U.S. Army Research Office Early Career Award under Grant W911NF-21-1-0242; and in part by the Office of Naval Research under Grant N00014-24-1-2164. References [1] A. Aghajanyan, L. Zettlemoyer, and S. Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning, 2020. [2] A. Asai, M. Salehi, M. E. Peters, and H. Hajishirzi. Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts, 2022. [3] N. Cancedda. Spectral filters, dark signals, and attention sinks, 2024. [4] A. Chavan, Z. Liu, D. Gupta, E. Xing, and Z. Shen. One-for-all: Generalized lora for parameter- efficient fine-tuning, 2023. [5] Y . Chen, D. Hazarika, M. Namazifar, Y . Liu, D. Jin, and D. Hakkani-Tur. Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention. arXiv preprint arXiv:2205.03720, 2022. [6] A. Chronopoulou, M. E. Peters, A. Fraser, and J. Dodge. Adaptersoup: Weight averaging to improve generalization of pretrained language models, 2023. [7] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021. [8] C. M. Creator. Chilloutmix diffusion model. https://civitai.com/models/6424/chilloutmix. [9] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas. Predicting parameters in deep learning, 2014. [10] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023. [11] A. Edalati, M. Tahaei, I. Kobyzev, V . P. Nia, J. J. Clark, and M. Rezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter, 2022. [12] Y . Gu, X. Wang, J. Z. Wu, Y . Shi, C. Yunpeng, Z. Fan, W. Xiao, R. Zhao, S. Chang, W. Wu, Y . Ge, S. Ying, and M. Z. Shou. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. arXiv preprint arXiv:2305.18292, 2023. [13] N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions, 2010. [14] K. Hambardzumyan, H. Khachatrian, and J. May. Warp: Word-level adversarial reprogramming, 2021. [15] L. Han, Y . Li, H. Zhang, P. Milanfar, D. Metaxas, and F. Yang. Svdiff: Compact parameter space for diffusion fine-tuning, 2023. [16] S. Hayou, N. Ghosh, and B. Yu. Lora+: Efficient low rank adaptation of large models, 2024. [17] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig. Towards a unified view of parameter-efficient transfer learning, 2022. [18] S. He, R.-Z. Fan, L. Ding, L. Shen, T. Zhou, and D. Tao. Mera: Merging pretrained adapters for few-shot learning. arXiv preprint arXiv:2308.15982, 2023. [19] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. At- tariyan, and S. Gelly. Parameter-efficient transfer learning for nlp, 2019. 11[20] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models, 2021. [21] C. Huang, Q. Liu, B. Y . Lin, T. Pang, C. Du, and M. Lin. Lorahub: Efficient cross-task generalization via dynamic lora composition, 2024. [22] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014. [23] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023. [24] M. Khodak, N. Tenenholtz, L. Mackey, and N. Fusi. Initialization and regularization of factorized neural layers, 2022. [25] D. J. Kopiczko, T. Blankevoort, and Y . M. Asano. Vera: Vector-based random matrix adaptation, 2024. [26] T. Lei, J. Bai, S. Brahma, J. Ainslie, K. Lee, Y . Zhou, N. Du, V . Zhao, Y . Wu, B. Li, et al. Conditional adapters: Parameter-efficient transfer learning with fast inference. Advances in Neural Information Processing Systems, 36, 2024. [27] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning, 2021. [28] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021. [29] Y . Li, Y . Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and T. Zhao. Loftq: Lora-fine- tuning-aware quantization for large language models, 2023. [30] Z. Lin, A. Madotto, and P. Fung. Exploring versatile generative language model via parameter- efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020. [31] Q. Liu, X. Wu, X. Zhao, Y . Zhu, D. Xu, F. Tian, and Y . Zheng. Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications, 2023. [32] S.-Y . Liu, C.-Y . Wang, H. Yin, P. Molchanov, Y .-C. F. Wang, K.-T. Cheng, and M.-H. Chen. Dora: Weight-decomposed low-rank adaptation, 2024. [33] W. Liu, Z. Qiu, Y . Feng, Y . Xiu, Y . Xue, L. Yu, H. Feng, Z. Liu, J. Heo, S. Peng, Y . Wen, M. J. Black, A. Weller, and B. Schölkopf. Parameter-efficient orthogonal finetuning via butterfly factorization, 2023. [34] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang. Gpt understands, too, 2023. [35] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks, 2021. [36] C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning, 2018. [37] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication- efficient learning of deep networks from decentralized data, 2023. [38] A. Mitra, H. Khanpour, C. Rosset, and A. Awadallah. Orca-math: Unlocking the potential of slms in grade school math, 2024. [39] C. Mou, X. Wang, L. Xie, Y . Wu, J. Zhang, Z. Qi, Y . Shan, and X. Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models, 2023. [40] mrm8488. Lora finetune deberta-v3 huggingface blog, 2021. Available at https://huggingface.co/mrm8488/deberta-v3-small-finetuned-mnli/commits/main. [41] J. Pfeiffer, A. Kamath, A. Rücklé, K. Cho, and I. Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. 12[42] R. Po, G. Yang, K. Aberman, and G. Wetzstein. Orthogonal adaptation for modular customiza- tion of diffusion models, 2023. [43] D. Povey, G. Cheng, Y . Wang, K. Li, H. Xu, M. A. Yarmohammadi, and S. Khudanpur. Semi- orthogonal low-rank matrix factorization for deep neural networks. In Interspeech, 2018. [44] pytorch group. Pytorch orthogonal parameterization method implementation, 2023. [45] Z. Qiu, W. Liu, H. Feng, Y . Xue, Y . Feng, Z. Liu, D. Zhang, A. Weller, and B. Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning, 2023. [46] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision, 2021. [47] E. Robb, W.-S. Chu, A. Kumar, and J.-B. Huang. Few-shot adaptation of generative adversarial networks, 2020. [48] N. Ruiz, Y . Li, V . Jampani, W. Wei, T. Hou, Y . Pritch, N. Wadhwa, M. Rubinstein, and K. Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models, 2023. [49] A. Rücklé, G. Geigle, M. Glockner, T. Beck, J. Pfeiffer, N. Reimers, and I. Gurevych. Adapter- drop: On the efficiency of adapters in transformers, 2021. [50] T. N. Sainath, B. Kingsbury, V . Sindhwani, E. Arisoy, and B. Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6655–6659, 2013. [51] H. Skogström. Lora finetune mistral 7b valohai blog, 2024. https://valohai.com/blog/finetune- mistral/. [52] A. Tang, L. Shen, Y . Luo, Y . Zhan, H. Hu, B. Du, Y . Chen, and D. Tao. Parameter efficient multi-task model fusion with partial linearization, 2023. [53] K. Turgutlu. Answer.ai qdora report, 2024. https://www.answer.ai/posts/2024-04-26-fsdp-qdora- llama3.html. [54] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi. Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation, 2023. [55] T. Vu, B. Lester, N. Constant, R. Al-Rfou, and D. Cer. Spot: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021. [56] Z. Wang, R. Panda, L. Karlinsky, R. Feris, H. Sun, and Y . Kim. Multitask prompt tuning enables parameter-efficient transfer learning, 2023. [57] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks, 2024. [58] L. Xu, H. Xie, S.-Z. J. Qin, X. Tao, and F. L. Wang. Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment, 2023. [59] Y . Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, Z. Chen, X. Zhang, and Q. Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models, 2023. [60] A. X. Yang, M. Robeyns, X. Wang, and L. Aitchison. Bayesian low-rank adaptation for large language models, 2024. [61] F. Zhang and M. Pilanci. Riemannian preconditioned lora for fine-tuning foundation models, 2024. [62] F. F. Zhang, L. Li, J.-C. Chen, Z. Jiang, B. Wang, and Y . Qian. Increlora: Incremental parameter allocation method for parameter-efficient fine-tuning. ArXiv, abs/2308.12043, 2023. 13[63] L. Zhang, L. Zhang, S. Shi, X. Chu, and B. Li. Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning, 2023. [64] M. Zhang, H. Chen, C. Shen, Z. Yang, L. Ou, X. Yu, and B. Zhuang. Loraprune: Pruning meets low-rank parameter-efficient fine-tuning, 2023. [65] Q. Zhang, M. Chen, A. Bukharin, N. Karampatziakis, P. He, Y . Cheng, W. Chen, and T. Zhao. Adalora: Adaptive budget allocation for parameter-efficient fine-tuning, 2023. [66] Y . Zhang, E. Chuangsuwanich, and J. Glass. Extracting deep neural network bottleneck features using low-rank matrix factorization. In2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 185–189. IEEE, 2014. [67] H. Zhao, H. Tan, and H. Mei. Tiny-attention adapter: Contexts are more important than the number of parameters, 2022. [68] Y . Zhao, J. Li, and Y . Gong. Low-rank plus diagonal adaptation for deep neural networks. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5005–5009. IEEE, 2016. [69] Y . Zhu, J. Feng, C. Zhao, M. Wang, and L. Li. Counter-interference adapter for multilingual machine translation, 2021. [70] B. Zi, X. Qi, L. Wang, J. Wang, K.-F. Wong, and L. Zhang. Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices, 2023. 14Appendix A Prior Work Here we provide an overview of recent PEFT methods. Dating back to 2019, Houlsby et al. [ 19] develop the idea of parameter-efficient fine-tuning and introduce Adapter model, which injects trainable components between pretrained model layers, though the number of trainable parameters has been reduced due to the small size of adapters, this method incurs inference latency and is thus not desirable. Later improvement of Adapter fine-tuning focuses on improving inference latency [49, 26], fusing multiple adapters [6, 41, 18], modifying adapter model architecture [67], introducing parallelism [17, 69], and creating task-specific and layer-specific adapter [ 35, 30]. Another line of fine-tuning is prompt-tuning [27] which usually adds the trainable components into the prompt. Variants of prompt-tuning involve WARP [14], prefix-tuning [28], P-tuning [34], and ATTEMPT [2] which consider injecting different forms of trainable components. Multitask prompt-tuning is considered in [55, 56]. The more relevant PEFT methods to our spectral adaptation mechanism involves LoRA [20] and OFT [45], which inspires our Spectral AdapterA and Spectral AdapterR respectively. LoRA originates from the observation that model fine-tuning is intrinsically low-rank [1]. Variants of LoRA involve different methods proposing dynamic allocation of LoRA rank budgets [54, 62, 65, 5]. LoRA has been combined with model pruning [64] and quantization [10, 59, 29]. Some other variants further cut down the trainable parameter budget or activation storage by modifying LoRA model [25, 11, 63]. DoRA [32] fixes LoRA’s low-rank limitation by decomposing pretrained model weights and isolating their magnitudes. Laplace-LoRA [ 60] incorporates Bayesian inference into LoRA parameters to improve calibration. LoRAHub [21], MOELoRA [31], and L-LoRA [52] consider multitask LoRA. Delta-LoRA [70] updates pretrained weights simultaneously from information of LoRA parameters. GLoRA [4] generalizes LoRA by introducing a prompt module. Another line of variants focuses on analyzing the optimization scheme of LoRA model [ 61, 16]. OFT studies the multiplicative fine-tuning and its variant BOFT [33] improves OFT by utilizing butterfly parametrization for better information delivery efficiency. [58] offers a comprehensive review of recent development of PEFT methods. B Rank Capacity Proof Proof. Consider weight matrix W ∈ Rn×m with n ≤ m of full row rank. For LoRA parameter A ∈Rm×r, B∈Rn×r with n ≥r, final weight matrix W +ABT has rank in [n −r, n]. With Spectral AdapterA parameters AS ∈ Rm×r, BS ∈ Rn×r where n ≥ 2r. Let Xr denote the first r columns of any matrix X and X−r denote the rest columns, final weight matrix ((Ur +AS)Sr(Vr +BS)T )+ U−rS−rV T −r has rank in [n−2r, n]. Therefore, R(LoRA; W) =r and R(Spectral AdapterA; W) = 2r can be derived trivially. C Cayley Parameterization Proof Proof. With any trainable square matrix A, we set Q = (A −AT )/2 and thus Q = −QT and Q is skew-symmetric thereby. Now we show that for any skew-symmetric Q, (I +Q)(I −Q)−1 is orthogonal. Let O =(I +Q)(I −Q)−1, then OT O =((I +Q)(I −Q)−1)T (I +Q)(I −Q)−1 =(I −QT )−1(I +QT )(I +Q)(I −Q)−1 by Q skew-symmetric, =(I +Q)−1(I −Q)(I +Q)(I −Q)−1 since (I −Q) and (I +Q) have same eigen-basis and are commutable, =I, which shows that the Cayley parametrization is exact and no re-SVD is needed for orthogonality preservation. 15D Connection to DoRA In DoRA [32], the authors observe that plain LoRA method tends to either increase or decrease the magnitude and direction updates proportionally and thus lacks ability to make slight direction change together with large magnitude change, to come across this limitation, the authors propose to decompose pretrained model weights into magnitude and direction and update them separately. The magnitude is replaced with a trainable scalar and the direction is updated with original LoRA method. Experiments in [32] show that such decomposition helps improve effectiveness of LoRA significantly. Here we show that our Spectral AdapterA is closely connected to the weight decomposition trick used in DoRA when pretrained model weight is of vector form. We note that in DoRA, after the weight decomposition, each column becomes unit-length while in Spectral AdapterA, we also operates on matrices with unit-length columns. Specifically, consider a pretrained model weight w0 ∈Rn×1, then DoRA becomes w =w w0 +ba ∥w0 +ba∥2 , where w is a trainable scalar initialized at ∥w0∥2. band a are trainable parameters of size n ×1 and 1 ×1 respectively, with ba =0 at initialization. Comparably, Spectral AdapterA becomes w =( w0 ∥w0∥2 +a′)∥w0∥2(1 +b′), with trainable vectora′ ∈Rn×1 and trainable scalarb′ both initialized at zero. We can thus equivalently view ∥w0∥2(1 +b′) as a single trainable scalar initialized at ∥w0∥2, which then plays the role of magnitude adapter as w in DoRA. a′ is adopted for directional adaptation since it directly operates on the normalized base vector. E Cost Investigation (More Detailed) Here we address the potential concern about the overhead of our proposed spectral adaptation mechanism. Firstly, we note that spectral adapter introduces similar number of trainable parameters and can be merged into original model weights, thus it is lightweight for sharing and introduces no additional inference latency, which preserves the strengths of additive fine-tuning methods. Therefore, the major overhead concern exists in the runtime and GPU storage overhead during online training. Note our method involves only matrix multiplication in the forward procedure and thus should run as quick as LoRA. Though the SVD procedure can bring additional runtime overhead, it needs to be done only once for a single model and can be reused for later fine-tuning on various downstream tasks. Besides, modern numerical tools such as randomized SVD [ 13] can also be exploited and the SVD procedure can be parallelized when multiple machines are available. As for GPU storage, unlike SVDiff [15] where all SVD components are required for training procedure thus introducing significant GPU storage burden, our method requires only the top spectral space to be stored additionally and consumes similar GPU storage to LoRA for relatively small tuning ranks (which is usually the case). F Supplemental Materials for Experiments F.1 Experimental Setup for Figure 1 For Figure 1 experiments, we follow QDoRA [53] experimental setup for fine-tuning Llama3 8B model, where all k_proj, q_proj, v_proj, up_proj, down_proj, and gate_proj weights are tuned. We adopt the same data processing method and train on 10K Orca Math data (shuffled) as in [53]. We fix learning rate as 1e −5 for all methods as in QDoRA and train for one epoch with batch size 8. r =8 is adopted for LoRA, DoRA, AdaLoRA, and Spectral AdapterA while for OFT, we set number of diagonal blocks to be 800 to maintain similar amount of trainable parameters. LoRA alpha is set to be 16 following DoRA [32] convention and AdaLoRA hyperparameter is set following what has been used for MNLI benchmark in the original AdaLoRA report [65] with regularization set to 1e −3 which we find works better. For evaluation, we test on GSM8K [7] benchmark for exact matching. For more comparisons, Figure 9 provides training loss for smaller rank r = 4 (oft_r = 1600) and larger rank r =64 (oft_r =95). All settings are the same except that LoRA alpha is always kept as 16Figure 9: More experiments with Llama3 8B model with different number of trainable parameters. In the left plot, the training loss of LoRA and DoRA overlaps. See Appendix F.1 for details. twice as rank number. From Figure 9 we can observe that though increasing trainable parameters closes the gap between different tuning methods, our spectral adapter method is always superior to other PEFT methods and stays closest to full fine-tuning. F.2 Hyperparameter Setting for DeBERTaV3-base Experiment (Section 4.1) Dataset learning rate batch size #epochs optimizer weight decay MNLI 1e −4 32 1 AdamW 0.01 RTE 3e −4 32 10 AdamW 0.01 QNLI 1e −4 32 1 AdamW 0.01 MRPC 7e −4 32 13 AdamW 0.01 QQP 1e −4 32 10 AdamW 0.01 SST-2 1e −4 32 5 AdamW 0.01 CoLA 3e −4 32 8 AdamW 0.01 STS-B 5e −4 32 30 AdamW 0.01 Table 4: Hyperparameters for DeBERTaV3-base model fine-tuning with Spectral AdapterA in Section 4.1 Table 4 shows the hyperparameter setting for our Spectral AdapterA used for fine-tuning DeBERTaV3- base model in Section 4.1. We set number of diagonal blocks to be 4 and enable block sharing for OFT to maintain similar amount of trainable parameters. F.3 More About DeBERTaV3-base Experiment Left plot in Figure 10 presents the training loss and validation score comparisons of LoRA, SVDiff and our Spectral AdapterA for fine-tuning DeBERTaV3-base model on CoLA benchmark. We set learning rates for both LoRA and Spectral AdapterA as what has been used in popular public blog [40] for LoRA fine-tuning with DeBERTaV3-base model, which is not tuned in favor of our method. For SVDiff, since it is originally proposed for vision model tuning, we extend it to this experiment by tuning all singular values of pretrained weights. We find the same learning rate leads to poor fine-tuning results with SVDiff, we thus pick the best learning rate among [1e −3, 1e −4, 1e −5] according to validation performance and set learning rate to be 1e −3. We use r = 8 for LoRA and Spectral AdapterA. From Figure 10, it can be observed that Spectral AdapterA achieves better training and validation performance compared to both LoRA and SVDiff. Interestingly, in LoRA [20], the authors provide a correlation analysis between the LoRA additive component △W = ABT and original pretrained weight matrix W (see Section H.3 in [ 20]), and they find that the additive component does not contain the top singular directions of W. The authors therefore conclude that the learned LoRA component amplifies \"task-specific\" directions which are not emphasized in the pretrained weight matrix. Naively, this seems to suggest that tuning top singular subspace of pretrained weights is not ideal and one should identify the desired \"task-specific\" directions to improve LoRA. Here we show that this is not the case and fine-tuning top directions provides a significant improvement to LoRA. In the right plot of Figure 10 above, we experiment 17Figure 10: Left plot presents training loss and validation results for fine-tuning DeBERTaV3-base model with LoRA, SVDiff, and Spectral AdapterA on CoLA benchmark. Right plot compares the same statistics between LoRA and spectral adapter with top ranks and bottom ranks tuned respectively. tuning the top eighth rank and the bottom eighth rank of singular vector space in our Spectral AdapterA, which we present as \"Spectral Top\" and \"Spectral Bottom\" respectively. Remarkably, \"Spectral Top\" converges faster and scores higher than LoRA, which is then superior to \"Spectral Bottom\". This result unravels the fact that tuning different part of spectral space brings different tuning effect and tuning the top columns of singular vector space improves LoRA tuning significantly. See Section 3 for more theoretic insights. F.4 Hyperparameter Setting for Mistral 7B Experiment (Section 4.1) Method lr lora alpha batch size #epochs lora dropout weight decay LoRA 2.5e −5 16 4 2 0.05 0.01 DoRA 2.5e −5 16 4 2 0.05 0.01 Spectral AdapterA 2.5e −5 - 4 2 - 0.01 Table 5: Hyperparameters for Mistral 7B model fine-tuning task in Section 4.1 Table 5 shows training hyperparameter setting for fine-tuning Mistral 7B model in Section 4.1. We train with bfloat16 precision and fine-tune all q_proj, k_proj, v_proj, o_proj, and gate_proj weights. We evaluate with lm-evaluation-harness [47]. Table 6 shows accuracy comparison of different tuning methods with learning rate 1e −5. Our Spectral AdapterA still exceeds both LoRA and DoRA. F.5 Supplemental Materials for Multi-Adapter Fusion Experiment (Section 4.2) F.5.1 Comparison of Single Object Generation We present more experimental results to show that Spectral AdapterA with top ranks tuned behaves at least as good as LoRA with same parameter budget and is better than Orthogonal Adaptation [42], which is likely due to that Orthogonal Adaptation fixes LoRA parameter B and thus has limited expressiveness. We also show that tuning bottom ranks in spectral adapter behaves worse than all other methods. Figure 11 shows generation results for custom toy concept tuning, where Orthogonal Adaptation and Spectral AdapterA (bottom) generate inaccurate happy-face octopus, sad-face octopus, and green tortoise. Figure 12 shows generation results for custom animal concept tuning, where Orthogonal Adaptation and Spectral AdapterA (bottom) sometimes miss first dog concept. Method #Param GSM8K Pre-Trained − 38.82 LoRAr=8 0.16% 43.29 ±1.36 DoRAr=8 0.17% 43.52 ±1.37 SpectralA r=8 0.16% 46.47 ±1.37 Table 6: Supplemental experiments of fine-tuning Mistral 7B model with different PEFT methods with a different learning rate on GSM8K benchmark. See Section F.4 for experimental details. 18Figure 11: Generation results for single toy concept tuning with LoRA, Orthogonal Adaptation, and Spectral AdapterA with top and bottom ranks tuned respectively. F.5.2 More Multi-Adapter Fusion Generation Results Here we present more results for multi-adapter fusion generation. Figure 13 shows generation results for multi-object generation for custom toy concepts and Figure 14 presents generation results for multi-character generation for three computer scientists. See below for experimental details. Multi-Object Generation. As in Section 4.2, we fine-tune Chilloutmix diffusion model [8] on four custom toy concepts, see \"reference\" in Figure 13 for original toy images. We use r =8 for all methods and tune first, second, third, and fourth top eighth columns of singular vector space of pretrained weights for first, second, third, and fourth toys in our Spectral AdapterA. We follow all default experimental settings in [ 12] and tune all embedding layer, U-Net, and text-encoder. For better spatial alignment, we employ T2I-Adapter with sketch condition listed in \"reference\" in Figure 13. We randomly select three scenes and prompt fused-adapters for the results, see \"prompts\" in Figure 13 for individual prompt being used. From Figure 13, it can be observed that FedAvg and Orthogonal Adaptation generate unsatisfactory happy-face octopus and green tortoise toys. On the contrary, our spectral adapter generates high-quality images similar to Gradient Fusion while saving 19Figure 12: Generation results for single animal concept tuning with LoRA, Orthogonal Adaptation, and Spectral AdapterA with top and bottom ranks tuned respectively. Figure 13: Generation results of Chilloutmix diffusion model [8] tuned on four custom toy concepts with different fused adapters. See Appendix F.5.2 for details. much more time. Multi-Character Generation. We also experiment fine-tuning Chilloutmix diffusion model [ 8] with photos of three computer scientists Yoshua Bengio, Yann LeCun, and Geoffrey Hinton. As in multi-object generation, we use r = 8 for all methods and tune first, second, and third top eighth columns of singular vector space of pretrained weights for Bengio, Lecun, and Hinton in our Spectral AdapterA. We use T2I-Adapter [ 39] with keypose condition. See \"reference\" in Figure 14 for scientists’ photos and keypose condition being used. Figure 14 shows generation results for prompt 20\"<Vbengio> and <Vlecun> and <Vhinton>, standing near a lake, 4K, high quality, high resolution\" with different fused adapters, from which it can be observed that our spectral adapter generates picture of most consistent styles across characters and renders all scientists’ faces clearly. Figure 14: Generation results of Chilloutmix diffusion model [8] tuned on photos of three computer scientists with different fused adapters. See Appendix F.5.2 for details. F.6 Supplemental Materials for Parameter Efficiency Experiment (Section 4.3) In this section, we present more tuning results with various parameter budgets for parameter efficiency experiment studied in Section 4.3, see Section 4.3 for baseline method explanation. Table 7 shows the learning rates used for each baseline method and Table 8 shows learning rates used for our method, the rest experimental settings are default as in [12]. Method text encoder lr unet lr LoRA 1e −5 1e −4 VeRA (r =1) 1e −3 1e −4 VeRA (r =1024, 4096) 5e −3 1e −4 OFTA 1e −5 1e −4 LiDB 5e −4 1e −4 SVDiff 1e −3 1e −4 Table 7: Hyperparameters for baseline methods for diffusion model fine-tuning task in Section 4.3 Method vase chair table text unet text unet text unet Spectral AdapterR (r =2, 40) 1e −3 1e −2 1e −2 1e −2 1e −3 1e −2 Spectral AdapterR (r =4) 5e −3 5e −3 1e −3 1e −2 Spectral AdapterR (r =8) 5e −4 5e −2 1e −3 1e −2 1e −3 1e −2 Spectral AdapterR (r =16) 1e −2 1e −3 1e −3 1e −2 Spectral AdapterR (r =24) 1e −4 1e −2 1e −3 1e −3 1e −4 1e −2 Spectral AdapterR (r =32) 1e −4 5e −2 Table 8: Hyperparameters for Spectral AdapterR for diffusion model fine-tuning task in Section 4.3 Figure 15 shows generation results of Chilloutmix diffusion model [8] fine-tuned on custom table concept with different methods under various parameter budgets. The prompt used is “a <Vtable>”. LoRA generates acceptable images for all rank r =1, 2, 3 though it starts with 273k parameters even if rank is set to 1. OFT generates desirable images only for parameter budget > 400k. VeRA and LiDB start to generate reasonable images with >300k trainable parameters and SVDiff has only a single fixed parameter budget. Meanwhile, our Spectral AdapterR recognizes the shape of custom table with as few as 6k parameters and produces ideal images since 100k parameters. See Appendix F.7 for alignment score computation details. 21Figure 15: Generation results for prompt “a <Vtable>” after fine-tuning Chilloutmix diffusion model [8] on custom table images with different PEFT methods. Spectral R is abbreviation for Spectral AdapterR. See Appendix F.6 for details. F.7 Alignment Score Computation For better quantitative measurement, we compute alignment scores for our Figure 5,6,7,15 results. Specifically, we first compute CLIP [46] embedding for all generated/reference images and prompt texts, then we compute the cosine similarity between generated images’ embedding and reference images’ embedding to serve as their alignment score. Likewise, text score stands for cosine similarity between generated images’ embeddings and their corresponding prompt texts’ embeddings. Intuition here is that if an image is close to another image (or text), their CLIP vectors are expected to stay close as well. For Figure 5 alignment score computation, we crop each generated image vertically into three columns, then we compute their alignment scores to each corresponding reference animal, we finally take the mean of these three scores. For Figure 6, 7, 15 scores, we compute average score over three random trials, with each trial consisting of 8 generated images. 22",
      "meta_data": {
        "arxiv_id": "2405.13952v2",
        "authors": [
          "Fangzhao Zhang",
          "Mert Pilanci"
        ],
        "published_date": "2024-05-22T19:36:55Z",
        "pdf_url": "https://arxiv.org/pdf/2405.13952v2.pdf"
      }
    },
    {
      "title": "Adapters Strike Back",
      "abstract": "Adapters provide an efficient and lightweight mechanism for adapting trained\ntransformer models to a variety of different tasks. However, they have often\nbeen found to be outperformed by other adaptation mechanisms, including\nlow-rank adaptation. In this paper, we provide an in-depth study of adapters,\ntheir internal structure, as well as various implementation choices. We uncover\npitfalls for using adapters and suggest a concrete, improved adapter\narchitecture, called Adapter+, that not only outperforms previous adapter\nimplementations but surpasses a number of other, more complex adaptation\nmechanisms in several challenging settings. Despite this, our suggested adapter\nis highly robust and, unlike previous work, requires little to no manual\nintervention when addressing a novel scenario. Adapter+ reaches\nstate-of-the-art average accuracy on the VTAB benchmark, even without a\nper-task hyperparameter optimization.",
      "full_text": "Adapters Strike Back Jan-Martin O. Steitz1 Stefan Roth1,2 1Department of Computer Science, TU Darmstadt 2 hessian.AI Abstract Adapters provide an efficient and lightweight mechanism for adapting trained transformer models to a variety of dif- ferent tasks. However, they have often been found to be outperformed by other adaptation mechanisms, including low-rank adaptation. In this paper, we provide an in-depth study of adapters, their internal structure, as well as vari- ous implementation choices. We uncover pitfalls for using adapters and suggest a concrete, improved adapter architec- ture, called Adapter+, that not only outperforms previous adapter implementations but surpasses a number of other, more complex adaptation mechanisms in several challenging settings. Despite this, our suggested adapter is highly robust and, unlike previous work, requires little to no manual inter- vention when addressing a novel scenario. Adapter+ reaches state-of-the-art average accuracy on the VTAB benchmark, even without a per-task hyperparameter optimization.† 1. Introduction Transfer learning from an off-the-shelf model, pre-trained on a large dataset like ImageNet [55] to a downstream task by fully fine-tuning the model’s parameters is a common paradigm. A typical CNN architecture, like a ResNet [24], has several tens of millions of parameters. However, since the introduction of transformers [59] into the realm of com- puter vision [4, 5, 13, 51, 52, 63], model sizes have grown exponentially from around a hundred million parameters for a vision transformer (ViT) [ 13] to more than a billion parameters [10, 46]. This leads to huge storage requirements when fine-tuning on multiple downstream tasks because a complete set of the model’s parameters needs to be saved per task. Additionally, large models require correspondingly large datasets [e.g., 56] to be trained to their full potential, yet tend to overfit easily if the target dataset in transfer learn- ing is too small. One solution is linear probing [12], where only the linear classifier is trained, but this usually yields inferior results compared to full fine-tuning. As a consequence, there is a growing interest in parameter- efficient tuning methods. The main idea is to freeze the †Code is available at https://github.com/visinf/adapter_plus. 0 0.1 0.2 0.3 0.4 0.5 0.6 72 74 76 78 Fine-tuning accuracy Linear probing # Parameters (M) Accuracy (%) Adapter+ (ours) SPT -Adapter [21]SSF [39] Adapter+ opt. (ours)SPT -Adapter⟳ SSF⟳ FacT -TK [32] Consolidator [20]VPT [31] FacT -TK⟳ LoRA [29] VPT⟳ Figure 1. Parameter-accuracy characteristics of adaptation methods on the VTAB [65] test sets. We report original results and re-evaluations ( ⟳) after a complete training schedule with suitable data normalization. Our Adapter+ has clearly the best parameter-accuracy trade-off. The vertical, dashed line shows the possible minimal number of tunable parameters when only the classifiers are trained, i.e., using linear probing (61% accuracy). parameters of the pre-trained model and add a compara- tively small amount of parameters to the model, which are then tuned together with the classifier to adapt the model to the downstream task at hand. Representative methods with different underlying concepts include VPT [31], which prepends the sequence of image tokens in the attention with trainable tokens to learn a prompt tuning, LoRA [29], where the attention weights are updated with learnable low-rank decomposition matrices, and Adapters [28], which are small bottleneck modules that are added to every transformer layer of the network. Adapters were first proposed for CNNs by Rebuffi et al. [53] and various formulations [22, 28, 49] exist for the now common ViT architecture. Recent work on parameter-efficient transfer learning [e.g., 20, 21, 31, 32, 39, 67] presents adapters as a baseline method for the adaptation to downstream tasks in computer vision. However, we identified various common issues in their imple- mentations, which we find to have a negative influence on the To appear in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, W A, USA, 2024. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. arXiv:2406.06820v1  [cs.CV]  10 Jun 2024LoRAVPT ⟳ SSF ⟳ FacT ⟳ Consol.SPT ⟳ Adapter+ 80 81 82 83 84 82.4 82.5 80.3 82.6 82.4 82.2 84.0 Accuracy (%) Natural LoRAVPT ⟳ SSF ⟳ FacT ⟳ Consol.SPT ⟳ Adapter+ 85 86 84.3 84.6 85.7 84.7 86.3 85.3 86.5 Accuracy (%) Specialized LoRAVPT ⟳ SSF ⟳ FacT ⟳ Consol.SPT ⟳ Adapter+ 58 60 62 60.1 62.1 58.0 62.3 60.9 60.5 63.3 Accuracy (%) Structured Figure 2. Average accuracy for VTAB subgroups on thetest sets. For methods marked with ⟳, we report results of our re-evaluation after a complete training schedule with suitable data normalization to ensure a fair comparison. Adapter+ is evaluated with rank r∈[1..32]. adaptation performance. For further details, refer to the sup- plemental material. Additionally, while adapters have been well studied in natural language processing (NLP), there is no study that broadly examines the different adapter config- urations for vision transformers. As a result, adapters have seemed to underperform in comparison to recent parameter- efficient adaptation methods, e.g., reported accuracies of adapters on VTAB of 73.9% in [67] and 60.8% in [31]. In this work, we therefore revisit the idea of adapters and investigate how they can perform at their best in con- nection with ViTs. Our contribution hereby is threefold: (1) We show the first in-depth and systematic study on the effects of the adapter position in the transformer and of the adapter’s inner structure with ViTs, as well as evaluate differ- ent variants of parameter initialization. (2) We further pro- pose a learnable, channel-wise scaling as extension to plain adapters, which proves to be beneficial for computer vision tasks. (3) Finally, we present Adapter+, an adapter configu- ration with an excellent parameter-accuracy trade-off com- pared to other work, as shown in Fig. 1. Adapter+ reaches a state-of-the-art average accuracy of 77.6% on VTAB [65] without any hyperparameter optimization per task and 3.7 percentage points (pp) over previous adapter baselines. We also reach a state-of-the-art accuracy of 90.7% on FGVC [31] with the lowest number of parameters compared to other methods. Finally, Adapter+ shows the best robustness in terms of accuracy across the VTAB subgroups, see Fig. 2. 2. Related work One possibility to adapt a pre-trained network to a novel task, apart from full fine-tuning, is to only selectively tune some of the parameters, e.g., only training the classifier [12]. Cai et al. [3] proposed to tune only the biases of an otherwise frozen network to adapt it to a downstream task. BitFit [ 64] then showed the efficacy of this method for NLP transformers. Modular adaptation. The concept of adding small, train- able modules with only a few parameters to an otherwise frozen network was first proposed for adapting CNNs by Rebuffi et al. [53] and called adapters. Other approaches replaced all convolutions in the network with depth-wise separable convolutions and only tuned their spatial parts [19], learned binary masks to prune a pre-trained network per target task [ 41], or created a student network by aug- menting the original network with adapter-like modules and skip connections, which then mimicked a teacher network by disabling parts of its pre-trained and added modules [43]. Following the rise of transformers in NLP [ 11, 50, 59], Houlsby et al. [28] proposed adapter modules in the form of bottlenecks for transformer layers. Pfeiffer et al. [49] conducted an architecture search on NLP tasks to find a more parameter-efficient configuration of adapter modules that only acts on the transformer’s feed-forward network (FFN), thus saving roughly half of the parameters over [28]. Prompt tuning. Inspired by changing the output of a net- work for NLP with hand-crafted textual prompts, which modifies the attention over the original input tokens, Lester et al. [37] proposed prompt tuning: A set of learnable to- kens is added to the input sequence and trained with back- propagation to prompt a frozen language model to perform downstream tasks. Li and Liang [38] extended on prompt tuning by adding learnable tokens at every transformer layer of the model, which they termed prefix tuning. Jia et al. [31] applied prompt tuning to vision transformers, then called visual prompt tuning (VPT), by preprending the sequence of image patch embeddings with such trainable tokens (VPT- Shallow). They also showed a variant resembling prefix tuning with stronger adaptation capabilities that adds tokens at every layer of the network (VPT-Deep). Low-rank approaches. Also focusing on the attention part of the transformer layers, Hu et al. [29] proposed low-rank adaptation (LoRA) where the attention weights are updated with low-rank decomposition matrices. The matrices can be merged with the attention weights for inference. The structure of LoRA is very similar to an adapter, which can be seen as a superset of LoRA acting on the transformer’s FFN. He et al. [22] proposed a formalism to unify LoRA, adapters, and prefix tuning [ 38]. It allowed them to combine the beneficial aspects of all three methods into a scaled parallel adapter (Scaled PA) for NLP tasks. AdaptFormer [6] then applied the concept of Scaled PA to vision transformers. 2Other related work. Newer approaches for vision trans- formers proposed different techniques to further enhance the parameter-accuracy trade-off in adaptation. NOAH [67] per- forms an architecture search for a combination of adapters, LoRA, and VPT for each task. SSF [39] scales and shifts the features in the network after every operation, i.e., attention, FFN, layer normalization, with task-specific, trainable mod- ules. Jie and Deng [32] aggregate the weights of a ViT into a single 3D tensor. Task-specific weight updates of this tensor are learned as a matrix decomposed into parameter-efficient factors, hence they termed their method factor-tuning (FacT). SPT [21] measures the importance of the weights of a pre- trained network for a downstream task. Based on a desired parameter budget, the most important parameters are chosen for tuning and adapters or LoRA are used for weight matrices that contain enough parameters of importance. Consolidator [20] adapts weights in multiple orderings of channel-wise groups. The updates for all groups are merged for efficient storage and inference. Despite these new developments, we show that the sim- ple concept of adapters exhibits an even better parameter- accuracy trade-off in combination with vision transformers – if done right and with the addition of a channel-wise scaling. 3. Adapters for vision transformers 3.1. Vision transformer basics In this work, we concentrate on the parameter-efficient adaptation of vision transformers (ViT) [ 13]. The ViT is closely modeled after the transformer model for natural lan- guage processing (NLP) proposed by Vaswani et al. [59]. A learned linear projection embeds non-overlapping and flat- tened patches of the input image into a sequence of n tokens x ∈ Rn×d, where d is called the hidden dimension of the transformer. A positional encoding is added to the embed- dings and the sequence is prepended with a trainable [CLS] token. The sequence length and the dimension of the tokens stay fixed throughout the architecture. The sequence is sent through consecutive transformer layers that each consist of a multi-head self-attention and a feed-forward network (FFN). For the self-attention, the tokens are projected to queries, keys, and values (Q, K, and V ) and the output of each of the M attention heads is calculated as Attention(x) =Softmax \u0012Q(x)K(x)T √ d′ \u0013 V (x), (1) with d′ = d/M being the inner dimension of the head. The FFN consists of a multilayer perceptron with two linear layers (with weights Wi and biases bi) and a GELU [ 26] non-linearity as activation in between: FFN(x) =GELU(xW1 + b1)W2 + b2. (2) Both attention and FFN are employed with a preceding layer normalization (LN) [1] and a skip connection and, therefore, transform an input sequence x sequentially as x 7→ Attention(LN(x)) +x (3a) x 7→ FFN(LN(x)) +x. (3b) To keep the notation concise, we will omit the LNs of atten- tion and FFN in the following; each attention and FFN is assumed to be always preceded by an LN. 3.2. Adapters and their inner structure Adapters [28] are small modules that are added to the trans- former layers. They allow to tailor a network to a new task or domain, where instead of tuning the parameters of the whole network, only the adapter parameters and the classi- fier are trained. Adapters take the form of bottlenecks with an inner dimension of r ≪ d. We call r the rank of the adapter. In detail, a down-projection to dimension r with weights Wdown ∈ Rd×r and biases bdown ∈ Rr is followed by a non-linear activation function σ(·), typically a GELU [26] as used throughout the ViT, and an up-projection with weights Wup ∈ Rr×d and biases bup ∈ Rd back to the hid- den dimension d of the transformer layer. This yields a base adapter module Adapterbase(x) =σ(xWdown + bdown)Wup + bup . (4) The base adapter module can be further enhanced with a normalization layer, e.g., a layer normalization (LN) [ 1]. Additionally, the output of the bottleneck can be scaled by s as Adapter(x) =s · Adapterbase \u0000 LN(x) \u0001 . (5) For layer-wise scaling, the factor s is taken to be a scalar, i.e. s ∈ R, and can be either fixed as a hyperparameter or learned during training. Layer-wise scaling was proposed by He et al. [22] and Hu et al. [29] but deemed not effective compared to a fixed scaling for tasks in NLP. Here, we additionally propose to use a channel-wise, learned scaling where s ∈ Rd. We investigate its capabilities in Sec. 4.3. In most cases, the adapter is used with a skip connection, hence the complete feature transformation becomes x 7→ Adapter(x) +x. (6) The complete inner structure of an adapter including its skip connection is visualized in Fig. 3a. 3.3. Adapter positions Although the architecture of bottleneck adapters for trans- formers is rather simple, there are various ways to plug them into the transformer layer. Previous work has not yet in- vestigated what the optimum position is for the use with a ViT [13]. Here, we evaluate four possible adapter positions, shown in Figs. 3b to 3e. We postulate that it is easier for an adapter to learn to modify features previously transformed 3FF down FF up Act LN Scaling (a) Inner structure FFN Adapter (b) Pre FFN Adapter (c) Post FFN Adapter (d) Parallel FFN Adapter (e) Intermediate Figure 3. Illustrations of (a) the inner structure of an adapter with feed-forward layers (FF), activation layer (Act), and optional layer normalization (LN) and scaling, (b)–(d) different possible adapter positions to connect the adapter to the FFN section of the transformer layer. Modules with trainable parameters are shown in red and frozen modules in blue. by a frozen module in the network rather than to anticipate what changes to the features are needed in adapting for a frozen module that follows the adapter. Putting it differently, we argue that the adapter should follow a frozen module. Pre-Adapter. The first adapter position we analyze ap- plies the adapter to the output x of the attention section of the transformer layer before it is passed into the FFN, but with the skip connection of the attention already added (Fig. 3b). The feature transformation of the FFN section with the adapter attached, therefore, becomes x 7→ FFN \u0000 Adapter(x) +x \u0001 + \u0000 Adapter(x) +x \u0001 . (7) Note that the two occurrences of Adapter(x) in Eq. (7) refer to the same instantiation. In this configuration, the adapter has the full information from the feature transformation hap- pening in the attention but needs to estimate the transforma- tion that will be happening in the FFN that follows. As a result, especially the last FFN before the linear classifier will be hard to adapt. To the best of our knowledge, this adapter position has not been considered in the literature. Post-Adapter. In this case, the adapter is positioned at the very end of the transformer layer on the output of the FFN with its skip connection added as x 7→ Adapter \u0000 FFN(x) +x \u0001 + \u0000 FFN(x) +x \u0001 , (8) where the FFNs refer to the same intantiation (Fig. 3c). That way, the adapter has access to the feature transformation happening in the FFN and the unmodified features via the skip connection. This position has been proposed by Pfeiffer et al. [49] as the result of an architecture search, but only for adapting transformers for NLP tasks and not for a ViT. Parallel-Adapter. Next, we consider a parallel setting as proposed by [22], where the adapter is located parallel to the FFN and both share a skip connection (Fig. 3d): x 7→ FFN(x) +Adapter(x) +x. (9) Therefore, both adapter and FFN work on the output of the attention section of the transformer layer and the adapter needs to learn the necessary residual transformation to the one produced by the frozen FFN. Intermediate-Adapter. Finally, we consider the original adapter position as proposed by Houlsby et al. [28]. The adapter is plugged behind the FFN but before the skip con- nection of the FFN is added (Fig. 3e). The adapter addition- ally possesses its own skip connection: x 7→ Adapter \u0000 FFN(x) \u0001 + FFN(x) +x. (10) Note that the two occurrences of FFN(x) in Eq. (10) refer to the same instantiation. The adapter sees the transformed features coming from the FFN but cannot access the features added later on by the skip connection of the FFN. 3.4. Initialization of adapter parameters Since training a deep learning model is a non-convex opti- mization problem, the initialization of parameters is impor- tant. In this work, we evaluate three different variants of parameter initializations for adapters proposed in the litera- ture. All of them have the goal to initialize the adapters in a way that minimizes the initial influence of the adapters at the start of their training. This is a sensible goal since adapters extend an already pre-trained frozen network. Houlsby initialization. Houlsby et al. [28] propose to draw the weights of the projection matrices from a zero-centered Gaussian distribution with a standard deviation of σ = 0.01, truncated at 2σ, and use zero for their biases. BERT initialization. For the BERT model [11], the initial- ization works similar to [28] but the Gaussian distribution has a standard deviation of σ = 0.02 and is not truncated. This form of initialization is used by Pfeiffer et al. [49]. LoRA initialization. LoRA [29] initializes the weights and biases of the down-projection with a uniform Kaiming He ini- tialization [23]; the weights and biases of the up-projection 4are initialized to zero. Therefore, the output of the adapter at the beginning of training equals zero and the adapter initially does not contribute. 3.5. Data normalization in pre-processing Data normalization is common practice during image pre- processing. It is typically done by shifting and scaling of each input pixel xij for each channel c as ˆxijc = (xijc − µc)/σc . (11) Most widely used are the mean µ = (0.485, 0.456, 0.406)T and standard deviation σ = (0.229, 0.224, 0.225)T of the ImageNet dataset [55], commonly referred to as ImageNet normalization. Another option is using 0.5 for every element of µ and σ, which is commonly referred to as Inception normalization because it is used for the Inception family of CNN architectures, starting with Inception-v3 [58]. The Im- ageNet normalization aims to center the input data around 0 with a standard deviation of 1. The Inception normalization, on the other hand, transforms the input values such they are strictly in range [−1, 1]. Because we try to adapt to a target domain on a very low parameter budget, it is important to use the data normaliza- tion the network saw during its pre-training. Otherwise, the parameter-efficient transfer method of choice needs to first compensate for the shift in input data statistics and loses parts of its capacity to adapt to the target domain. 4. Experiments 4.1. Datasets In order to carry out a detailed study of the utility of adapters in the context of ViT models, we experiment with two stan- dard benchmarks for task adaptation. VTAB. The Visual Task Adaptation Benchmark (VTAB) [65] consists of 19 tasks, which are further grouped into three categories: Natural, Specialized, and Structured. The Natural group contains natural images captured using stan- dard photographic equipment. The Specialized group is built from datasets of images captured with specialized equip- ment, from remote sensing and medical domains. Lastly, the Structured group is for evaluating the understanding of the scene structure. Here, the majority of the datasets are compiled from synthetic images with scenes that are easy to assess for humans but have a large domain gap to natural image datasets. Each task of VTAB consists of 800 train- ing and 200 validation images. The test sets have the same number of images as the test sets in the original datasets. FGVC. Following Jia et al. [31], we compile five datasets for fine-grained visual classification (FGVC): CUB-200- 2011 [61], NABirds [ 27], Oxford Flowers [ 45], Stanford Dogs [34], and Stanford Cars [17]. Because VTAB bench- marks task adaptation in a low-data regime in terms of the Table 1. Adapter position. We report the average accuracy in % (± std. dev.) on the VTABval sets for different adapter positions. Adapterbase with Houlsby initialization and rank r=8 is used in all experiments. Position Natural Specialized Structured Average Pre 82.4 ± 0.4 86.2 ± 0.8 57.5 ± 0.5 75.3 ± 0.3 Intermediate 83.0 ± 0.4 85.0 ± 0.8 57.2 ± 0.5 75.1 ± 0.3 Parallel 83.0 ± 0.3 86.2 ± 0.6 57.7 ± 0.6 75.6 ± 0.3 Post 83.0 ± 0.3 85.7 ± 0.4 59.1 ± 0.3 76.0 ± 0.2 number of available training images, we use FGVC to eval- uate adaptation methods in settings where training data is abundant. Where validation sets are not available in FGVC, we follow Jia et al. [31] to create the validation splits. For further details regarding the dataset properties of VTAB and FGVC, see supplemental material. 4.2. Experimental settings For all our experiments, we use a ViT-B/16 network [13] that was pre-trained on ImageNet-21k [55]. We follow its pre- training settings, in particular, regarding input data normal- ization. We train all models with an AdamW [40] optimizer with a learning rate of 10−3, a weight decay of 10−4, and a batch size of 64, following [ 67]. For full fine-tuning, we use a learning rate of 10−4, which we found leads to better results. We use a cosine learning rate schedule with a linear warm-up over the first 10 epochs and train for 100 epochs in total. We use stochastic depth with linearly increasing drop rates as a function of network depth from 0 to 0.1 for the frozen network and with a drop rate of 0.1 for the adapters during training. Apart from data normalization (cf . Sec. 3.4), we resize input images to 224×224 px for VTAB and use a randomly resize crop to 224×224 px and horizontal flipping for FGVC. For the ablations and to determine hyperparam- eters, we evaluate on the validation splits. We include the validation sets in the training data for producing final results. 4.3. Exploring adapter configurations Adapter position. We first evaluate the four possible posi- tions to connect an adapter to the FFN section of the trans- former layer, as described in Sec. 3.3. In our ablation, we use Adapterbase (cf . Eq. (4)) with rank r=8 and use the Houlsby initialization. In this experiment, the adapters neither have a layer normalization nor use scaling. The results on the VTAB validation set for all four adapter positions are presented in Tab. 1. The Post-Adapter yields the best result with 76.0% average accuracy over all VTAB subgroups. It confirms our hypothesis that the adapter should follow the frozen FFN module because it can then post-hoc modify the features flowing through the network. The par- allel configuration comes in second with 75.6% average accuracy, receiving the same input as the FFN but having to 5Table 2. Inner adapter structure. We evaluate the different com- ponents of the adapter structure, e.g., normalization layer (Norm), layer-wise and channel-wise learnable scaling on the VTAB val sets. The difference to Adapterbase (first row) is shown in ∆base. Bias Norm Scaling Initialization Accuracy (%) ∆base ✓ Houlsby 76.0 ± 0.2 0.0 Houlsby 75.6 ± 0.4 −0.4 ✓ LoRA 75.5 ± 0.3 −0.5 ✓ BERT 75.8 ± 0.3 −0.2 ✓ ✓ Houlsby 75.9 ± 0.3 −0.1 ✓ ✓ layer Houlsby 75.9 ± 0.3 −0.1 ✓ layer Houlsby 76.2 ± 0.3 +0.2 ✓ ✓ channel Houlsby 75.8 ± 0.3 −0.2 ✓ channel Houlsby 76.5 ± 0.2 +0.5 learn a residual modification to the FFN instead of a subse- quent one. Pre-Adapter and Intermediate-Adapter are subpar compared to the other positions. They either do not have access to the feature transformation happening afterwards in the FFN or to the features of the skip connection containing the output of the attention. Inner structure. Next, we investigate the impact of the in- ner structure of adapters including their initialization. Tab. 2 shows our findings with average accuracies calculated over the three VTAB subgroups. Removing the biases from the linear layers leads to a decrease in accuracy of 0.4 percent- age points (pp). We find that the Houlsby initialization of the adapter parameters is best while BERT and LoRA initializa- tions reduce the accuracy by 0.2 pp and 0.5 pp. Adding layer normalization (LN) to the adapter is slightly detrimental for all settings, both with scaling and without, while addition- ally adding 2d parameters per layer. We find that a learned scaling is in general beneficial for image-classification tasks. Adding layer-wise scaling leads to a gain of 0.2 pp. The inclusion of a learned, channel-wise scaling, as proposed here, gives the strongest improvement of 0.5 pp, reaching an accuracy of 76.5% on the VTAB validation set while only adding half of the parameters compared to LN. What makes a great adapter? From our systematic explo- ration of possible adapter configurations, we conclude that adapter modules in the Post-Adapter position with a learn- able, channel-wise scaling and Houlsby initialization work best for computer vision tasks. We call our proposed adapter configuration Adapter+. The addition of layer normaliza- tion, as suggested by Pfeiffer et al. [49], is not necessary and even leads to detrimental effects in our setting. Configurations from previous work. Different configu- rations of adapters have been established in previous work. We compare their configurations to our systematic approach with rank r=8 on the VTAB validation sets. Using our own implementations already leads to better results than reported in literature but enables us to compare on equal footing. Houlsby et al. [28] use an Intermediate-Adapter with their Table 3. Comparison of Adapter+ with adapter configurations from previous work. We report the average accuracy in % (± std. dev.) of each subgroup and across all groups on the VTAB val sets. Configuration # Param (M) Natural Specialized Structured Average Houlsby [28], r=8 0.39 82.9 ± 0.2 85.5 ± 0.3 58.9 ± 0.8 75.8 ± 0.3 Houlsby [28], r=4 0.24 82.9 ± 0.4 84.9 ± 0.3 58.3 ± 0.6 75.4 ± 0.3 Pfeiffer [49] 0.21 82.9 ± 0.3 86.1 ± 0.9 58.4 ± 0.7 75.8 ± 0.4 AdaptFormer [6] 0.19 83.0 ± 0.4 85.0 ± 0.2 57.4 ± 0.5 75.2 ± 0.2 Adapter+ 0.20 83.0 ± 0.2 86.8 ± 0.6 59.7 ± 0.4 76.5 ± 0.2 proposed initialization both at the FFN section as well at the attention part of the transformer layer. Additionally, they adapt the LN parameters of the backbone. We, therefore, compare their setting additionally with r = 4to compare on roughly the same parameter budget. Pfeiffer et al. [49] suggest a Post-Adapter like us but with a BERT initialization and they employ a layer normalization inside the adapter. AdaptFormer [6] has the same configuration as a scaled parallel adapter (Scaled PA) [22], which was proposed for NLP tasks, the only difference being the layer-wise scalings. Scaled PA uses a fixed scaling of s = 4 for the adapters whereas AdaptFormer suggests to use s = 0.1 for vision tasks. Optimizing s for VTAB may lead to better results. Our results are presented in Tab. 3. We see a clear advantage of our Adapter+ configuration, gaining at least 0.7 pp over all previous adapter realizations considered despite having the second lowest number of trainable parameters. 4.4. Main results VTAB. We evaluate Adapter+ on the VTAB test sets and compare to other methods in Tab. 4. We provide results for full fine-tuning and tuning only the linear classifier while freezing the rest of the backbone [12] as a baseline of classi- cal fine-tuning methods. As competing parameter-efficient tuning methods, we include LoRA [29], VPT [31], NOAH [67], SSF [39], FacT [32], Consolidator [20], and SPT [21]. Wherever possible, we re-evaluate the other methods with a suitable data normalization for the pre-trained backbone and after the full training schedule to enable a fair compar- ison. For LoRA, we use our own implementation because the original work does not cover VTAB. For VPT, we adopt the number of tokens per task from their hyperparameter optimization but find that we do not need to tune learning rate and weight decay per task. Additionally, deviating from the original implementation, we optimize with AdamW [40] instead of SGD [54] and change to an appropriate data nor- malization. We present the original results from [ 31] on VTAB together with our re-evaluation. Our improved imple- mentation of VPT increases the average accuracy by 4.4 pp from 72.0% to 76.4%. SSF, FacT, and SPT released code to evaluate on VTAB. For FacT and SPT, we change the data normalization to match the backbone; SSF already uses the correct one. We re-run the provided code and present the 6Table 4. Detailed results on the VTAB test sets. We report original results and re-evaluations (⟳) in % after a complete training schedule with suitable data normalization. Grayed out numbers are not included in the ranking for best and second best results. †: Early-stopping based on the test set, •: unsuitable data normalization, E: per-task hyperparameter optimization. 1Average across the average accuracies of the VTAB groups, following previous work. 2No complete code release for Consolidator, hence training and evaluation details are unknown. Natural Specialized Structured # Param (M) Cifar100 [35] Caltech101 [15] DTD [8] Flower102 [45] Pets [47] SVHN [44] Sun397 [62] Average Camelyon [60] EuroSAT [25] Resisc45 [7] Retinopathy [14] Average Clevr-Count [33] Clevr-Dist. [33] DMLab [2] KITTI-Dist. [18] dSpr-Loc. [42] dSpr-Ori [42] sNORB-Azi. [36] sNORB-Ele. [36] Average Global Average1 Full 85.8 73.2 92.6 70.4 97.9 86.2 90.6 39.6 78.6 87.1 96.6 87.5 74.0 86.3 66.6 61.0 49.8 79.7 82.6 51.9 33.5 37.0 57.8 74.2 Linear 0.04 78.1 88.1 69.0 99.1 90.0 36.0 56.9 73.9 79.8 90.7 73.7 73.7 79.5 32.4 30.5 35.9 61.9 11.2 26.2 14.3 24.5 29.6 61.0 LoRA [29] 0.29 83.0 91.7 71.6 99.2 90.9 83.8 56.7 82.4 86.2 95.7 83.5 71.9 84.3 77.7 62.3 49.0 80.2 82.2 51.7 31.0 47.0 60.1 75.6 VPT-Deep E• [31] 0.60 78.8 90.8 65.8 98.0 88.3 78.1 49.6 78.5 81.8 96.1 83.4 68.4 82.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 55.0 72.0 VPT-Deep E⟳ 0.60 83.0 93.0 71.2 99.0 91.3 84.1 56.0 82.5 84.9 96.6 82.5 74.5 84.6 77.5 58.7 49.7 79.6 86.2 56.1 37.9 50.7 62.1 76.4 NOAH E†•◦ [67] 0.43 69.6 92.7 70.2 99.1 90.4 86.1 53.7 80.2 84.4 95.4 83.9 75.8 84.9 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 61.3 75.5 SSF E† [39] 0.24 69.0 92.6 75.1 99.4 91.8 90.2 52.9 81.6 87.4 95.9 87.4 75.5 86.6 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 59.0 75.7 SSF E⟳ 0.24 61.9 92.3 73.4 99.4 92.0 90.8 52.0 80.3 86.5 95.8 87.5 72.8 85.7 77.4 57.6 53.4 77.0 78.2 54.3 30.3 36.1 58.0 74.6 FacT-TK8 E†• [32] 0.05 70.3 88.7 69.8 99.0 90.4 84.2 53.5 79.4 82.8 95.6 82.8 75.7 84.2 81.1 68.0 48.0 80.5 74.6 44.0 29.2 41.1 58.3 74.0 FacT-TK8 E⟳ 0.05 74.9 92.7 73.7 99.1 91.3 85.5 57.7 82.1 86.8 94.9 84.1 70.9 84.2 81.9 64.1 49.2 77.2 83.8 53.1 28.2 44.7 60.3 75.5 FacT-TK≤32 E†• [32] 0.10 70.6 90.6 70.8 99.1 90.7 88.6 54.1 80.6 84.8 96.2 84.5 75.7 85.3 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 60.7 75.6 FacT-TK≤32 E⟳ 0.10 74.6 93.7 73.6 99.3 90.6 88.7 57.5 82.6 87.6 95.4 85.5 70.4 84.7 84.3 62.6 51.9 79.2 85.5 52.0 36.4 46.6 62.3 76.5 Consolidator 2 [20] 0.30 74.2 90.9 73.9 99.4 91.6 91.5 55.5 82.4 86.9 95.7 86.6 75.9 86.3 81.2 68.2 51.6 83.5 79.8 52.3 31.9 38.5 60.9 76.5 SPT-Adapter †• [21] 0.23 72.9 93.2 72.5 99.3 91.4 84.6 55.2 81.3 85.3 96.0 84.3 75.5 85.3 82.2 68.0 49.3 80.0 82.4 51.9 31.7 41.2 60.8 75.8 SPT-Adapter ⟳ 0.22 74.7 94.1 73.0 99.1 91.2 84.5 57.5 82.0 85.7 94.9 85.7 70.2 84.1 81.3 63.2 49.1 80.7 83.5 52.0 26.4 41.5 59.7 75.3 SPT-Adapter †• [21] 0.43 72.9 93.2 72.5 99.3 91.4 88.8 55.8 82.0 86.2 96.1 85.5 75.5 85.8 83.0 68.0 51.9 81.2 82.4 51.9 31.7 41.2 61.4 76.4 SPT-Adapter ⟳ 0.43 74.9 93.2 71.6 99.2 91.1 87.9 57.2 82.2 87.0 95.4 86.5 72.4 85.3 81.1 63.2 50.3 80.2 84.4 51.4 31.5 42.2 60.5 76.0 Adapter+, r=1 0.07 85.4 92.4 73.1 99.1 91.3 83.1 58.1 83.2 87.2 96.6 85.3 72.6 85.5 80.7 60.6 50.9 79.9 83.3 55.6 27.1 43.0 60.1 76.3 Adapter+, r=2 0.09 85.4 93.0 72.7 99.2 90.6 85.3 58.0 83.5 87.9 96.8 85.5 71.4 85.4 83.2 61.0 51.6 80.1 86.1 56.3 30.7 46.5 61.9 76.9 Adapter+, r=4 0.13 84.8 93.8 72.7 99.2 90.6 86.5 57.4 83.6 87.5 96.9 85.9 71.5 85.4 83.4 61.6 53.6 81.4 87.3 55.3 34.4 48.1 63.1 77.4 Adapter+, r=8 0.20 84.6 94.2 72.3 99.3 90.7 87.6 56.7 83.6 87.7 97.0 86.7 72.3 85.9 83.2 60.9 53.8 80.3 88.1 55.6 35.7 47.7 63.1 77.6 Adapter+, r=16 0.35 83.7 94.2 71.5 99.3 90.6 88.2 55.8 83.3 87.5 97.0 87.4 72.9 86.2 82.9 60.9 53.7 80.8 88.4 55.2 37.3 46.9 63.3 77.6 Adapter+, r∈[1..4] E 0.11 85.4 93.8 72.7 99.1 90.6 86.5 58.1 83.7 87.5 96.8 85.9 71.4 85.4 83.4 61.0 53.6 81.4 87.3 55.3 34.4 48.1 63.1 77.4 Adapter+, r∈[1..8] E 0.16 85.4 93.8 72.7 99.1 90.7 87.6 58.1 83.9 87.7 96.8 86.7 72.3 85.9 83.4 60.9 53.8 80.3 88.1 55.3 35.7 47.7 63.1 77.7 Adapter+, r∈[1..32] E 0.27 85.4 93.8 72.7 99.1 90.7 88.2 58.1 84.0 87.5 96.8 87.8 73.9 86.5 83.4 60.9 53.8 80.3 87.2 55.3 37.9 47.7 63.3 77.9 results after a full training schedule. For completeness, we also report the results from the original publications. How- ever, we found that the code releases of [21, 32, 39] use early stopping based on the best result on the test set. We argue that tuning hyperparameters such as the number of training epochs on the test set goes against established practices in machine learning; rather the validation set should be used for early stopping. Yet, due to the limited size of the training and validation sets in VTAB, it is not feasible to report test results without also training on the validation data. Hence, we chose to complete a full training schedule of 100 epochs instead of using early stopping. Training SSF for the full schedule leads to a decrease in average accuracy of 1.1 pp over the original publication and re-evaluating SPT leads to a decrease of up to 0.5 pp, even with a corrected data normalization. FacT on the other hand benefits from our re-revaluation, since the accuracy decrease from training a complete schedule is offset by improvements from applying the appropriate data normalization. There was no complete code release with configurations to train Consolidator on VTAB at the time of writing, hence we report results as-is. Adapter+ shows the best parameter-accuracy trade-off among all methods evaluated. This can also be clearly seen in Fig. 1. Additionally, Adapter+ sets a new state of the art with an average accuracy of up to 77.6% over all VTAB subgroups even without any per-task hyperparameter optimization. If we determine the optimal rankr per task on the validation set, we can further improve the accuracy to 77.9%. Optimizing the rank leads to a better parameter-accuracy trade-off than using a fixed rank across all tasks. In Fig. 2, we compare the average accuracy on the sub- groups of VTAB. Wherever possible, we present the results of re-evaluating methods after the last training epoch and matching the data normalization to the backbone. The aver- age accuracies of Adapter+ with r ∈ [1..32] are consistently higher than those of the competing methods. Note that the accuracies of other methods except SPT differ drastically across the different VTAB subgroups. Adapter+, on the other hand, shows a high degree of robustness to the domain shifts between groups. FGVC. Next, we present our results on the FGVC bench- mark in Tab. 5. From the contenders, only SSF [ 39] has released code and hyperparameter configurations for train- ing on FGVC at the time of writing. As we know from the code releases for VTAB, the reported numbers show the accuracy for early stopping based on the test set. There- fore, we expect a similar evaluation for FGVC. While we do not endorse early stopping based on the test set, we ad- 7Table 5. Detailed results on the FGVC test sets. We report original results and re-evaluations (⟳) in % after a complete training schedule with suitable data normalization. Grayed out numbers are not included in the ranking for best and second best results. # Param (M) CUB200 [61] NABirds [27] Oxford Flowers [45] Stanford Dogs [34] Stanford Cars [17] Average Full 86.0 88.0 81.5 99.2 85.6 90.6 89.0 Linear 0.18 88.9 81.8 99.5 92.6 52.8 83.1 VPT-Deep [31] 0.85 88.5 84.2 99.0 90.2 83.6 89.1 VPT-Deep ⟳ 0.85 90.1 83.3 99.6 90.3 85.0 89.7 SSF [39] 0.39 89.5 85.7 99.6 89.6 89.2 90.7 SSF ⟳ 0.39 88.9 85.0 99.6 88.9 88.9 90.3 SPT-Adapter [21] 0.40 89.1 83.3 99.2 91.1 86.2 89.8 SPT-LoRA [21] 0.52 88.6 83.4 99.5 91.4 87.3 90.1 Adapter+, r∈[1..32] 0.34 90.0 83.2 99.6 91.6 89.1 90.7 Adapter+ (best epoch) 0.34 90.4 85.0 99.7 92.6 89.1 91.4 ditionally provide numbers for that setting in Tab. 5 for the sake of comparability. Even when training for a complete schedule, Adapter+ shows the best average accuracy with 90.7% over all five datasets in FGVC,0.4 pp over the second best method under similar evaluation. When early stopping with the test set, Adapter+ reaches 91.4% average accuracy, 0.7 pp over the second best method and 2.4 pp better than full fine-tuning. This demonstrates that Adapter+ also yields state-of-the-art results for task adaptation when training data is abundant while having the best parameter efficiency. 4.5. Ablations Data normalization. We showcase the effect of using an unsuitable data normalization for the chosen ViT in Tab. 6. The gap between ImageNet and Inception normalization (see Sec. 3.5) is largest for VPT [31], with a 3.4 pp difference in average accuracy, which explains around two-thirds of the gain for our re-evaluation as shown in Fig. 1. We suspect that VPT has less of an ability to scale and shift the data because the learnable tokens only act on the attention mechanism. LoRA [29], FacT [32], and adapters all employ linear layers that can directly scale and shift the features of the frozen backbone and thus compensate better for improper data nor- malization. It is worth mentioning that our Adapter+ is the most robust to improper normalization out of the methods evaluated, with a gap of only 2.6 pp average accuracy. Training regularization. We investigate the importance of training regularization methods like stochastic depth [30] and dropout [16] for training adapters on a frozen ViT backbone and evaluate on the VTAB validation sets. We use linearly increasing drop rates as a function of network depth from 0 to 0.1 for the frozen layers of the ViT model, and a drop rate Table 6. Effects of ImageNet vs. Inception data normalization. All methods are evaluated on the VTABval sets. In column ∆Average we report the increase in accuracy in pp across all VTAB subgroups. ImageNet norm Inception normNatural Specialized Structured Average Natural Specialized Structured Average ∆Average VPT 79.2 83.0 53.8 72.0 82.2 86.2 57.9 75.4 3.4 LoRA 78.4 84.1 53.2 71.9 82.0 85.8 56.4 74.7 2.8 FacT-TK 78.0 83.3 56.1 72.4 81.6 85.6 58.1 75.1 2.7 Adapter+ 80.5 85.0 56.0 73.9 83.0 86.8 59.7 76.5 2.6 Table 7. Influence of training regularization. We evaluate accu- racy in % with Adapterbase with rank r=8 on the VTAB val sets. Adapter Stochastic Depth Dropout None ViT Stochastic Depth 76.0 75.4 75.3 None 74.5 74.3 73.7 of 0.1 when using dropout or stochastic depth for the adapter modules. The results in Tab. 7 show a clear benefit for using stochastic regularization for the frozen layers as well as the adapters during training. Using dropout in the adapters is only slightly better than no regularization for adapters, with a gain of only 0.1 pp. With an increase in accuracy of 0.7 pp, stochastic depth is the preferred regularization method for adapters. However, our results show that the more important part is the stochastic depth regularization for the frozen modules of the ViT backbone. Disabling it in training leads to a loss of 1.5 pp accuracy compared to a training where stochastic depth is used throughout the model. 5. Conclusion Applied at the right position and with an optimal inner struc- ture, the simple concept of adapters produces state-of-the-art results for task adaptation. To understand how adapters can “strike back”, we conducted the first systematic and in-depth study on how to best construct adapters and integrate them with vision transformers. This allowed us to determine the optimal connection point for the adapter in the transformer layer. Further, we proposed to use a learnable, channel-wise scaling and showed its benefit for computer vision tasks. Our insights led us to the creation of Adapter+ that yields the highest accuracy and the best parameter-accuracy trade-off on VTAB (77.6%, 0.2M) without any per-task hyperparame- ter optimization and on FGVC (90.7%, 0.34M), showing its superiority over more complicated methods. Acknowledgements. This work has been funded by the LOEWE initiative (Hesse, Germany) within the emergenCITY center. 8References [1] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv:1607.06450 [stat.ML], 2016. [2] Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Si- mon Green, Víctor Valdés, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg, and Stig Petersen. DeepMind Lab. arXiv:1612.03801 [cs.AI], 2016. [3] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. TinyTL: Reduce memory, not parameters for efficient on-device learn- ing. In NeurIPS*2020. [4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End- to-end object detection with transformers. In ECCV, pages 213–229, 2020. [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg- ing properties in self-supervised vision transformers. InICCV, pages 9630–9640, 2021. [6] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yib- ing Song, Jue Wang, and Ping Luo. AdaptFormer: Adapt- ing vision transformers for scalable visual recognition. In NeurIPS*2022. [7] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proc. IEEE, 105(10):1865–1883, 2017. [8] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, pages 3606–3613, 2014. [9] Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Le. RandAugment: Practical automated data augmentation with a reduced search space. In NeurIPS*2020. [10] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul- mohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin Fathy Elsayed, Aravindh Mahen- dran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bast- ings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birod- kar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby. Scaling vision transformers to 22 billion parameters. In ICML, pages 7480–7512, 2023. [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional trans- formers for language understanding. In NAACL-HLT, pages 4171–4186, 2019. [12] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In ICML, pages 647–655, 2014. [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [14] Emma Dugas, Jorge Jared, and Will Cukierski. Diabetic retinopathy detection. Kaggle, 2015. [15] Li Fei-Fei, Robert Fergus, and Pietro Perona. One-shot learn- ing of object categories. IEEE T. Pattern Anal. Mach. Intell., 28(4):594–611, 2006. [16] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learn- ing. In ICML, pages 1050–1059, 2016. [17] Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, and Li Fei-Fei. Fine-grained car detection for visual census estimation. In AAAI, pages 4502–4508, 2017. [18] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The KITTI dataset. Int. J. Robotics Res., 32(11):1231–1237, 2013. [19] Yunhui Guo, Yandong Li, Liqiang Wang, and Tajana Rosing. Depthwise convolution is all you need for learning multiple visual domains. In AAAI, pages 8368–8375, 2019. [20] Tianxiang Hao, Hui Chen, Yuchen Guo, and Guiguang Ding. Consolidator: Mergable adapter with group connections for visual adaptation. In ICLR, 2023. [21] Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, and Bohan Zhuang. Sensitivity-aware visual parameter-efficient tuning. In ICCV, 2023. [22] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg- Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In ICLR, 2022. [23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level perfor- mance on imagenet classification. In ICCV, pages 1026–1034, 2015. [24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. InCVPR, pages 770–778, 2016. [25] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. EuroSAT: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens., 12(7):2217–2226, 2019. [26] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv:1606.08415 [cs.LG], 2023. [27] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge J. Be- longie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection. In CVPR, pages 595–604, 2015. [28] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In ICML, pages 2790–2799, 2019. [29] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 9LoRA: Low-rank adaptation of large language models. In ICLR, 2022. [30] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic depth. In ECCV, pages 646–661, 2016. [31] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge J. Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV, pages 709–727, 2022. [32] Shibo Jie and Zhi-Hong Deng. FacT: Factor-tuning for lightweight adaptation on vision transformer. In AAAI, pages 1060–1068, 2023. [33] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, pages 1988–1997, 2017. [34] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for fine-grained image catego- rization. In CVPR Workshop on Fine-grained Visual Classifi- cation, 2011. [35] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Canadian Institute for Ad- vanced Research, 2009. [36] Yann LeCun, Fu Jie Huang, and Léon Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In CVPR, pages 97–104, 2004. [37] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In EMNLP, pages 3045–3059, 2021. [38] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In ACL/IJCNLP, pages 4582–4597, 2021. [39] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features: A new baseline for efficient model tuning. In NeurIPS*2022. [40] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. [41] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggy- back: Adapting a single network to multiple tasks by learning to mask weights. In ECCV, pages 72–88, 2018. [42] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dSprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset, 2017. [43] Pedro Morgado and Nuno Vasconcelos. NetTailor: Tuning the architecture, not just the weights. In CVPR, pages 3044–3054, 2019. [44] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y . Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. [45] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In ICVGIP, pages 722–729, 2008. [46] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po- Yao Huang, Shang-Wen Li, Ishan Misra, Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jégou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. arXiv:2304.07193 [cs.CV], 2023. [47] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V . Jawahar. Cats and dogs. InCVPR, pages 3498–3505, 2012. [48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In NeurIPS*2019, pages 8024–8035. [49] Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. AdapterFusion: Non- destructive task composition for transfer learning. In EACL, pages 487–503, 2021. [50] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by gener- ative pre-training. Technical report, OpenAI, 2018. [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pages 8748–8763, ICML. [52] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi- sion transformers for dense prediction. InICCV, pages 12159– 12168, 2021. [53] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. In NIPS*2017, pages 506–516. [54] Frank Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychol. Rev., 65(6):386–408, 1958. [55] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale visual recognition challenge. Int. J. Comput. Vision, 115(13):211–252, 2015. [56] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Lud- wig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION- 5B: An open large-scale dataset for training next generation image-text models. In NeurIPS*2022. [57] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your ViT? Data, augmentation, and regularization in vision transformers. Trans. Mach. Learn. Res., 2022. [58] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the In- 10ception architecture for computer vision. In CVPR, pages 2818–2826, 2016. [59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor- eit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS*2017, pages 5998–6008. [60] Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant CNNs for digital pathology. In MICCAI, pages 210–218, 2018. [61] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The Caltech-UCSD Birds-200-2011 dataset. Technical report, California Institute of Technology, 2011. [62] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In CVPR, pages 3485–3492, 2010. [63] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, José M. Álvarez, and Ping Luo. SegFormer: Simple and efficient design for semantic segmentation with transformers. In NeurIPS*2021, pages 12077–12090. [64] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In ACL, 2022. [65] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv:1910.04867 [cs.CV], 2020. [66] Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. In ICLR, 2018. [67] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv:2206.04673 [cs.CV], 2022. 11Adapters Strike Back Supplementary Material In this appendix, we provide further details and results, which could not be included in the main paper due to space limitations. A. Why did adapters underperform for ViTs? First, we want to shed more light on why adapters do not rank well in the literature for parameter-efficient transfer learning for vision tasks. By comparison of numbers reported for adapters on VTAB in the publications referenced in Tab. 4 of the main paper, we found that they essentially stem from only two sources. The first source is VPT [31], where results for an adapter with a reduction factor of 256, amongst other configurations, are reported. For a ViT-B/16 with a hidden dimension of d=768, this is equal to an adapter with rank r=3. Despite citing Pfeiffer et al. [49], who suggest a Post-Adapter po- sition, the actual implementation in the code base * equals an Intermediate-Adapter that performs worse on VTAB (see Sec. 3.3 of the main paper). The initialization used for the adapter parameters most resembles a LoRA initialization but sets the adapter parameters to zero everywhere. Therefore, there is no randomization in the initialization of the adapter parameters, and different seeds only affect the initialization of the classifier. Additionally, the intermediate features in the adapter bottlenecks then become all zero, leading to iden- tical gradients in the up-projections at the start of training, which hinders optimization. As a result, the adapter baseline used by VPT only reaches 60.0% average accuracy on the VTAB test sets. This is a gap of 17.6 percentage points (pp) compared to our Adapter+ with rank r=8 (77.6% average accuracy). Even when considering the loss of around 2 pp to 3 pp caused by an unsuitable data normalization in the VPT implementation, this is still a very significant gap. The numbers for an adapter with rank r =3 from VPT are also reported in [39] as a baseline. The second source for adapter baseline results is the NOAH pre-print [67]. There, an adapter with rank r = 8 is used. Its implementation† performs the following feature transformation: x 7→ Adapter \u0000 FFN(x) \u0001 + x. (12) This is closest to the Intermediate-Adapter (cf . Eq. (10) of the main paper) but misses the skip connection bypassing the adapter and containing FFN(x). Thus, the adapter does not learn a residual function to an identity mapping but instead must learn a more complex mapping to transform its input. *https://github.com/KMnP/vpt †https://github.com/ZhangYuanhan-AI/NOAH Therefore, the adapter becomes harder to train [24], leading to an average accuracy of 73.9% on the VTAB test sets or 3.7 pp behind our Adapter+. For the NOAH adapter results, we see a proliferation to the publications of FacT [32] and SPT [21]. The adapter implementation from NOAH is also used in the code released for Consolidator ‡ [20] but their results are produced with rank r=16, giving a slightly better average accuracy of 74.3%, or 3.3 pp less than Adapter+. In summary, the examined baseline implementations dif- fer from the configurations proposed by Houlsby et al. [28] and Pfeiffer et al. [49] and introduce issues that lead to their underperformance. In our paper, we show that adapters are capable of reaching 77.6% average accuracy for rank r=8 and 77.9% for our optimized version of Adapter+, uplifting adapters from an easy-to-beat baseline to a state-of-the-art transfer method. B. Dataset properties In Tabs. 8 and 9, we show the statistics of each task in VTAB [65] and FGVC [31] with regard to the number of classes and the number of images in the train, validation, and test splits. The tables are largely “borrowed” from [31]. Table 8. Dataset details for VTAB. Group Task # Classes Splits Train Val Test Natural CIFAR-100 [35] 100 800 200 10 000 Caltech-101 [15] 102 6 084 DTD [8] 47 1 880 Oxford Flowers [45] 102 6 149 Pets [47] 37 3 669 SVHN [44] 10 26 032 Sun397 [62] 397 21 750 Specialized Patch Camelyon [60] 2 800 200 32 768 EuroSAT [25] 10 5 400 RESISC45 [7] 45 6 300 Diabetic Retinopathy [14] 5 42 670 Structured CLEVR-Count [33] 8 800 200 15 000 CLEVR-Distance [33] 6 15 000 DMLab [2] 6 22 735 KITTI-Distance [18] 4 711 dSprites-Location [42] 16 73 728 dSprites-Orientation [42] 16 73 728 smallNORB-Azimuth [36] 18 12 150 smallNORB-Elevation [36] 9 12 150 ‡https://github.com/THU-MIG/Consolidator iTable 9. Dataset details for FGVC.For datasets markedwith *, we follow [31] to randomly sample train and validation splits because validation sets are not available from the original datasets. Dataset # Classes Splits Train Val Test CUB-200-2011* [61] 200 5 394 600 5 794 NABirds* [27] 555 21 536 2 393 6 084 Oxford Flowers [45] 102 1 020 1 020 6 149 Stanford Dogs* [34] 120 10 800 1 200 8 580 Stanford Cars* [17] 196 7 329 815 8 041 C. More experimental settings For all experiments conducted with our implementation, we average the results over three seeds. This includes the (re-)evaluations of LoRA and VPT. We built our implemen- tation on PyTorch [48], PyTorch Lightning,§ and timm.¶ We run experiments with bfloat16 mixed precision on a NVIDIA RTX A6000 GPU. For our experiments in the main paper, we report results for a fixed adapter rank r as well as ranks optimized per task. For the per-task optimization of Adapter+, we use a hyper- parameter sweep over the set of ranks r∈{1, 2, 4, 8, 16, 32}. We evaluate on the validation sets of VTAB and FGVC and choose the per-task ranks from the specified range(s) to steer the number of average parameters. The ranks we used to produce the results on the VTAB and FGVC test sets (see Tabs. 4 and 5 in the main paper) are shown in detail in Tab. 10 and Tab. 11, respectively. D. Calculation of no. of trainable parameters Suppose we have a ViT with a hidden dimension d, N trans- former layers, and adapters with rank r. The total num- ber of learnable parameters for Adapter base modules ( cf . Eq. (4) of the main paper) attached to the FFN of every transformer layer then amounts to N(2dr + r + d). Includ- ing layer normalization in the adapter modules amounts to N2d additional parameters. The addition of learned, layer- wise scaling amounts to N extra parameters and choosing learned, channel-wise scaling instead adds Nd extra parame- ters. Adapter+ (see Sec. 4.3 of the main paper) thus amounts to N(2dr + 2d + r) total parameters. Additionally, for a task with c classes, we add a classifier with dc + c learnable parameters. E. Vision transformer pre-training As we add only very few parameters to an otherwise frozen backbone, the generalization capability of the feature repre- sentations produced by the backbone is important. For ViTs, there are a number of off-the-shelf models available with §https://lightning.ai/pytorch-lightning ¶https://github.com/huggingface/pytorch-image-models Table 10. Adapter rank r for each VTAB task for optimized versions of Adapter+ with different ranges of permitted ranks. Natural Specialized Structured # Param (M) CIFAR-100 [35] Caltech-101 [15] DTD [8] Flowers [45] Pets [47] SVHN [44] Sun397 [62] Camelyon [60] EuroSAT [25] RESISC45 [7] Retinopathy [14] CLEVR-Count [33] CLEVR-Dist. [33] DMLab [2] KITTI-Dist. [18] dSpr-Loc. [42] dSpr-Ori. [42] sNORB-Azi. [36] sNORB-Ele. [36] r∈[1..4] 0.11 1 4 2 1 4 4 1 4 2 4 2 4 2 4 4 4 4 4 4 r∈[1..8] 0.16 1 4 2 1 8 8 1 8 2 8 8 4 8 8 8 8 4 8 8 r∈[1..32] 0.27 1 4 2 1 8 16 1 16 2 32 32 4 8 8 8 32 4 32 8 Table 11. Adapter rank r for each FGVC dataset for optimized versions of Adapter+ with different ranges of permitted ranks. # Param (M) CUB-200 [61] NABirds [27] Oxford Flowers [45] Stanford Dogs [34] Stanford Cars [17] r∈[1..32] 0.34 2 2 1 1 32 differences in their training procedures. Here, we examine three different pre-trainings as examples: (1) Original: The ViT-B/16 weights used in the main paper, pre-trained with su- pervision on ImageNet-21k [55] following the training proce- dure of the original ViT publication [13],|| (2) ImageNet-1k: the same ViT weights further fine-tuned on ImageNet-1k [55],** and (3) AugReg: weights from a pre-training with stronger data augmentation in the form of Mixup [66] and RandAugment [9] following [57].†† In Tab. 12, we summarize our results for Adapter+ with rank r=8 evaluated on the VTAB validation sets. We notice that additional fine-tuning on ImageNet-1k gives a slight edge (83.4% average accuracy over 83.0% for second best) in adaption for tasks that contain natural images. However, the fine-tuning is detrimental for the Specialized and Structured group. Not fine-tuning on ImageNet-1k is beneficial for the Structured group with a large increase of 3.7 pp. The Aug- Reg training setting improves the transfer to the Specialized group but is worse than the other settings for natural images. Overall, the original supervised training on ImageNet-21k generalizes best across all tasks in VTAB with an average accuracy of 76.5%, 0.3 pp better than AugReg training and 1.2 pp better than ImageNet-1k fine-tuning. ||https://storage.googleapis.com/vit_models/imagenet21k/ ViT-B_16.npz **https://storage.googleapis.com/vit_models/imagenet21k+ imagenet2012/ViT-B_16-224.npz ††https://storage.googleapis.com/vit_models/augreg/B_16- i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0.npz iiTable 12. Influence of ViT pre-training. We use Adapter+ with rank r=8 for the evaluation and report the average accuracy in % for each subgroup and across all groups on the VTAB val sets. Pre-training Natural Specialized Structured Average ImageNet-1k 83.4 86.5 56.0 75.3 AugReg 81.6 87.2 59.7 76.2 Original 83.0 86.8 59.7 76.5 Table 13. Adapter position with DINO backbone. We report average accuracy in % ( ± std. dev.) on the VTAB val sets for different adapter positions. Adapterbase with Houlsby initialization and rank r=8 is used in all experiments. Position Natural Specialized Structured Average Pre 76.8 ± 0.4 86.2 ± 0.6 53.6 ± 0.7 72.2 ± 0.3 Intermediate 76.8 ± 0.4 85.8 ± 0.8 52.6 ± 0.9 71.8 ± 0.4 Parallel 76.7 ± 0.3 86.8 ± 0.4 54.1 ± 0.7 72.5 ± 0.3 Post 76.9 ± 0.2 86.3 ± 0.5 55.3 ± 0.7 72.8 ± 0.3 Table 14. Comparison of Adapter+ with adapter configurations from previous work with DINO backbone.We report the average accuracy in % (± std. dev.) of each subgroup and across all groups on the VTAB val sets. Configuration # Param Natural Specialized Structured Average Houlsby [28], r=8 0.39 77.4 ± 0.4 86.5 ± 0.7 52.9 ± 0.8 72.3 ± 0.4 Houlsby [28], r=4 0.24 77.2 ± 0.5 86.2 ± 0.5 53.2 ± 0.8 72.2 ± 0.3 Pfeiffer [49] 0.21 76.8 ± 0.4 86.2 ± 0.3 54.4 ± 1.0 72.5 ± 0.4 AdaptFormer [6] 0.19 76.5 ± 0.4 85.8 ± 0.4 53.0 ± 0.5 71.8 ± 0.3 Adapter+ 0.20 76.7 ± 0.3 86.4 ± 0.5 55.4 ± 0.8 72.8 ± 0.3 F. Generality of the conclusions Using DINO [5] as an example of a ViT trained with self- supervision, we show in Tab. 13 that the orders of best-to- worst adapter position is consistent with that of a supervised backbone in terms of average accuracy, albeit with a higher standard deviation. The ranking also stays the same for the comparison of Adapter+ with adapter configurations from previous work as presented in Tab. 14. This shows that our conclusions generalize beyond backbones with super- vised pre-training to backbones based on self-supervised pre-training. iii",
      "meta_data": {
        "arxiv_id": "2406.06820v1",
        "authors": [
          "Jan-Martin O. Steitz",
          "Stefan Roth"
        ],
        "published_date": "2024-06-10T22:07:57Z",
        "pdf_url": "https://arxiv.org/pdf/2406.06820v1.pdf"
      }
    },
    {
      "title": "Distance-Based Regularisation of Deep Networks for Fine-Tuning",
      "abstract": "We investigate approaches to regularisation during fine-tuning of deep neural\nnetworks. First we provide a neural network generalisation bound based on\nRademacher complexity that uses the distance the weights have moved from their\ninitial values. This bound has no direct dependence on the number of weights\nand compares favourably to other bounds when applied to convolutional networks.\nOur bound is highly relevant for fine-tuning, because providing a network with\na good initialisation based on transfer learning means that learning can modify\nthe weights less, and hence achieve tighter generalisation. Inspired by this,\nwe develop a simple yet effective fine-tuning algorithm that constrains the\nhypothesis class to a small sphere centred on the initial pre-trained weights,\nthus obtaining provably better generalisation performance than conventional\ntransfer learning. Empirical evaluation shows that our algorithm works well,\ncorroborating our theoretical results. It outperforms both state of the art\nfine-tuning competitors, and penalty-based alternatives that we show do not\ndirectly constrain the radius of the search space.",
      "full_text": "arXiv:2002.08253v3  [stat.ML]  15 Jan 2021 Published as a conference paper at ICLR 2021 DI S TA N C E -B A S E D RE G U L A R IS AT IO N O F DE E P NE T WO R K S F O R FI N E -T U N I N G Henry Gouk & Timothy M. Hospedales School of Informatics University of Edinburgh {henry.gouk,t.hospedales}@.ed.ac.uk Massimiliano Pontil Department of Computer Science University College London m.pontil@ucl.ac.uk ABSTRACT W e investigate approaches to regularisation during ﬁne-tu ning of deep neural networks. First we provide a neural network generalisation bound based on Rademacher complexity that uses the distance the weights ha ve moved from their initial values. This bound has no direct dependence on the nu mber of weights and compares favourably to other bounds when applied to convolu tional networks. Our bound is highly relevant for ﬁne-tuning, because providing a network with a good initialisation based on transfer learning means that learn ing can modify the weights less, and hence achieve tighter generalisation. Inspired b y this, we develop a simple yet effective ﬁne-tuning algorithm that constrains the hyp othesis class to a small sphere centred on the initial pre-trained weights, thus obt aining provably better generalisation performance than conventional transfer le arning. Empirical evalu- ation shows that our algorithm works well, corroborating ou r theoretical results. It outperforms both state of the art ﬁne-tuning competitors , and penalty-based alternatives that we show do not directly constrain the radi us of the search space. 1 I NTRODUC TI ON The ImageNet Large Scale V isual Recognition Challenges hav e resulted in a number of neural network architectures that obtain high accuracy when train ed on large datasets of labelled exam- ples (He et al., 2016; T an and Le, 2019; Russakovsky et al., 20 15). Although these models have been shown to achieve excellent performance in these benchmarks , in many real-world scenarios such volumes of data are not available and one must resort to ﬁne-t uning an existing model: taking the weights from a model trained on a large dataset, to initialis e the weights for a model that will be trained on a small dataset. The assumption being that the wei ghts from the pre-trained model provide a better initialisation than randomly generated weights. A pproaches for ﬁne-tuning are typically ad hoc, requiring one to experiment with many problem-depen dent tricks, and often a process that will work for one problem will not work for another. Transfor ming ﬁne-tuning from an art into a well principled procedure is therefore an attractive prosp ect. This paper investigates, from both a theoretical and empirical point of view , the impact of diff erent regularisation strategies when ﬁne-tuning a pre-trained network for a new task. Existing ﬁne-tuning regularisers focus on augmenting the cross entropy loss with terms that indirectly or directly penalise the distance the ﬁne-tuned weights mov e from the pre-trained values. The intuition behind this seems sensible—the closer the ﬁne-tuned weight s are to the pre-trained weights, the less in- formation is forgotten about the source dataset—but it is no t obvious how this idea should be translated into an effective algorithm. One should expect that the choi ce of distance metric is quite important, but existing methods exclusively make use of Euclidean distanc e (Li et al., 2019; 2018) without any theo- retical or empirical justiﬁcation regarding why that metri c was chosen. These methods achieve only a small improvement in performance over standard ﬁne-tuning , and it is reasonable to expect that using a metric more suited to the weight space of neural networks wo uld lead to greater performance. More- over, while the use of penalty terms to regularise neural net works is well established, the impact of using penalties vs constraints as regularisers has not been well studied in the context of deep learning. In order to study the generalisation error of ﬁne-tuned mode ls, we derive new bounds on the empirical Rademacher complexity of neural networks based on the dista nce the trained weights move from 1Published as a conference paper at ICLR 2021 their initial values. In contrast to existing theory (e.g., Neyshabur et al. (2018); Bartlett et al. (2017); Long and Sedghi (2019)), we do not resort to covering numbers or make use of distributions over models to make these arguments. By deriving two bounds utili sing different distance metrics, but proved with the same techniques, we are able to conduct a cont rolled theoretical comparison of which metric one should use as the basis for a ﬁne-tuning regu larisation scheme. Our ﬁndings show that a metric based on the maximum absolute row sum (MARS) mat rix norm is a more suitable measure of distance in the parameter space of convolutional neural networks than Euclidean distance. Additionally, we challenge the notion that using a penalty t erm to encourage the ﬁne-tuned weights to lie near the pre-trained values is the best way to restrict the effective hypothesis class. W e demonstrate that the equivalence of penalty methods and con straint methods in the case of linear models (Oneto et al., 2016) does not translate to the context of deep learning. As a result, using projected stochastic subgradient methods to constrain the distance the weights in each layer can move from the initial settings can lead to improved performance. Several regularisation methods are proposed, with the aim of both corroborating the theoretical analysis with empirical evidence, and improving the perfor mance of ﬁne-tuned networks. One of these approaches is a penalty-based method that regularise s the distance from initialisation according to the MARS-based distance metric. The other two techniques make use of efﬁcient projection functions to enforce constraints on the Euclidean and MARS d istance between the pre-trained and ﬁne-tuned weights throughout training. The experimental r esults demonstrate that projected subgra- dient methods improve performance over using penalty terms , and that the widely used Euclidean metric is typically not the best choice of metric to measure d istances in network parameter space. 2 R ELATED WORK The idea of developing an algorithm to restrict the distance of weights from some unbiased set of reference weights has been explored in various forms to impr ove the performance of ﬁne-tuned networks. Li et al. (2018) presented the ℓ2-SP regulariser, which consists of adding a term to the objective function that penalises the squared ℓ2 distance of the trained weights from the initial weights. This is based on an idea initially made use of when performing domain adaptation, where it was applied to linear support vector machines (Y ang et al., 2007 ). The subsequent work of Li et al. (2019) follows the intuition that the features produced by the ﬁne-tuned network should not differ too much from the pre-trained features. They also use Euclidean dist ance, but to measure distance between feature vectors rather than weights. The idea is extended to incorporate an attention mechanism that weights the importance of each channel. The method is implem ented by adding a penalty term to the standard objective function. In contrast to these appro aches, we solve a constrained optimisation problem rather than adding a penalty, and we demonstrate tha t the MARS norm is more effective than the Euclidean norm when measuring distance in weight space. Many recent meta-learning algorithms also make use of idea that keeping ﬁne-tuned weights close to their initial values is desirable. However, these a pproaches typically focus on developing methods for learning the initial weights, rather than worki ng with pre-speciﬁed initial weights. The model-agnostic meta-learning approach (Finn et al., 2017) does this by simulating few-shot learning tasks during the meta-learning phase in order to ﬁnd a good se t of initial weights for a neural network. Once the learned algorithm is deployed, it adapts to new few- shot learning tasks by ﬁne-tuning the initial weights for a small number of iterations. Denevi et a l. (2018) proposes a modiﬁed penalty term for ridge regression where, instead of penalising the dista nce of the parameters from zero, they are regularised towards a bias vector. This bias vector is learn ed during the course of solving least squares problems on a collection of related tasks. Denevi et al. (201 9) extend this approach to a fully online setting and a more general family of linear models. Previous work investigating the generalisation performance of neural network based on the distance the weights have travelled from their initial values has don e so with the aim of explaining why existing methods for training models work well. Bartlett et al. (2017) present a bound derived via covering numbers that shows the generalisation perform ance of fully connected networks is controlled by the distance the trained weights are from the i nitial weights. Their bound makes use of a metric that scales with the number of units in the netw ork, which means if the result is extended to a class of simple convolutional networks then th e generalisation performance will scale with the resolution of the feature maps. A similar boun d can also be proved through the use 2Published as a conference paper at ICLR 2021 of P AC-Bayesian analysis (Neyshabur et al., 2018). One can m ake use of different metrics and techniques for applying covering numbers to bounding gener alisation that do not have the same implicit dependence on the number of units, but they instead depend directly on the number of weights in the network (Long and Sedghi, 2019). Neyshabur et al. (2019) investigate the performance of two layer neural networks with ReLU activation functions , demonstrating that as the size of the hidden layer increases, the Frobenius distance (i.e., the F robenius norm of the difference between initial and trained weight matrices) shrinks. Inspired by t his observation, they show that one can construct a bound on the Rademacher complexity of this class by using Euclidean distance between the initial weights and trained weights of each individual u nit. Although they bound the Rademacher complexity directly, they still incur an explicit dependen ce on the size of the hidden layer, and their analysis is restricted to fully connected networks with onl y a single hidden layer. In contrast to these previous studies, our focus is on design ing an algorithm that will improve the performance of ﬁne-tuned networks, rather than explain ing the performance of the standard ﬁne-tuning methods that are already widespread. Therefore , we do not aim to infer what properties of existing methods enable networks to generalise well—we i nstead derive such properties and then develop algorithms that enforce them. Moreover, we put a par ticular emphasis on choosing a metric that is suitable for contemporary convolutional networks, and thus will not scale with the size of the feature maps, while also being easy to implement efﬁciently . 3 D ISTANCE -B AS ED GENERALISATI ON BOUNDS Throughout this section we will analyse a loss class consist ing of feed-forward neural networks, f(⃗ x) = (φL ◦... ◦φ1)(⃗ x), where each φj (⃗ x) = ϕ(Wj ⃗ x) is a layer with a 1-Lipschitz activation function, ϕ. Both the norm of the weight matrix of each layer and the dista nce of the ﬁne-tuned weights from the pre-trained weights are bounded from above . Formally, we deﬁne F∗ = {(⃗ x, y) ↦→l(y, f (⃗ x)) :∥Wj ∥∗ ≤B∗ j , ∥W 0 j ∥∗ ≤B∗ j , ∥Wj −W 0 j ∥∗ ≤D∗ j }, where j goes from 1 to L, l is a ρ-Lipschitz loss function with a range of [0, 1], and W 0 j is the pre-trained weight matrix for layer j. Each of the pre-trained weight matrices can be either rando m variables drawn from a distribution with support such that t he constraints are always fulﬁlled, or they can be ﬁxed (i.e., drawn from a Dirac delta distribution). Th e only other requirement is that they are independent of the training data used for ﬁne-tuning. Fr om an empirical perspective it is most useful to consider them non-random quantities. W e have used ∗as a placeholder for the norm used to measure the magnitude of weight matrices and the distance from the pre-trained weights. The primary focus is on using the MARS norm, ∥W ∥∞ = max j ∑ i=1 |Wj,i|, to prove a bound on the empirical Rademacher complexity (Bar tlett and Mendelson, 2002) of F∞. Our main theoretical result, presented below , is obtained b y modifying the “peeling”-style arguments typically used to directly prove bounds on the Rademacher co mplexity of neural network hypothesis classes (for examples, see Neyshabur et al. (2015); Golowic h et al. (2018)). Our modiﬁcation of the argument allows us to rephrase the resulting theorem in term s of the distance the parameters can move from their initialisation during training. Theorem 1.F or all δ ∈(0, 1), the expected loss of all models in F∞ is, with probability 1 −δ, bounded by E(⃗ x,y)[l(f(⃗ x), y)] ≤ 1 m m∑ i=1 l(f(⃗ xi), yi)+ 4 √ log(2d)cρC∞ ∑ L j=1 D∞ j B∞ j ∏ L j=1 2B∞ j √m +3 √ log(2/δ) 2m , where m is the number of training examples, c is the number of classes, ⃗ xi ∈Rd, and ∥⃗ xi∥∞ ≤C∞. Crucially, when l is chosen carefully (e.g., the ramp loss), the expectation i n Theorem 1 is an upper bound for the expected classiﬁcation error rate. The proof f or Theorem 1 can be found in the supplemental material. The two main terms in this theorem ar e (i) the product of bounds on the layer norms; and (ii) the summation, which is a bound on the di stance the ﬁne-tuned weights can be from the pre-trained weights. The ﬁrst of these is primari ly dependent on the weights obtained via pre-training, whereas (ii) can be controlled during the ﬁne-tuning process. Moreover, one would expect that if better initial weights are selected via pre-t raining, then the distance the ﬁnal weights will 3Published as a conference paper at ICLR 2021 be from the initial values will be smaller, thus leading to be tter generalisation. This is the motivation behind the regularisers we develop in Section 4. An analogous bound is also derived for the Frobenius norm (i.e., FF ) to facilitate a theoretical comparison between the proposed regularisation method and ℓ2-SP , an existing approach that relies on penalising Euclidean distance between the pre-trained a nd ﬁne-tuned parameters. In order to avoid a direct dependence on the number of parameters (as accompli shed in Theorem 1) it is necessary to restrict FF to use only ReLU activation functions. Theorem 2. F or all δ ∈(0, 1), the expected loss of all models in FF is, with probability 1 −δ, bounded by E (⃗ x,y) [l(f(⃗ x), y)] ≤ 1 m m∑ i=1 l(f(⃗ xi), yi)+ 2 √ 2cρC2 ∑ L j=1 DF j 2BF j ∏ j i=1 √ ni L∏ j=1 2BF j √nj √m +3 √ log(2/δ) 2m , where m is the number of training examples, c is the number of classes, ∥⃗ xi∥2 ≤C2, ϕ(·) =ReLU(·), and nj is the number of columns in Wj . W e can make several observations by comparing our two bounds with each other and previously published bounds: (i) in contrast to Theorem 1, Theorem 2 inc urs a signiﬁcant dependence on the resolution of the intermediate feature maps due to the √nj factors; (ii) each term in the summations in our bounds can be at most one, whereas the corresponding term s in Bartlett et al. (2017) can be more than one due to the use of multiple types of norms; (iii) our bo und from Theorem 1 does not have a direct dependence on the number of weights, in contrast to th e bound provided by Long and Sedghi (2019). More detailed comparisons can be found in the supple mental material. 4 F INE -T UN IN G WITH DISTANCE REGULARISATI ON The analysis presented in Section 3 suggests that the weight s in ﬁne-tuned models should be close to the pre-trained weights in order to achieve good generali sation performance. More speciﬁcally, the learning process should search only within a set of weigh ts within a predeﬁned distance from the pre-trained weights. W e discuss two strategies for accompl ishing this: using projected subgradient methods to enforce a hard constraint, and augmenting the sta ndard cross entropy objective with a term that penalises the distance between the pre-trained an d ﬁne-tuned weights. The method based on projection functions is attractive because it guarantees t hat the constraints will be fulﬁlled even if one uses a heuristic optimisation method to train the network pa rameters. However, the penalty-based ap- proaches are more common in the literature, and convenient f rom an implementation point of view due to the ubiquity of automatic differentiation. Nevertheles s, in contrast to the projection-based meth- ods, the techniques that use penalties have weaker assuranc es on whether a constraint is actually being enforced. W e discuss the drawbacks of this non-equivalence further in the supplemental material. 4.1 O P T IM IS IN G W IT H PRO J E CT IO N S One way to enforce constraints on the weights of neural netwo rks during training is to use a variant of the projected stochastic subgradient method. This is simil ar to typical stochastic subgradient methods used when training neural networks, but has the additional s tep of applying a projection operation after each weight update to ensure that the new weights lie in side the set of feasible parameters that satisfy the constraints. In the case of classic subgradient descent, in order to guarantee convergence towards a stationary point the projection function must per form a Euclidean projection, π(ˆW ) = arg min W 1 2 ∥W −ˆW ∥2 2 s.t. g(W ) ≤0, (1) where ˆW are the newly updated parameters that may violate the constr aint, W are the projected parameters, and g(·) is a convex function specifying the constraint. Although th e Euclidean projection is required for the classic projected subgradient method, o ther optimisation algorithms may require the projection to be performed with respect to a different me tric. Looking at different optimisers as instantiations of mirror descent with different Bregman di vergences is one way to determine the type of projection that should be performed. Unfortunately some of the most common optimisers used in deep learning, such as Adam (Kingma and Ba, 2015), are not g uaranteed to converge even when 4Published as a conference paper at ICLR 2021 there are no constraints on the parameters being optimised ( Reddi et al., 2018). This makes extending it to perform constrained optimisation a purely heuristic e ndeavour, further compounded by the fact that it is also not clear which metric the projection should b e performed with respect to. As such, the projections used in our approaches are performed with respe ct to the norm that is most convenient from an efﬁciency point of view . Rather than attempting to constrain the distance between the all pre-trained and ﬁne-tuned weights in the network using a single projection, constraints are ap plied on a layer-wise basis. This makes optimisation more manageable and also allows practitioner s to favour ﬁne-tuning certain parts of the network—e.g., if one is ﬁne-tuning a network pre-trained on photos to perform a task on paintings where the same underlying classes are present, one might wis h to allocate more ﬁne-tuning capacity to earlier layers in the network. The types of constraints th at we wish to enforce take the form ∥Wj −W (0) j ∥∗ ≤γj , where γj is a hyperparameter that corresponds to the maximum allowab le distance between the pre-trained weights and the ﬁne-tuned weights for layer j. With a minor rearrangement, this yields a constraint speciﬁcation in the form required for Equation 1 , g∗ j (W (0) j , Wj , γj ) =∥Wj −W (0) j ∥∗ −γj , where we have made the dependence on the pre-trained weights and the hyperparameter explicit. The resulting optimisation problem is min W1:L m∑ i=1 l(yi, (φL ◦... ◦φ1)(⃗ xi)) ∥Wj −W (0) j ∥∗ ≤γj ∀j ∈{1 ... L}. (2) The remainder of this section presents derivations for the p rojection functions, πF and π∞, cor- responding to this constraint speciﬁcation when it is insta ntiated with the Frobenius norm and the MARS norm, respectively. W e provide pseudocode in the suppl ementary material that illustrates how these projections are integrated to the neural network ﬁ ne-tuning procedure when using a variant of the stochastic subgradient method. W e refer to the Froben ius norm instantiation as ℓ2-PGM and the MARS norm version as MARS-PGM, where the PGM indicates th e use of projection gradient methods. 4.1.1 C O N S T RA IN IN G FRO BE N IU S DIS TA N CE When using the Frobenius distance, Equation 1 can be rewritt en as πF (W (0), ˆW , γ) = arg min W 1 2∥W −ˆW ∥2 F s.t. ∥W −W (0)∥F −γ ≤0. T o simplify the problem, we can instead work on a translated v ersion of the same parameter space where W (0) is the origin. Setting ˆT = ˆW −W (0) and T = W −W (0), the problem becomes π2( ˆT , γ) = arg min T 1 2∥T −ˆT ∥2 F s.t.∥T ∥F −γ ≤0, which is the Euclidean projection onto the ℓ2 ball with radius γ, and has the known closed form solution π2( ˆT , γ) = 1 max ( 1, ∥ ˆT ∥F γ ) ˆT . Expanding the deﬁnition of ˆT and translating back into the correct parameter space yield s the Frobenius distance projection function, πF (W (0), ˆW , γ) =W (0) + 1 max ( 1, ∥ ˆW −W (0)∥F γ ) (ˆW −W (0)). (3) 5Published as a conference paper at ICLR 2021 4.1.2 C O N S T RA IN IN G MARS D IS TA N CE The constraint on the MARS distance can be equivalently expr essed as a collection of constraints on the ℓ1 distance of each row in the weight matrix from the correspond ing row in the pre-trained weight matrix. That is, ∥W −W (0)∥∞ ≤γ ⇐⇒ ∥⃗ wi −⃗ w(0) i ∥1 ≤γ ∀i, where ⃗ wi is the ith row of W . One can then make use of the same translation trick used to de rive the Frobenius distance projection function to change the ℓ1 distance constraints to ℓ1 norm constraints, π1(ˆ⃗ti, γ) = arg min ⃗ti 1 2∥⃗ti −ˆ⃗ti∥2 2 s.t. ∥⃗ti∥1 −γ ≤0, (4) where ⃗t = ⃗ wi −⃗ w(0) i . The problem in Equation 4 is a Euclidean projection onto the ℓ1 ball with radius γ, for which there is no known closed form solution. There exis t algorithms to ﬁnd the ℓ1 projection in time linearly proportional to the dimensiona lity of the vector (Duchi et al., 2008), but they are not amenable to implementation on graphics process ing units due to the sequential nature of the computations involved. Instead, we apply a projection t hat minimises the ℓ1 distance between the original point and its projection, subject to the projected point lying inside the ℓ1 ball with radius γ, π1(ˆ⃗ti, γ) = 1 max(1, ∥ ˆ⃗ti∥1 γ ) ˆ⃗ti. This projection, while not providing the closest feasible p oint measured in Euclidean distance, still provides a point that satisﬁes the constraints, but is trivi al to implement efﬁciently. Finally, the projection function for the entire weight matrix is given by applying π1 row-wise, and translating back into the correct parameter space, π∞(W (0), ˆW , γ) =    π1(ˆ⃗ w1 −⃗ w(0) 1 , γ) +⃗ w(0) 1 . . . π1( ˆ ⃗ wn −⃗ w(0) n , γ) +⃗ w(0) n   , where ˆW contains n rows. 4.2 P E NA LT Y ME T H O D S One popular approach in the literature to encourage a model t o not move too far from a set of initial weights is to augment the loss function with a penalty term. I n our case, this would involve taking the standard objective for the problem at hand (e.g., cross entr opy or the hinge loss), and adding penalty terms corresponding to each layer, min W1:L m∑ i=1 l(yi, f (⃗ xi)) + L∑ j=1 λj ∥Wj −W (0) j ∥∗, (5) where λj are hyperparameters used to balance the regularisation ter ms with the main loss function. Due to the subdifferentiability of the two norms considered in this paper, instantiations of Equation 5 can be trained via automatic differentiation and a variant o f the stochastic subgradient method. For the Frobenius norm, we actually penalise the squared Frobenius norm, which recovers the ℓ2-SP approach of Li et al. (2018). W e refer to the instantiation that penali ses the MARS distance as MARS-SP . 5 E XPERIME NT S This section provides an empirical investigation into the p redictive performance of the proposed methods relative to existing approaches for regularising ﬁ ne-tuning, and also conducts experiments to demonstrate which properties of the novel algorithms are re sponsible for the change in performance. T wo network architectures are used: ResNet-101 (He et al., 2 016), which is representative of a typical large neural network, and EfﬁcientNetB0 (T an and Le , 2019), a leading architecture intended for use on mobile devices. Both networks are pre-trained on t he 2012 ImageNet Large Scale V isual Recognition Challenge dataset (Russakovsky et al., 2015). The Adam optimiser is used for all experiments (Kingma and Ba, 2015). Information regardi ng the datasets and hyperparameter optimisation procedure can be found in the supplemental mat erial. 6Published as a conference paper at ICLR 2021 T able 1: Results obtained with different regularisation ap proaches when ﬁne-tuning ResNet-101 (top) and EfﬁcientNetB0 (bottom) models pre-trained on the ILSVRC-2012 subset of ImageNet. W e report the mean ±std. dev. of accuracy measured across ﬁve different random s eeds. Regularisation Aircraft Butterﬂy Flowers Pets PubFig DTD C altech A vg. Rank None 51.81 ±0.87 70.02 ±0.16 76.68 ±1.07 84.19 ±0.34 75.36 ±0.67 66.15 ±0.55 75.32 ±0.61 6.71 DEL T A (Li et al., 2019) 60.38 ±1.26 77.91 ±0.24 86.57 ±0.27 88.11 ±0.52 82.23 ±2.48 69.38 ±0.68 78.88 ±0.27 4.14 LS (M ¨ uller et al., 2019) 60.86 ±0.65 76.52 ±0.23 86.49 ±0.38 90.50±0.31 84.38±0.47 68.08 ±0.81 80.04±0.10 3.14 ℓ2-SP (Li et al., 2018) 60.16 ±1.33 76.56 ±0.19 83.11 ±0.27 86.23 ±0.41 83.79 ±3.69 69.87 ±0.24 79.94 ±0.17 4.00 MARS-SP 58.52 ±1.23 69.54 ±0.36 75.90 ±1.04 84.22 ±0.45 79.17 ±0.73 69.53 ±0.93 79.19 ±0.33 5.57 ℓ2-PGM 69.61 ±0.15 77.97 ±0.13 86.91 ±0.66 90.47 ±0.33 84.14 ±0.81 70.97±0.71 78.45±0.13 2.57 MARS-PGM 72.04±0.70 79.50 ±0.36 87.42 ±0.41 89.23±1.36 88.75±0.28 70.23±0.53 79.11 ±0.19 1.86 Regularisation Aircraft Butterﬂy Flowers Pets PubFig DTD C altech A vg. Rank None 54.58 ±0.65 69.43 ±0.47 77.43 ±0.14 84.87 ±0.19 75.51 ±0.83 64.98 ±0.43 81.07 ±0.38 7.00 DEL T A (Li et al., 2019) 70.61 ±0.18 79.61 ±0.21 84.60 ±0.23 89.27 ±0.22 88.62 ±0.54 71.37 ±0.35 81.53 ±0.38 3.57 LS (M ¨ uller et al., 2019) 56.14 ±0.27 71.52 ±0.14 83.42 ±0.70 84.95 ±0.55 77.05 ±0.31 65.53 ±0.26 83.12 ±0.17 5.43 ℓ2-SP (Li et al., 2018) 69.26 ±0.26 78.88 ±0.27 86.61 ±0.48 89.79 ±0.28 84.04 ±0.98 71.12 ±0.52 82.91 ±0.39 3.71 MARS-SP 66.96 ±0.49 72.01 ±0.20 77.79 ±0.48 89.24 ±0.40 85.33 ±0.59 69.41 ±0.49 82.10 ±0.24 5.0 ℓ2-PGM 70.87 ±0.33 81.81 ±0.22 86.95 ±0.17 89.33 ±0.19 88.45 ±0.36 71.63 ±0.59 84.30 ±0.09 2.29 MARS-PGM 75.22±0.34 82.32 ±0.10 90.36 ±0.15 91.38 ±0.24 90.30 ±0.40 73.57 ±0.38 84.84 ±0.17 1.00 5.1 P RE D ICT IV E PE RF O RM A N CE The ﬁrst set of experiments are a performance comparison of t he proposed methods and existing regularisation approaches for ﬁne-tuning. The baselines c onsidered are standard ﬁne-tuning with no specialised regularisation, ℓ2-SP (Li et al., 2018), DEL T A (Li et al., 2019), and label smoot hing (LS) as formalised in M ¨ uller et al. (2019). Once hyperparameter s are obtained, each network architecture and regulariser combination is ﬁne-tuned on both the traini ng and validation folds of each dataset. The ﬁne-tuning process is repeated ﬁve times with different random seeds to measure the robustness of each method to the composition of minibatches and initial isation of the ﬁnal linear layer, which is trained from scratch. T est set accuracy is reported for th e ResNet-101 and EfﬁcientNetB0 architectures in T able 1. Comparing the average ranks of the methods across differentdatasets (Demˇ sar, 2006), the most salient trend is that the projection-based methods exhibit a signiﬁcant increase in accuracy over their penalty counterparts that use the same distance metrics. Th is suggests that, in the case of ﬁne-tuning, using a projection to enforce a constraint on the weights thr oughout the training process is a better regularisation strategy than adding a term to the objective function that penalises the deviations of weights from their pre-trained values. Additionally, look ing further at the relative performance of the two projection-based regularisers, we can see that the M ARS distance variant is more often a better choice than the Frobenius distance. This observatio n further supports the conclusions of the comparison of the bounds in Section 3. 5.2 D IS TA N CE F RO M IN IT IA L IS AT IO N T o further investigate the relationship between the penalt y and projection strategies for regularisation, we analyse the distances between the pre-trained and ﬁne-tu ned weights at a per-layer basis. Figure 1 provides histograms indicating the distributions of per-l ayer MARS distances for the models without any regularisation, regularised with MARS-SP , trained wit h the MARS-PGM, and also DEL T A. All networks were trained on the Pets dataset with the same hy perparameters used by the models examined in Section 5.1. W e observe three trends from these p lots. Firstly, both the penalty and projection methods reduce the MARS distance between the pre -trained and ﬁne-tuned parameters relative to the unregularised model. Secondly, a signiﬁcan t number of the constraints enforced by the projection method are activated—i.e., many of the weight ma trices lie on the boundary of the feasible set. In contrast, the penalty-based regulariser does not en force similarly activated constraints. Finally, the results demonstrate that the DEL T A method of Li et al. (20 19) does not operate by implicitly regularising the same quantity, as its MARS distance histog ram is longer tailed than the others. 7Published as a conference paper at ICLR 2021 0 1 2 3 4 5 0 10 20 30 40 50 MARS Distance Number of Layers (a) None 0 1 2 3 4 5 0 10 20 30 40 50 MARS Distance Number of Layers (b) MARS-PGM 0 1 2 3 4 5 0 10 20 30 40 50 MARS Distance Number of Layers (c) MARS-SP 0 1 2 3 4 5 0 10 20 30 40 MARS Distance Number of Layers (d) DEL T A 0 5 10 15 20 25 30 0 20 40 60 MARS Distance Number of Layers (e) None 0 5 10 15 20 25 30 0 20 40 60 MARS Distance Number of Layers (f) MARS-PGM 0 5 10 15 20 25 30 0 20 40 60 MARS Distance Number of Layers (g) MARS-SP 0 5 10 15 20 25 30 0 20 40 60 MARS Distance Number of Layers (h) DEL T A Figure 1: Histograms of MARS distances between pre-trained and pets dataset ﬁne-tuned weights for the speciﬁed regularisation strategies. MARS-PGM success fully constrains weight distances to be less than γj , indicated by the dashed line. T op: ResNet101, Bottom: Efﬁc ientNetB0. 10−4 10−2 100 102 104 0 50 c Accuracy DEL T A ℓ2-SP ℓ2-PGM MARS-SP MARS-PGM (a) ResNet101 10−4 10−2 100 102 104 0 50 c Accuracy DEL T A ℓ2-SP ℓ2-PGM MARS-SP MARS-PGM (b) EfﬁcientNetB0 Figure 2: Sensitivity of the regularisation methods to the c hoice of hyperparameters. Measurements are taken on the pets dataset, and c is a factor applied to the hyperparameters found during tuni ng. 5.3 C A PACIT Y CO N T RO L T o demonstrate the ability of the distance-based regularis ation methods to control model capacity, we sweep through a range of hyperparameter values and plot the c orresponding predictive performance. Hyperparameters were generated according to λj = cˆλj and γj = cˆγj, where c is varied, and ˆλj , ˆγj are the values found during the hyperparameter optimisatio n process. Plots of c versus the resulting accuracy on the pets dataset are given in Figure 2 for both the ResNet101 and EfﬁcientNetB0 architec- tures. W e can see that the PGM methods behave as the theoretic al analysis predicts: hyperparameter conﬁgurations that lead to very small distances between pre -trained and ﬁne-tuned weights result in underﬁtting, relaxing the hyperparameters too much lead s to overﬁtting, and using the optimised hyperparameters (i.e., when c = 1) achieves the best performance. 8Published as a conference paper at ICLR 2021 2 4 6 8 102 103 104 105 106 γ Measure Frobenius MARS Spectral (a) ℓ2-PGM 5 10 15 20 25 30 102 103 104 105 106 γ Measure (b) MARS-PGM 0 2 4 6 8 10 12 14 103 104 105 106 Epoch Measure (c) Unregularised 0 2 4 6 8 10 12 14 0.97 0.98 0.98 0.99 0.99 Epoch Accuracy Train T est (d) Unregularised Accuracy Figure 3: An empirical comparison of the tightness of our bou nds on the Rademacher complexity, and that of Long and Sedghi (2019). The (a) and (b) plots demonstr ate the empirical value of the bounds as a function of the regularisation parameters: the vertica l axes correspond to the model complexities for each of the three measures considered, and the horizonta l axes represents the regularisation strength for ℓ2-PGM (a) and MARS-PGM (b). The plot in (c) shows how the model c omplexity measures change throughout the training of an unregularise d model, and (d) shows the training and test set performance throughout training for the same unreg ularised model. 5.4 E M P IRICA L CO M PA RIS O N O F BO U N D S W e perform an empirical comparison of our two bounds, along w ith a bound based on the spectral norm (Long and Sedghi, 2019), to demonstrate the relative ti ghtness. This is done by training neural networks on the MNIST dataset (LeCun et al., 1998). W e use the architecture of Scherer et al. (2010), which consists of a single convolutional layer with 9 ×9 ﬁlters and 112 channels, followed by 5 ×5 max pooling, and ﬁnally a linear classiﬁer layer. The networ k is trained for 15 epochs using the Adam optimiser (Kingma and Ba, 2015), as training any further doe s not result in any performance increase. Following similar work that has performed empirical evalua tion of neural network bounds, we evaluate the bounds by computing the relevant norms and dist ances of the trained network weights. These quantities are then used in place of the upper bounds (i .e., B∗ i and D∗ i ) used in the deﬁnition of the hypothesis classes considered by Theorems 1 and 2. In k eeping with previous work that has performed this type of empirical comparison, we measure the distance from random initialisation, rather than ﬁne-tuning from a pre-trained network (Bartlet t et al., 2017; Neyshabur et al., 2019). Figure 3 shows how these three quantities vary for different choices of γ (the same hyperparameter is used for both layers) when using MARS-PGM and ℓ2-PGM, and also how they differ through the process of training and unregularised model. W e observe sev eral trends: (i) like previous work in this area, all of the empirically evaluated Rademacher complexi ty bounds are still too loose to be useful for model selection and providing performance guarantees o n the expected test set performance; (ii) the bounds based on the MARS norm are consistently tight er (by orders of magnitude) than the bounds based on the Frobenius and spectral norms; (iii) t he empirical measurements of model complexity plateau even though the hyperparameters govern ing the worst-case capacity continue to increase—we suspect this is caused by implicit regularis ation from early stopping. This implicit regularisation from early stopping also provides an explan ation for why we observe only a small degradation in performance in Figure 2 when the hyperparame ters are set to very large values. 6 C ONCLUSIO N This paper investigates different regularisation methods for ﬁne-tuning deep learning networks. T o facilitate this, we provide two new bounds on the generalisa tion performance of neural networks based on the distance of the ﬁnal weights from their initial v alues. The discussion comparing these bounds suggests that the MARS distance is a more appropriate metric in the parameter space of convolutional networks than Frobenius distance. Addition ally, several new algorithms are presented that enable an experimental comparison between different r egularisation strategies. The empirical results corroborate our theoretical investigation, demon strating that constraining MARS distance is more effective than constraining Euclidean distance. Cruc ially, we also show that, in line with our theoretical results, enforcing a hard constraint througho ut the entire training process on the distances 9Published as a conference paper at ICLR 2021 the parameters can move is far more effective than the widely used strategy of adding a penalty term to the objective function. Implementations of the methods u sed in this paper are available online. 1 ACKNOWLE DG M EN TS This work was supported by the Engineering and Physical Scie nces Research Council (EPSRC) Grant number EP/S000631/1; and the MOD University Defence R esearch Collaboration (UDRC) in Signal Processing. REFERENC ES Peter L Bartlett and Shahar Mendelson. Rademacher and gauss ian complexities: Risk bounds and structural results. Journal of Machine Learning Research , 3:463–482, 2002. Peter L Bartlett, Dylan J Foster, and Matus J T elgarsky. Spec trally-normalized margin bounds for neu- ral networks. In Advances in Neural Information Processing Systems 30 , pages 6240–6249, 2017. James Bergstra, Brent Komer, Chris Eliasmith, Dan Y amins, a nd David D. Cox. Hyperopt: A Python library for model selection and hyperparameter opti mization. Computational Science & Discovery, 8(1):014008, July 2015. Tianshui Chen, W enxi Wu, Y uefang Gao, Le Dong, Xiaonan Luo, a nd Liang Lin. Fine-grained representation learning and recognition by exploiting hie rarchical semantic embedding. In Proceedings of the 26th ACM International Conference on Mul timedia, pages 2023–2031, 2018. M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. V edaldi. Describing textures in the wild. In Proceedings of the IEEE Conf. on Computer V ision and P attern Recognition (CVPR) , 2014. Janez Demˇ sar. Statistical comparisons of classiﬁers over multiple data sets. Journal of Machine learning research, 7(Jan):1–30, 2006. Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massi miliano Pontil. Learning to learn around a common mean. In Advances in Neural Information Processing Systems 31 , 2018. Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massi miliano Pontil. Learning-to-learn stochastic gradient descent with biased regularization. I n Proceedings of the 36th International Conference on Machine Learning , 2019. John Duchi, Shai Shalev-Shwartz, Y oram Singer, and Tushar C handra. Efﬁcient projections onto the l 1-ball for learning in high dimensions. In Proceedings of the 25th International Conference on Machine Learning , 2008. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agno stic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machin e Learning , 2017. Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-In dependent Sample Complexity of Neural Networks. In Conference On Learning Theory , pages 297–299, July 2018. Gregory Grifﬁn, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep r esidual learning for image recog- nition. In Proceedings of the IEEE Conference on Computer V ision and P a ttern Recognition , 2016. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic o ptimization. In International Conference on Learning Representations , 2015. Y ann LeCun, L ´ eon Bottou, Y oshua Bengio, and Patrick Haffne r. Gradient-based learning applied to document recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998. Michel Ledoux and Michel T alagrand. Probability in Banach Spaces Isoperimetry and Processes . Springer, 1991. 1 https://github.com/henrygouk/mars-finetuning 10Published as a conference paper at ICLR 2021 Xingjian Li, Haoyi Xiong, Hanchao W ang, Y uxuan Rao, Liping L iu, and Jun Huan. DEL T A: DEep Learning Transfer using Feature Map with Attention for Conv olutional Networks. In International Conference on Learning Representations , 2019. Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit in ductive bias for transfer learning with convolutional networks. In Proceedings of the 35th International Conference on Machin e Learning, 2018. Philip M Long and Hanie Sedghi. Size-free generalization bo unds for convolutional neural networks. arXiv preprint arXiv:1905.12600 , 2019. S. Maji, J. Kannala, E. Rahtu, M. Blaschko, A. V edaldi, Subhr ansu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea V edaldi. Fine-grained visual classiﬁcation of aircraft. T echnical report, 2013. Andreas Maurer. A vector-contraction inequality for radem acher complexities. In International Conference on Algorithmic Learning Theory , pages 3–17. Springer, 2016. Rafael M ¨ uller, Simon Kornblith, and Geoffrey E Hinton. Whe n does label smoothing help? In Advances in Neural Information Processing Systems 32 , 2019. Behnam Neyshabur, Ryota T omioka, and Nathan Srebro. Norm-b ased capacity control in neural networks. In Conference on Learning Theory , 2015. Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro . A P AC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks. In International Conference on Learning Representations , February 2018. Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Y ann L eCun, and Nathan Srebro. T o- wards understanding the role of over-parametrization in ge neralization of neural networks. In International Conference on Learning Representations , 2019. Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂowe r classiﬁcation over a large number of classes. In Proceedings of the 6th Indian Conference on Computer V ision , Graphics & Image Processing, pages 722–729. IEEE, 2008. Luca Oneto, Sandro Ridella, and Davide Anguita. Tikhonov, i vanov and morozov regularization for support vector machine learning. Machine Learning , 103(1):103–136, 2016. Omkar M Parkhi, Andrea V edaldi, Andrew Zisserman, and CV Jaw ahar. Cats and dogs. In Proceedings of the IEEE Conference on Computer V ision and P a ttern Recognition , 2012. Nicolas Pinto, Zak Stone, T odd Zickler, and David Cox. Scali ng up biologically-inspired computer vision: A case study in unconstrained face recognition on fa cebook. In IEEE CVPR W orkshops , pages 35–42. IEEE, 2011. Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the conver gence of Adam and beyond. In International Conference on Learning Representations , 2018. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjee v Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexan der C. Berg, and Li Fei-Fei. ImageNet Large Scale V isual Recognition Challenge. International Journal of Computer V ision , 115(3): 211–252, December 2015. Dominik Scherer, Andreas M ¨ uller, and Sven Behnke. Evaluat ion of pooling operations in convo- lutional architectures for object recognition. In International Conference on Artiﬁcial Neural Networks, pages 92–101. Springer, 2010. Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014. Mingxing T an and Quoc Le. Efﬁcientnet: Rethinking model sca ling for convolutional neural networks. In Proceedings of the 36th International Conference on Machin e Learning , 2019. Jun Y ang, Rong Y an, and Alexander G Hauptmann. Adapting svm c lassiﬁers to data with shifted distributions. In Proceedings of the 7th IEEE International Conference on Dat a Mining W orkshops, pages 69–76. IEEE, 2007. 11Published as a conference paper at ICLR 2021 A P ROOF OF THEOREM 1 W e begin with the deﬁnition of empirical Rademacher complex ity. Deﬁnition 1. The empirical Rademacher complexity of a class, C, is deﬁned as ˆR(C) = 1 mE⃗ σ [ sup h∈C m∑ i=1 ⃗ σih(⃗ xi) ] , where ⃗ σis a vector of independent Rademacher distributed random va riables and m is the number of training examples. Using a standard result from statistical learning theory, one can make use of the empirical Rademacher complexity of F∞ to bound the generalisation error of models contained withi n the class. Theorem 3 ((Shalev-Shwartz and Ben-David, 2014, p. 378)) . F or a class, C, containing functions mapping from some set to the interval [a, b], for all δ ∈(0, 1) the following holds with probability at least 1 −δ: E ⃗ x,y [ h(y, ⃗ x) ] ≤ 1 m m∑ i=1 h(yi, ⃗ xi) + 2ˆR(C) + 3(b −a) √ log(2/δ) 2m , where (⃗ xi, yi) are independent and identically distributed. W e will make use of the following lemma, which uses the “peeli ng” method of Neyshabur et al. (2015) to bound the Rademacher complexity of a network based on the p roduct of weight matrix norms. Lemma 1. F orF∞ as deﬁned in Section 3, the following inequality holds E σ [ sup W1:k      m∑ i=1 σifk(⃗ xi)      ∞ ] ≤ √ 2mlog(2d)C∞ k∏ j=1 2B∞ j where fk = φk ◦... ◦φ1, ⃗ xi ∈Rd, and ∥⃗ xi∥∞ ≤C∞. Proof. E σ [ sup W1:k      m∑ i=1 σiϕ(Wkfk−1(⃗ xi))      ∞ ] = E σ [ sup W1:k− 1, ⃗ wk ⏐ ⏐ ⏐ ⏐ ⏐ m∑ i=1 σiϕ( ⃗ wT k fk−1(⃗ xi)) ⏐ ⏐ ⏐ ⏐ ⏐ ] ≤2E σ [ sup W1:k− 1, ⃗ wk ⏐ ⏐ ⏐ ⏐ ⏐ m∑ i=1 σi ⃗ wT k fk−1(⃗ xi) ⏐ ⏐ ⏐ ⏐ ⏐ ] ≤2E σ [ sup W1:k− 1, ⃗ wk ∥⃗ wk∥1      m∑ i=1 σifk−1(⃗ xi)      ∞ ] = 2E σ [ sup W1:k ∥Wk∥∞      m∑ i=1 σifk−1(⃗ xi)      ∞ ] = 2B∞ k E σ [ sup W1:k− 1      m∑ i=1 σifk−1(⃗ xi)      ∞ ] , where the ﬁrst inequality is due to the Lipschitz compositio n property of Rademacher complexi- ties (Ledoux and T alagrand, 1991) and the second follows fro m H ¨ older’s inequality. Iterating this process for all layers of the network results in E σ [ sup W1:k      m∑ i=1 σifk(⃗ xi)      ∞ ] ≤ ( k∏ j=1 2B∞ j ) E σ      m∑ i=1 σi⃗ xi      ∞ . The expectation can be bounded via Massart’s inequality (Sh alev-Shwartz and Ben-David, 2014, p. 383) to conclude the proof. The following lemma demonstrating how to “split” the Radema cher complexity bound into terms associated with the ﬁne-tuned and pre-trained weights will also be useful. 12Published as a conference paper at ICLR 2021 Lemma 2. F orF∞ as given in Section 3, the following inequality holds E σ [ sup W1:k ⃗ vT m∑ i=1 σifk(⃗ xi) ] ≤ √ 2mlog(2d)C∞∥⃗ v∥1D∞ k k−1∏ j=1 2B∞ j + ∥⃗ v∥1 max j E σ [ sup W1:k− 1 ⃗ w0T k,j m∑ i=1 σifk−1(⃗ xi) ] , where fk, ⃗ xi, and C∞ are as deﬁned in Lemma 1, and ⃗ vis a ﬁxed vector in Rn. Proof. E σ [ sup W1:k ⃗ vT m∑ i=1 σifk(⃗ xi) ] = E σ [ sup W1:k ⃗ vT m∑ i=1 σiϕ(Wkfk−1(⃗ xi)) ] ≤ n∑ j=1 E σ [ sup W1:k vj m∑ i=1 σiϕ( ⃗ wT k,j fk−1(⃗ xi)) ] ≤ n∑ j=1 |vj |E σ [ sup W1:k m∑ i=1 σi ⃗ wT k,j fk−1(⃗ xi) ] , where the ﬁrst inequality comes from moving the summation ou tside the supremum, and the second from multiplying the hypothesis class by a constant and appl ying the Lipschitz composition property of Rademacher complexities. This can be further bounded fro m above by n∑ j=1 |vj |E σ [ sup W1:k m∑ i=1 σi ⃗ wT k,j fk−1(⃗ xi) ] = n∑ j=1 |vj |E σ [ sup W1:k ⃗ wT k,j m∑ i=1 σifk−1(⃗ xi) ] = n∑ j=1 |vj |E σ [ sup W1:k ( ⃗ wk,j −⃗ w0 k,j + ⃗ w0 k,j )T m∑ i=1 σifk−1(⃗ xi) ] ≤ n∑ j=1 |vj |E σ [ sup W1:k ( ⃗ wk,j −⃗ w0 k,j )T m∑ i=1 σi ⃗fk−1(⃗ xi) ] + n∑ j=1 |vj |E σ [ sup W1:k− 1 ⃗ w0T k,j m∑ i=1 σifk−1(⃗ xi) ] ≤∥⃗ v∥1 max j E σ [ sup W1:k ( ⃗ wk,j −⃗ w0 k,j )T m∑ i=1 σi ⃗fk−1(⃗ xi) ] + ∥⃗ v∥1 max j E σ [ sup W1:k− 1 ⃗ w0T k,j m∑ i=1 σifk−1(⃗ xi) ] where the ﬁrst inequality comes from moving a summation outs ide the supremum, and the second from H ¨ older’s inequality. Because Rademacher complexiti es are always non-negative, we can omit the absolute value in the deﬁnition of the vector ∞-norm that one would expect to be enclosing each of the expectations. The ﬁrst term can be bounded from above by m oving the maxj inside the expectation and then applying H ¨ older’s inequality and the deﬁnition of the MARS norm, ∥⃗ v∥1 max j E σ [ sup W1:k ( ⃗ wk,j −⃗ w0 k,j )T m∑ i=1 σi ⃗fk−1(⃗ xi) ] ≤∥⃗ v∥1D∞ k E σ [ sup W1:k− 1      m∑ i=1 σi ⃗fk−1(⃗ xi)      ∞ ] . Applying Lemma 1 to the resulting expectation completes the proof. W e now prove Theorem 1. Proof.The main result of Maurer (2016) tells us that ˆR(F∞) ≤ √ 2ρ m E σ [ sup W1:L c∑ j=1 m∑ i=1 σi,j (φL ◦... ◦φ1)j (⃗ xi) ] , where (φL ◦... ◦φ1)j (⃗ xi) is the j-th component of the output of (φL ◦... ◦φ1)(⃗ xi). Denoting fk = φk ◦... ◦φ1 and the j-th row of WL as ⃗ wT L,j , the summation over classes can be brought outside 13Published as a conference paper at ICLR 2021 the Rademacher complexity and φL expanded as ˆR(F∞) ≤ c∑ j=1 √ 2ρ m E σ [ sup W1:L m∑ i=1 σiϕj ( ⃗ wT L,j fL−1(⃗ xi)) ] to which the Lipschitz contraction inequality can be applie d to obtain c∑ j=1 √ 2ρ m E σ [ sup W1:L m∑ i=1 σi ⃗ wT L,j fL−1(⃗ xi) ] = c∑ j=1 √ 2ρ m E σ [ sup W1:L ( ⃗ wL,j −⃗ w0 L,j + ⃗ w0 L,j )T m∑ i=1 σifL−1(⃗ xi) ] . H ¨ older’s inequality and the deﬁnition of the MARS norm can b e used to further bound this as c∑ j=1 √ 2ρ m ( E σ [ sup W1:L ∥⃗ wL,j −⃗ w0 L,j ∥1      m∑ i=1 σifL−1(⃗ xi)      ∞ ] + E σ [ sup W1:L− 1 ⃗ w0T L,j m∑ i=1 σifL−1(⃗ xi) ]) ≤ c∑ j=1 √ 2ρ m ( E σ [ sup W1:L D∞ L      m∑ i=1 σifL−1(⃗ xi)      ∞ ] + E σ [ sup W1:L− 1 ⃗ w0T L,j m∑ i=1 σifL−1(⃗ xi) ]) , of which the ﬁrst expectation can be dealt with via Lemma 1, yi elding ˆR(F∞) ≤ 2 √ log(2d)cρC∞ D∞ L 2B∞ L ∏ L i=1 2B∞ i √m + c∑ j=1 √ 2ρ m E σ [ sup W1:L− 1 ⃗ w0T L,j m∑ i=1 σifL−1(⃗ xi) ] . Lemma 2 can be applied to the expectation, yielding E σ [ sup W1:L ⃗ w0T L,j m∑ i=1 σifL−1(⃗ xi) ] ≤ √ 2mlog(2d)C∞∥⃗ w0 L,j ∥1D∞ L−1 L−2∏ i=1 2B∞ i + ∥⃗ w0 L,j ∥1 max k E σ [ sup W1:L− 2 ⃗ w0T L−1,k m∑ i=1 σifL−2(⃗ xi) ] ≤ √ 2mlog(2d)C∞ D∞ L−1 2B∞ L−1 L∏ i=1 2B∞ i + B∞ L max k E σ [ sup W1:L− 2 ⃗ w0T L−1,k m∑ i=1 σifL−2(⃗ xi) ] . Noting that maxk ∥⃗ w0 l,k∥1 ≤B∞ l , Lemma 2 can be recursively applied to the remaining expecta tion until the following bound is obtained ˆR(F∞) ≤ 2 √ log(2d)cρC∞ ∑ L j=1 D∞ j B∞ j ∏ L i=1 2B∞ i √m + c∑ j=1 √ 2ρ ∏ L i=2 B∞ i m max k E σ [ ⃗ w0T 1,k m∑ i=1 σi⃗ xi ] . The remaining expectation evaluates to zero irrespective o f which k is selected by the max, hence nullifying the whole term, and an application of Theorem 3 co mpletes the proof. B P ROOF OF THEOREM 2 The proof of Theorem 2 follows the same structure as the proof for Theorem 1. For conciseness, some details are omitted due to their similarity with the pre vious proof. W e ﬁrst provide a lemma demonstrating the peeling argument for Frobenius norm-bou nded networks. Lemma 3. F orFF as deﬁned in Section 3, the following inequality holds E σ [ sup W1:k      m∑ i=1 σifk(⃗ xi)      2 ] ≤√ mC2 k∏ j=1 2BF j where f = φk ◦... ◦φ1, ϕ is the ReLU activation function, and ∥⃗ xi∥2 ≤C2. Proof. Iteratively applying Lemma 1 from Golowich et al. (2018) res ults in E σ [ sup W1:k      m∑ i=1 σifk(⃗ xi)      2 ] ≤E σ [      m∑ i=1 σi⃗ xi      2 ] k∏ j=1 2BF j , 14Published as a conference paper at ICLR 2021 and the expectation can be further bounded by E σ [      m∑ i=1 σi⃗ xi      2 ] = E σ [    √      m∑ i=1 σi⃗ xi      2 2 ] ≤    √ E σ [      m∑ i=1 σi⃗ xi      2 2 ] =    √      m∑ i=1 ⃗ xi      2 2 = √ mC2. Next, a lemma corresponding to the splitting method is given . Lemma 4. F orFF as given in Section 3, the following inequality holds E σ [ sup W1:k ⃗ vT m∑ i=1 σifk(⃗ xi) ] ≤√mnC2∥⃗ v∥2DF k k−1∏ j=1 2BF j + √n∥⃗ v∥2 max j E σ [ sup W1:k− 1 ⃗ w0T k,j m∑ i=1 σifk−1(⃗ xi) ] , where fk, ⃗ xi, and C2 are as deﬁned in Lemma 3, and ⃗ vis a ﬁxed vector in Rn. Proof. From the proof for Lemma 2, we have E σ [ sup W1:k ⃗ vT m∑ i=1 σifk(⃗ xi) ] ≤ n∑ j=1 |vj |E σ [ sup W1:k ( ⃗ wk,j −⃗ w0 k,j )T m∑ i=1 σi ⃗fk−1(⃗ xi) ] + n∑ j=1 |vj |E σ [ sup W1:k− 1 ⃗ w0T k,j m∑ i=1 σifk−1(⃗ xi) ] , of which the ﬁrst term be further bounded n∑ j=1 |vj |E σ [ sup W1:k ( ⃗ wk,j −⃗ w0 k,j )T m∑ i=1 σi ⃗fk−1(⃗ xi) ] ≤∥⃗ v∥2    √ n∑ j=1 E σ [ sup W1:k ( ⃗ wk,j −⃗ w0 k,j )T m∑ i=1 σi ⃗fk−1(⃗ xi) ] 2 ≤∥⃗ v∥2    √ n∑ j=1 E σ [ sup W1:k DF k      m∑ i=1 σi ⃗fk−1(⃗ xi)      2 ] 2 = √ n∥⃗ v∥2DF k E σ [ sup W1:k      m∑ i=1 σi ⃗fk−1(⃗ xi)      2 ] ≤√ mnC2∥⃗ v∥2DF k k−1∏ j=1 2BF j , 15Published as a conference paper at ICLR 2021 where the ﬁrst two inequalities are due to Cauchy-Schwarz, a nd the third is via Lemma 3. Moving on to the second term, we begin by applying Cauchy-Schwarz, n∑ j=1 |vj |E σ [ sup W1:k− 1 ⃗ w0T k,j m∑ i=1 σifk−1(⃗ xi) ] ≤∥⃗ v∥2    √ n∑ j=1 E σ [ sup W1:k− 1 ⃗ w0T k,j m∑ i=1 σi ⃗fk−1(⃗ xi) ] 2 ≤∥⃗ v∥2    √ n max j E σ [ sup W1:k− 1 ⃗ w0T k,j m∑ i=1 σi ⃗fk−1(⃗ xi) ] 2 = √n∥⃗ v∥2 max j E σ [ sup W1:k− 1 ⃗ w0T k,j m∑ i=1 σi ⃗fk−1(⃗ xi) ] , which concludes the proof. Finally, we prove Theorem 2. Proof.The proof for this theorem follows the same process as the pro of for Theorem 1. The main result of Maurer (2016) is used to obtain ˆR(FF ) ≤ √ 2ρ m E σ [ sup W1:L c∑ j=1 m∑ i=1 σi,j (φL ◦... ◦φ1)j (⃗ xi) ] , which, by bringing the summation over c outside the expectation and using the Lipschitz contractio n inequality, can be bounded from above by c∑ j=1 √ 2ρ m E σ [ sup W1:L ( ⃗ wL,j −⃗ w0 L,j + w0 L,j )T m∑ i=1 σifL−1(⃗ xi) ] . This can be expanded into two expectations, as in the proof fo r Theorem 1, and Lemma 3 applied to the ﬁrst term, yielding ˆR(FF ) ≤ √ 2cρC2 DF L BF L ∏ L i=1 2BF i √m + c∑ j=1 √ 2ρ m E σ [ sup W1:L− 1 ⃗ w0T L,j m∑ i=1 σifL−1,k(⃗ xi) ] . Applying Lemma 4 to the expectation results in E σ [ sup W1:L ⃗ w0T L,j m∑ i=1 σifL−1(⃗ xi) ] ≤√mnLC2∥⃗ w0 L,j ∥DF L−1 L−2∏ i=1 2BF i + √nL∥⃗ w0 L,j ∥2 max k E σ [ sup W1:L− 1 ⃗ w0T L−1,k m∑ i=1 σifL−2(⃗ xi) ] ≤√mC2 DF L−1 2BF L−1 ∏ L−1 i=1 √ni L∏ i=1 2√ niBF i + √nLBF L max k E σ [ sup W1:L− 1 ⃗ w0T L−1,k m∑ i=1 σifL−2(⃗ xi) ] . As in the proof for Theorem 1, we note that maxk ∥wl,k∥2 ≤BF l and recursively apply Lemma 4 until we obtain the following bound ˆR(FF ) ≤ √ 2cpC2 ∑ L j=1 DF j 2BF j ∏ j i=1 √ ni ∏ L i=1 2√ niBF i √m + c∑ j=1 √ 2ρ ∏ L i=2 √ niBF i m max k E σ [ ⃗ w0T 1,k m∑ i=1 σi⃗ xi ] , of which the right term is equal to zero. The result follows fr om Theorem 3. 16Published as a conference paper at ICLR 2021 C C OMPARISO N WITH PREVIOUS BOUNDS This section provides a comparison of our bounds with previo us work that investigates bounding the generalisation gap, 2 G = E ⃗ x,y [ f(y, ⃗ x) ] − 1 m m∑ i=1 f(yi, ⃗ xi). In particular, we consider the main result of Bartlett et al. (2017), G ≤O ( ln(N)√m ρC2 ( L∑ j=1 ( D2,1 j B2 j ) 2/3) 3/2 L∏ j=1 B2 j ) , (6) and using our notation the main result of Long and Sedghi (201 9) is, G ≤O ( √ N m ρC2 L∑ j=1 D2 j L∏ j=1 B2 j ) . (7) These bounds depend on slightly different quantities: we us e N to represent the number of parameters in the network, B2 j and D2 j indicates upper bounds on the spectral norm and the the assoc iated distance metric, and D2,1 j corresponds to an upper bound on distance metric induced by t he matrix (2, 1)-norm, ∥W ∥2,1 = ∑ j √ ∑ i W 2 j,i. In contrast to the aforementioned bounds, which were shown t o be true via covering number argu- ments, the bounds presented in this paper are proved by direc tly bounding the empirical Rademacher complexity. As a consequence of these different proof techn iques, the constants that appear in each bound are quite different. The covering number bound both ha ve explicit dependences on the number of parameters in the network—with the bound of Long and Sedgh i (2019) being much stronger than that of Bartlett et al. (2017). Our bound does not have this de pendence, but it does incur an extra factor of two for every application of the Ledoux-T alagrand contraction inequality used in the peeling argument. This results in an explicit exponential dependen ce on the depth of the network. W e note that recent work by Golowich et al. (2018) shows how the peeli ng method can be adapted to reduce this to a very mild dependence in some situations. In addition to inspecting the constants in each bound, one can consider how the norms compare. The (2, 1)-norm used in Equation 6, the spectral norm bound from Long an d Sedghi (2019), and the Frobenius norm used in Theorem 2 are all sensitive to the reso lution of the intermediate feature maps in convolutional networks, as they do not take into account t he parameter sharing in convolutional layers. In contrast, the MARS norm bound in Theorem 1 is indep endent of the feature map sizes. This is because the structure of W in a convolutional layer is a block matrix, where each block i s, in turn, a doubly block circulant matrix, so each unique weight in the layer appears at most once in each row . The MARS norm only considers the “worst” row in the matrix, so each weight is considered at most once, irrespective of feature map resolution. As such, the M ARS bound are more suited to explaining generalisation in convolutional networks. Furthermore, t he way in which the norms are utilised in the bounds are different. Consider the value of each term in the s ummations of the bounds; for the bound of Bartlett et al. (2017), the value of each term can be greate r than one, because different norms are used in the numerator and denominator. Long and Sedghi (2019 ) do not perform any normalisation, so each term can be greater than one. In contrast, because the numerator and denominator of the terms in the summations of our bound use the same norms, and from the deﬁnition of our hypothesis class, each term is bounded from above by one. D A LGORITHM PSEUDOCO DE Algorithm 1 provides high-level pseudocode demonstrating how our proposed projection functions can be integrated with a generic gradient-based optimiser. 2 T o simplify the comparison, we omit the terms proportional t o √ ln(1/δ) m that appear in all bounds. It should be noted that all bounds hold with probability 1 − δ. 17Published as a conference paper at ICLR 2021 Algorithm 1 A projected stochastic subgradient method for enforcing a d istance constraint on each layer of a ﬁne-tuned neural network, where ft is the objective function evaluated on a batch of data indexed by t, update (·) is a weight update rule such as Adam, and π is a projection function. Inputs: W (0) 1:L, and γi t ←0 while W1:L not converged do t ←t + 1 ˆW (t) 1:L ←update(W (t−1) 1:L , ∇W1:L ft(W (t−1) 1:L , V (t−1))) for j = 1to L do W (t) j ←π(W (0) j , ˆW (t) j , γj ) end for end while E C ONVERGE NC E OF PENALTY METHODS The usual motivation for using a penalty term to enforce a nor m constraint is based on the equivalence of constraints and penalties in the case of linear models wit h convex loss functions (Oneto et al., 2016). This equivalence tells us that for each choice of pena lty hyperparameter, λ, there is a corresponding setting for a constraint hyperparameter, γ, such that the penalty-based approach and the projection-based optimiser will search the same hypoth esis space. Although our case is quite different from the setting of Oneto et al. (2016), since we ma ke use of nonlinear models and apply multiple norm constraints, it is still common to apply penal ties to neural networks when one wishes to encourage some sort of structure in the weights of a model. The technique used to prove the equivalence in the linear case is based on reinterpreting th e penalised loss function as the Lagrangian of the constrained problem. The Karush–Kuhn–Tucker theore m states that stationary points of the Lagrangian are local optimisers of the constrained problem . The problem with applying this in a deep learning context is that convergence is almost always measu red in terms of predictive performance, rather than the training loss. As a consequence, it is rare th at the ﬁne-tuned weights will be a stationary point of the Lagrangian and the constraints implied by the va lues chosen for the penalty hyperparam- eter may not be enforced. In this case, the model resulting fr om the training process is not guaranteed to come from the pre-speciﬁed hypothesis class. In contrast , the projection-based methods are guaranteed to provide a model from the chosen hypothesis cla ss even if a stationary point is not found. W e explore this further by examining the when the training lo ss converges. Figure 4 shows plots of the training loss, as well as the training and testing accuracy, over the course of training ResNet101 and Ef- ﬁcientNetB0 networks on the pets dataset while applying the MARS-SP regulariser. From these plots we can see that the training and testing accuracy converges i n the expected manner, but the training loss either continues to decrease throughout training or co nverges signiﬁcantly later than the validation accuracy. This corroborates our hypothesis that users who s top based on train or validation accuracy are likely not to be at or near extrema of the loss, and hence no t beneﬁt from any enforced constraints. F A DDITIONAL EXPERIMEN T DETAILS W e provide a summary of each datasets used in our experiments in T able 2. W e selected datasets from CV online3 that were of a size for which ﬁne-tuning, rather than trainin g from scratch, is required to achieve good performance. Notably, the CUB-2011 and Stan ford Dogs datasets used by prior work (Li et al., 2018; 2019) to evaluate ﬁne-tuning regulari sers are not included. This is because these two datasets overlap with ImageNet, the dataset used d uring pre-training. As such, we cannot be sure that the pre-trained networks have not already been t rained on the test sets we are using for evaluation. Like Li et al. (2018; 2019), we also evaluate on C altech-256. However, there are no standard training, validation, and testing splits for this dataset, and the prior work did not release their splits. As a result, we constructed our own split that consis ts of 30, 20, and 20 examples per class for the training, validation, and testing folds, respectively . 3 http://homepages.inf.ed.ac.uk/rbf/CV online/Imagedbase.htm 18Published as a conference paper at ICLR 2021 0 5 10 15 20 25 300 0.2 0.4 0.6 0.8 1 1.2 Epoch Training Loss (a) ResNet-101 Loss 0 5 10 15 20 25 30 0.6 0.7 0.8 0.9 1 Epoch Accuracy Train T est (b) ResNet101 Accuracy 0 5 10 15 20 25 30 0 10 20 30 Epoch Training Loss (c) EfﬁcientNetB0 Loss 0 5 10 15 20 25 30 0 0.2 0.4 0.6 0.8 1 Epoch Accuracy Train T est (d) EfﬁcientNetB0 Accuracy Figure 4: Plots demonstrating the convergence properties o f MARS-SP when applied to ResNet101 and EfﬁcientNetB0 when ﬁne-tuned on the pets dataset. Subﬁg ures (a) and (c) demonstrate that the training loss does not necessarily converge at the same poin t as the accuracies in subﬁgures (b) and (d) have converged. Note that the learning rate is reduced by a factor of 10 at epoch 20. All regularisation hyperparameters are tuned using the Hyp erOpt package developed by Bergstra et al. (2015), which uses a tree of Parzen estimators to generate pr omising combinations of hyperparame- ters based on previously observed losses. T o remain compara ble with ℓ2-SP and DEL T A, we make use of only two hyperparameters for each of our proposed methods : the hyperparameter corresponding to the layer trained from scratch ( λL and γL for the penalty and constraint methods, respectively), and a single hyperparameter applied to all layers being ﬁne- tuned ( λj and γj ). This also makes the hyperparameter optimisation process reliable, as the numb er of hyperparameters that must be tuned is not proportional to the number of layers being ﬁne-tuned. HyperOpt is given 20 search iterations and hyperparameter combination with the best validation lo ss is selected. F.1 C O M PA RIS O N U S IN G DELTA E X P E RIM E N TA L SE T U P In order to facilitate a direct comparison with previously p ublished results achieved by ℓ2-SP and DEL T A, we conduct experiments using the same experimental s etup as DEL T A. These experiments involve data augmentation; namely, random horizontal ﬂips and random crops. A known issue with the experimental setup used for evaluation in prior work is t he overlap between the ImageNet dataset used for pre-training and the datasets used for evaluation. W e observe two interesting trends in the 19Published as a conference paper at ICLR 2021 T able 2: Statistics for the datasets used throughout the exp eriments. The Train, V alidation, and T est columns contain the number of instances in each of the corres ponding subsets. Dataset Train V alidation T est Classes Aircraft (Maji et al., 2013) 3,334 3,333 3,333 100 Butterﬂy (Chen et al., 2018) 5,135 5,135 15,009 200 Flowers (Nilsback and Zisserman, 2008) 1,020 1,020 6,149 102 Pets (Parkhi et al., 2012) 2,000 2,000 3,390 37 PubFig (Pinto et al., 2011) 10,518 1,660 1,660 83 DTD (Cimpoi et al., 2014) 1,880 1,880 1,880 47 Caltech (Grifﬁn et al., 2007) 7,680 5,120 5,120 256 T able 3: Comparison of ﬁne-tuning methods on DEL T A experime ntal setup using ResNet-101. Performance measurements for techniques other than MARS-P GM are taken from Li et al. (2019). Method Stanford Dogs CUB Food ℓ2-FR 84.7 ±0.1 61.5 ±0.1 64.3 ±0.1 ℓ2 83.3±0.2 78.4 ±0.1 85.3 ±0.1 ℓ2-SP (Li et al., 2018) 88.3 ±0.2 79.5 ±0.1 86.4±0.1 DEL T A (Li et al., 2019) 88.7±0.1 80.5 ±0.1 86.3±0.2 MARS-PGM (Ours) 88.7±0.2 79.6±0.1 86.3 ±0.1 results presented in T able 3. First, our method is still comp etitive with the existing ﬁne-tuning regu- larisers when using the ﬂawed experimental protocol utilis ed by previous work. Second, there is a very narrow range in performance for all methods compared to the e xperiments conducted in Section 5. W e suspect this is due to the overlap between the pre-trainin g set and the datasets used for ﬁne-tuning. Any strategy that keeps the weights or features sufﬁciently close to their pre-trained values will is likely sufﬁcient when there is overlap between the pre-trai ning set and the dataset used for ﬁne-tuning. 20",
      "meta_data": {
        "arxiv_id": "2002.08253v3",
        "authors": [
          "Henry Gouk",
          "Timothy M. Hospedales",
          "Massimiliano Pontil"
        ],
        "published_date": "2020-02-19T16:00:47Z",
        "pdf_url": "https://arxiv.org/pdf/2002.08253v3.pdf"
      }
    },
    {
      "title": "A Theoretical Analysis of Fine-tuning with Linear Teachers",
      "abstract": "Fine-tuning is a common practice in deep learning, achieving excellent\ngeneralization results on downstream tasks using relatively little training\ndata. Although widely used in practice, it is lacking strong theoretical\nunderstanding. We analyze the sample complexity of this scheme for regression\nwith linear teachers in several architectures. Intuitively, the success of\nfine-tuning depends on the similarity between the source tasks and the target\ntask, however measuring it is non trivial. We show that a relevant measure\nconsiders the relation between the source task, the target task and the\ncovariance structure of the target data. In the setting of linear regression,\nwe show that under realistic settings a substantial sample complexity reduction\nis plausible when the above measure is low. For deep linear regression, we\npresent a novel result regarding the inductive bias of gradient-based training\nwhen the network is initialized with pretrained weights. Using this result we\nshow that the similarity measure for this setting is also affected by the depth\nof the network. We further present results on shallow ReLU models, and analyze\nthe dependence of sample complexity there on source and target tasks. We\nempirically demonstrate our results for both synthetic and realistic data.",
      "full_text": "A Theoretical Analysis of Fine-tuning with Linear Teachers Gal Shachaf Blavatnik School of Computer Science, Tel Aviv University, Israel Alon Brutzkus Blavatnik School of Computer Science, Tel Aviv University, Israel Amir Globerson Blavatnik School of Computer Science, Tel Aviv University, Israel and Google Research Abstract Fine-tuning is a common practice in deep learning, achieving excellent general- ization results on downstream tasks using relatively little training data. Although widely used in practice, it is lacking strong theoretical understanding. Here we analyze the sample complexity of this scheme for regression with linear teachers in several architectures. Intuitively, the success of ﬁne-tuning depends on the similarity between the source tasks and the target task, however measuring this similarity is non trivial. We show that generalization is related to a measure that considers the relation between the source task, target task and covariance structure of the target data. In the setting of linear regression, we show that under realistic settings a substantial sample complexity reduction is plausible when the above measure is low. For deep linear regression, we present a novel result regarding the inductive bias of gradient-based training when the network is initialized with pretrained weights. Using this result we show that the similarity measure for this setting is also affected by the depth of the network. We further present results on shallow ReLU models, and analyze the dependence of sample complexity on source and target tasks in this setting. 1 Introduction In recent years ﬁne-tuning has emerged as an effective approach to learning tasks with relatively little labeled data. In this setting, a model is ﬁrst trained on a source task where much data is available (e.g., masked language modeling for BERT), and then it is further tuned using gradient descent methods on labeled data of a target task [1, 2, 3, 4]. Furthermore, it has been observed that ﬁne-tuning can outperform the strategy of ﬁxing the representation learned on the source task, mainly in natural language processing [1, 5]. Despite its empirical success, ﬁne-tuning is poorly understood from a theoretical perspective. One apparent conundrum is that ﬁne-tuned models can be much larger than the number of target training points, resulting in a heavily overparameterized model that is prone to overﬁtting and poor generalization. Thus, the answer must lie in the fact that ﬁne-tuning is performed with gradient descent and not an arbitrary algorithm that could potentially “ignore” the source task [6]. Here we set out to formalize this problem and understand the factors that determine whether ﬁne-tuning will succeed. We note that this question can be viewed as part of the general quest to understand the implicit bias of gradient based methods [6, 7, 8, 9, 10, 11, 12, 13], but in the particular context of ﬁne-tuning. 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2107.01641v2  [cs.LG]  7 Nov 2021We begin by highlighting the obvious link between ﬁne-tuning and initialization. Namely, the only difference between “standard” training of a target task and ﬁne-tuning on it, is the initial value of the model weights before beginning the gradient updates. Our goal is to understand the interplay between the model parameters at initialization (namely the source task), the target distribution, and the accuracy of the ﬁne-tuned model. A natural hypothesis is that the distance between the pretrained and ﬁne-tuned model weights is what governs the success of ﬁne-tuning. Indeed, some argue that this is both the key to bound the generalization error of a model and the implicit regularization of gradient-based methods [ 14, 15, 16, 17]. However, this approach has been discouraged both by empirical testing of the generalization bounds inspired by it [18] and by theoretical works showing this cannot be the inductive bias in deep neural networks [ 19]. Our results further establish the hypothesis that the success of ﬁne-tuning is affected by other factors. In this paper we focus on the case in which both source and target regression tasks are linear functions of the input. We start by considering one layer linear networks, and derive novel sample complexity results for ﬁne-tuning. We then proceed to the more complex case of deep linear networks, and prove a novel result characterizing the ﬁne-tuned model as a function of both the weights after pretraining and the depth of the network, and use it to derive corresponding generalization results. Our results provide several surprising insights. First, we show that the covariance structure of the target data has a signiﬁcant effect on the success of ﬁne-tuning. In particular, sample complexity is affected by the degree of alignment between the source-target weight difference and the eigenvectors of the target covariance. Second, we ﬁnd a strong connection between the depth of the network and the results of the ﬁne-tuning process, since deeper networks will serve to cancel the effect of scale differences between source and target tasks. Our results are corroborated by empirical evaluations. We conclude with results on ReLU networks, providing the ﬁrst sample complexity result for ﬁne- tuning. For the case of linear teachers, this asserts a simple connection between the source and target models and the test error of ﬁne-tuning. Taken together, our results demonstrate that ﬁne-tuning is affected not only by some notion of distance between the source and target tasks, but also by the target covariance and the architecture of the model. These results can potentially lead to improved accuracy in this setting via appropriate design of the tasks used for pretraining and the choice of the model architecture. 2 Related work Empirical work [20] has shown that two instances of models initialized from pre-trained weights are more similar in features space than those initialized randomly. Other works [21, 22, 23] have shown that ﬁne-tuned models generalize well when the representation used by the target task is similar to the one used by the source tasks. In linear regression, [24] showed that gradient descent ﬁnds the solution with minimal distance to the initial weights. More recently, attention has turned towards the phenomenon of “benign overﬁtting” [25, 26] in high dimensional linear regression, where despite ﬁtting noise in training data, population risk may be low. Theoretical analysis of this setting [ 25] studied how it is affected by the data covariance structure. Benign overﬁtting was also recently analyzed in the context of ridge-regression [27] and online stochastic gradient descent [ 28]. Our work continues this line of work on high dimensional regression, but differs from the above papers as we start from a source task, then train on a ﬁxed training set from a target task and consider the global optimum of the this training loss (unlike online SGD). Furthermore, we go beyond the linear regression framework, and obtain surprising characteristics of ﬁne-tuning in deep linear networks. For linear regression with deep linear models, [29] have recently shown an implicit bias for a two- layer network with deterministic initialization, and [30] have shown an implicit bias for a network with arbitrary depth and near-zero random initialization. Our work generalizes the inductive bias found by [29] to a network of arbitrary depth, and analyses the generalization error of such networks for inﬁnite depth. For linear regression with shallow linear networks [31] have shown a generalization bound that depends only on the norm of the target task, which we use in Section 6. 23 Preliminaries and settings Notations Let ∥·∥ be the L2 norm for vectors and the spectral norm for matrices. For a vector v we denote ˆv≜ v ∥v∥. For a matrix M ∈Rd×d and some 0 ≤m≤d, we deﬁne M≤m ∈Rd×m to be the matrix containing the ﬁrst mcolumns of M. Similarly, we let M>m denote the matrix containing the columns from m+ 1 to din M. Let Dbe a distribution overRd. Let Σ be the covariance matrix ofDand let VΛV⊤be its eigenvalue decomposition such that λ1 ≥... ≥λd. We deﬁne the projection matrices: P≤k ≜V≤kV⊤ ≤k; P>k ≜V>kV⊤ >k, projecting onto the span of the topkeigenvectors of Σ, onto the span of thed−kbottom eigenvectors of Σ, respectively. We will refer to the former as the “top- k span” of Σ, and to the latter as the “bottom-kspan” of Σ. Let X ∈Rn×d be the row matrix of n < dsamples drawn from D, and denote the empirical covariance matrix 1 nXTX by ˜Σ. Deﬁne P∥to be the projection matrix into the row space of X, and P⊥to be the projection matrix into its orthogonal complement, i.e.: P∥≜X⊤(XX⊤)−1X, P⊥≜I −P∥. Consider a set of parameters Θ, and let Θ(t) denote the set of parameters at time t. We denote the output of a model whose weights are Θ(t) on a vector x by f(x; Θ(t)) ∈R. In the different sections of this work we will overload f with different architectures. We consider the problem of ﬁne-tuning based transfer learning in regression tasks with linear teachers. Let θT ∈Rd be the ground-truth parameters of the target task, i.e. the linear teacher which we wish to learn, and y ∈Rn be the target labels of X, s.t. y = XθT. We deﬁne L(Θ) to be the empirical MSE loss on X,y and deﬁne R(Θ) as the Dpopulation loss: L(Θ) ≜1 n∥f(X,Θ) −y∥2 2 , R (Θ) ≜Ex∼D [( x⊤θT −f(x,Θ) )2] . We separate the training procedure into two parts. In the ﬁrst “pretraining” part, we train a model on nS pretraining samples XS ∈RnS×d labeled by a linear teacher θS (i.e., yS = XSθS ∈RnS), resulting in the set of model weights ΘS. In the second part, which we call ﬁne-tuning, we initialize a model with the pretrained weights Θ(0) = ΘS and learn the target task by optimizing L(Θ(t)). Optimization is done by either gradient descent (GD) or gradient ﬂow (GF). Let θ(t) be some weight vector or weight matrix in Θ(t). The dynamics for gradient descent optimization with some learning rate η >0 are θ(t+1) = θ(t)−η∂L(Θ(t)) ∂θ(t) , and the dynamics for gradient ﬂow are ˙θ(t) = −∂L(Θ(t)) ∂θ(t) . Next we state several assumptions about our setup. Assumption 3.1. XXT is non-singular. i.e. the rows of X are linearly-independent. This assumption holds with high probability for, e.g., a continuous distribution with support over a non-zero measure set. This assumption is only used for simplicity, as the high probability can be incorporated into the analysis. Assumption 3.2 (Perfect pretraining). The pretraining optimization process learns the linear teacher perfectly, e.g. for linear regression we assume thatf(x,ΘS) = x⊤θS, for x ∼D. Notice that for linear and deep linear models, perfect pretraining can be achieved when nS ≥d. Our results can be easily extended to the case where the equality f(x,ΘS) = x⊤θS holds approximately and with high probability, but for simplicity we assume equality. Assumption 3.3 (Zero train loss). The ﬁne-tuning converges, i.e. limt→∞L(Θ(t)) = 0. We note that when f is standard linear regression, arbitrarily small train loss can be obtained via gradient descent. For deep linear networks, it can be shown [32] that under suitable initialization a global optimum can be reached, and thus Assumption 3.3 holds for this framework as well. 34 Analyzing ﬁne-tuning in linear regression In this section we analyze ﬁne-tuning for the case of linear teachers for linear regression when using gradient descent for optimization. We deﬁneΘ(t) = w(t) ∈Rdand overloadf(x,Θ(t)) ≜x⊤w(t). In what follows we denote the parameter learned in the ﬁne-tuning process by γ≜limt→∞w(t). 4.1 Results The following known results (e.g., [ 24, 25, 10]) show the inductive bias of gradient descent with non-zero initialization in under-determined linear regression and the corresponding population loss. Theorem 4.1. [24, 25, 10] When f(x,Θ) is a linear function, ﬁne-tuning with GD under Assump- tion 3.1, Assumption 3.2 and Assumption 3.3 results in the following model: γ= P⊥θS + P∥θT, (1) and R(γ) = Σ 1/2 P⊥(θT −θS)  2 . (2) Theorem 4.1 provides two interesting observations: the ﬁrst is that γ consists of two parts, one which is the projection of the initial weights θS into the null space of X, and the other which is the projection of θT into the span of X. The second observation is that the population risk depends solely on the difference θT −θS that is projected to the null space of the data. For completeness, the proof of Theorem 4.1 is given in the supplementary. Theorem 4.1 depends on the data matrixX (via P∥,P⊥). However, to better understand the properties of ﬁne-tuning, a high probability bound on Rthat does not depend on X is desirable. We provide such a bound, highlighting the dependence of the population risk on the source and target tasks, and the target covariance Σ. Theorem 4.2. Assume the conditions of Theorem 4.1 hold, and assume that the rows of X are i.i.d. subgaussian centered random vectors. Then, there exists a constant c> 0, such that, for all δ≥1, and for all 1 ≤m≤dsuch that λm >0, with probability at least 1 −e−δ over X, the population risk R(γ) is bounded by: 2g(λ,δ,n)3 ∥P≤m(θT −θS)∥2 λ2m + 2g(λ,δ,n)∥P>m(θT −θS)∥2, (3) where g(λ,δ,n) = cλ1 max{ √∑ iλi nλ1 , ∑ iλi nλ1 , √ δ n, δ n}and ˜Σ −Σ ≤g(λ,δ,n). In the proof, we address the randomness of P⊥(θT −θS) in (2), by decomposing θT −θS into its top-kspan and bottom-kspan components, and then applying the Davis-Kahan sin(Θ) theorem [33] to bound the norm of the projection of the former to the null space of the data. The full proof is given in the supp. The bound in Theorem 4.2 has two key components. The ﬁrst is the function g(λ,δ,n) that captures how well the covariance Σ is estimated, and shows the dependence of the bound on the number of train samples used (as it depends on n−0.5). The second relates to the two matrix norms of θT −θS with respect to different parts of the covariance Σ. Notice that the term relating to the top-k span decreases like n−1.5, while the term relating to bottom-k span decreases like n−0.5. This theorem highlights the conditions under which ﬁne-tuning is expected to perform well. For small enough ns.t. g(λ,δ,n) >1, the bound mainly depends on∥P≤m(θT −θS)∥. In this case, the bound will be low if θT and θS are close in the span of the top eigenvectors of the target distribution. On the other hand, for large enough ns.t. g(λ,δ,n) <1, the bound mainly depends on ∥P>m(θT −θS)∥. Thus, the bound will be low if θT and θS are close in the span of the bottom eigenvectors of the target distribution. We conclude with a remark regarding the integer m appearing in the bound, in the case where g(λ,δ,n) < 1. While ﬁnding the exact m that minimizes the bound is not straightforward, the trade-off in selecting it suggests taking the largest mwhich holds λm+1 ≈λm. This will “cover” more of P>m(θT −θS) without greatly increasing the left part of (3). 4Table 1: Correlation coefﬁcient R2 between the accuracy on different transfer tasks in MNIST and various population risk upper bounds. Each value is a mean over 10 calculations of R2 with different initialization, and each R2 is calculated from 20 points, each one representing a mean accuracy value of 25 random samples. Number of Samples 10 15 20 25 30 ∥θT −θS∥2 0.69 ±0.03 0.68 ±0.04 0.66 ±0.04 0.64 ±0.03 0.62 ±0.02 Bound from [25] 0.73 ±0.03 0.75 ±0.03 0.74 ±0.03 0.71 ±0.02 0.67 ±0.02 Ours for m= 2 0.86 ±0.02 0.89 ±0.02 0.84 ±0.02 0.75 ±0.01 0.69 ±0.02 4.2 Experiments In Figure 1 we empirically verify the conclusions from the bound in (3). We set d= 1000 and design the target covariance Σ s.t. the ﬁrst m= 50 eigenvalues are signiﬁcantly larger than the rest (1.5 vs. 0.3). We then consider two settings for θT −θS. In the ﬁrst, which we call “Top Eigen Align”, we select θT and θS such that P≤m(θT −θS) = 0. In the second which we call “Bottom Eigen Align” we set P>m(θT −θS) = 0. In both settings we use the same norm ∥θT −θS∥2, to show that the bound is not affected by this norm. As discussed above, our bound suggests better generalization performance of “Bottom Eigen Align” for large nand better performance of “Top Eigen Align” for small n. Indeed, we see that while for very few samples “Top Eigen Align” has a lower population loss than “Bottom Eigen Align”, the population loss of ”Bottom Eigen Align” drops signiﬁcantly as ngrows, and drops to zero well before n= d. We next evaluate the bound on ﬁne-tuning tasks taken from the MNIST dataset [34], and compare it to alternative bounds. Speciﬁcally, since we do not expect bounds to be numerically accurate, we calculate the correlation between the actual risk in the experiment and the risk predicted by the bounds. The task we consider (both source and target) is binary classiﬁcation, which we model as regression to outputs {−1,+1}. We generate K source-target task pairs (e.g., source task is label 2 vs label 3 and target tasks is label 5 vs label 6). For each such pair we perform source training followed by ﬁne-tuning to target. We then record both the 0-1 error on an independent test set and the value predicted by the bounds. This way we obtain K pairs of points (i.e., actual error vs bound), and calculate the R2 for these pairs, indicating the level to which the bound agrees with the actual error. In addition to our bound in (3), we consider the following: the norm of source-target difference ∥θT −θS∥2 and a bound adapted from [25] to the case of ﬁne-tuning.1 The results in Table 1 show that there is a strong correlation between our bound and the actual error, and the correlation is weaker for the other bounds. 5 Analyzing ﬁne-tuning in deep linear networks In this section we focus on the setting of overparameterized deep linear networks. Although the resulting function is linear in its inputs, like in the previous section, we shall see that the effect of ﬁne-tuning is markedly different. Previous works (e.g. [ 35, 36]) have shown that linear networks exhibit many interesting properties which make them a good study case towards more complex non-linear networks. We consider networks withLlayers, given by the following matrices:Θ(t) = {W1(t),··· ,WL(t)} s.t. Wj(t) ∈Rdj−1×dj, d0 = d, dL = 1 and for 1 ≤j ≤L−1 : dj ≥d. We also deﬁne: β(t) = W1(t) ·W2(t) ···WL(t), such that f(x; Θ(t)) (t) = x⊤β(t). From Assumption 3.2, we have that β(0) = θS. We recall the condition of perfect balancedness (or 0-balancedness) [32]: Deﬁnition 5.1. The weights of a depth Ldeep linear network at time tare called 0-balanced if: Wj(t)⊤Wj(t) = Wj+1(t)Wj+1(t)⊤ for j ∈[L−1]. (4) 1The adaptation is straightforward: since the population loss for non-random initialization depends on θT − θS instead of θT , we can replace the ground-truth expression θ⋆ in Theorem 4 from [25] with θT − θS. 5Figure 1: Comparison between different θT −θS. ”Top Eigen Align” is the linear predictor initialized with P≤m(θT −θS) = 0 and ”Bottom Eigen Align” is the linear predictor initialized with P>m(θT −θS) = 0, for m=50. The top meigenvalues have the value 1.5, compared to the rest which have the value 0.3. Our analysis requires the initial random initialization (prior to pretraining) to be 0-balanced, which can be achieved with a near zero random initialization, as discussed in [32]. We provide three results on the effect of ﬁne-tuning in this setting. The ﬁrst result shows the inductive bias of ﬁne-tuning a depth Ldeep linear network (Theorem 5.2), which holds for arbitrary Land generalizes known results for L= 1 (Theorem 4.1) and L= 2 [29]. The second result analyzes the population risk of such a predictor when L→∞ for certain settings (Theorem 5.3 and Theorem 5.4). The third result shows why ﬁxing the ﬁrst layer (or any set of layers containing the ﬁrst layer) after pretraining can harm ﬁne-tuning (Theorem 5.5). The next theorem characterizes the model learned by ﬁne-tuning in the above setting (it can thus be viewed as the deep-linear version of the L= 1 result in Theorem 4.1): Theorem 5.2. Assume that before pretraining, the weights of the model were 0-balanced and that Assumption 3.1, Assumption 3.2 and Assumption 3.3 hold. Then: lim t→∞ β(t) = (∥limt→∞β(t)∥ ∥θS∥ )L−1 L P⊥θS + P∥θT (5) and: lim L→∞ lim t→∞ β(t) = ∥P∥θT∥ ∥P∥θS∥P⊥θS + P∥θT. (6) To prove this, we focus on W1, and notice that the gradients ˙W1(t) are in the span of X, and hence P⊥W1(0) and its norm remain static during the GF optimization ([30]). We then analyze the norm of the ﬁne-tuned model by using the 0-balancedness property of the weights and the min-norm solution to the equivalent linear regression problem, and achieve (5). (6) is achieved by calculating the limit w.r.t. L. The proof of Theorem 5.2 is given in the supplementary. Although the expression in (5) is not a closed form expression for limt→∞β(t) (because ∥limt→∞β(t)∥appears on the RHS), taking L to inﬁnity (6) does result in a closed form ex- pression and demonstrates the effect of increasing model depth. As in (1), we see that the end-to-end equivalent has two components: one which is parallel to the data and one which is orthogonal to it. However, while in (1) the orthogonal component has the original norm of the orthogonal projection of θS, the expression in (6) offers a re-scaling of the norm of this component by some ratio that also depends on θT. Presenting this phenomenon for the inﬁnity depth limit might look impractical, but the empirical results given in this section show that the effect of depth is apparent even for models of relatively small depth. 65.1 When Does Depth Help Fine-Tuning? In this subsection we wish to understand the effect of depth on the population risk of the ﬁne-tuned model. For simplicity we focus on the limit in (6), and denote β= limL→∞limt→∞β(t). Since the linear network is a linear function of x, we can derive an expression for the population risk of the network, similar to (2): R(β) = Σ 1 2 P⊥ ( θT −∥P∥θT∥ ∥P∥θS∥θS ) 2 . (7) However, since P∥depends on the random matrix X, without further assumptions this expression by itself is not enough to understand the behaviour of R(β). Theorem 5.3 and Theorem 5.4 analyze cases for which a bound on (7) can be achieved, showing that it depends on ∥θT∥( ˆθT −ˆθS), i.e. the product of the norm of θT and the difference of the normalized θT and θS, compared to (2) which depends on the difference between the un-normalized vectors. This observation further highlights the fact that the distance between source and target vectors is not a good predictor of ﬁne-tuning accuracy for some architectures, as ﬁne-tuning can still succeed even if the source and target are very far as long as they are aligned. We formalize this in the following result, where θT is identical to θS in direction, but not in norm. Theorem 5.3. Assume that the conditions of Theorem 5.2 hold, and that ˆθT = ˆθS. Namely: θT = αθS, for α> 0, then for L→∞ the risk of the end-to-end solution βis R(β) = 0, while for the L= 1 solution γ, the risk is: R(γ) = (α−1 α )2 ∥Σ 1/2 P⊥θT∥2 ̸= 0 for α̸= 1,α> 0. (8) This setting highlights our conclusion on the role of alignment in deep linear models: if the tasks are aligned, the deep linear predictor achieves zero generalization even with a single sample, while the population risk of the L= 1 predictor still depends on n. Another example for this behaviour can be seen when X is i.i.d Gaussian (i.e., D= N(0,1)d). Theorem 5.4. Assume that the conditions of Theorem 5.2 hold, and let X ∼N (0,1)d. Suppose n ≤ d, then there exists a constant c > 0 such that for any ϵ > 0 with probability at least 1 −4 exp(−cϵ2n) −4 exp ( −cϵ2(d−n) ) the population risk for the L→∞ end-to-end predictor βis bounded as follows: R(β) ≤d−n d (1 + ϵ)2 ∥θT∥2 ˆθT −ˆθS  2 + d−n d ζ(∥θT∥)2, (9) for ζ(∥θT∥) ≈ϵ∥θT∥. For the L= 1 linear regression solution γthis risk is bounded by R(γ) ≤d−n d (1 + ϵ)2 ∥θT −θS∥2 . (10) The above result is a direct analysis of (7) when Σ = I by using Lemma 5.3.2 from [37] to analyze the effects of P∥,P⊥. Comparing (9) and (10), we see that while (10) depends on the distance between the two un-normalized tasks, (9) depends on the norm of the target task and the alignment of the tasks, but not at all on the norm of the source task. The proofs of Theorem 5.3 and Theorem 5.4 are given in the supp. 5.2 Deep linear ﬁne-tuning with ﬁxing the ﬁrst layer(s) A common trick when performing ﬁne-tuning is to ﬁx, or “freeze” (i.e. not train), the ﬁrst klayers of a model during the optimization on the target task. This method reduces the risk of over-ﬁtting these layers to the small training set.2 The next theorem shows that for deep linear networks this method degenerates the training process. 2This over-ﬁtting is sometimes referred to as “catastrophic forgetting” of the source task. 7(a)  (b) Figure 2: (a) The effect of depth on ﬁne-tuning when θT is a αscaled, ϵnoised version of θS with d/10 samples. (b) The effect of changing the scale of either source weights or target weights in a 7-layers model. Theorem 5.5. Assume the setting of Theorem 5.2. Then, if we freeze the ﬁrst layer (or any number kof ﬁrst layers) during ﬁne-tuning, the ﬁne-tuned model will be given by ⟨β(t),x⟩= c⟨x,θS⟩, for some constant c. The key idea in the proof is to show that the product of the kﬁrst layers is equal to θS up to a scaling factor, which is a result of [30]. The result implies that after ﬁne-tuning the model is still equal to the source task, independently of the target task. Thus, ﬁne-tuning essentially fails completely, and its error cannot be reduced with additional target data. Figure 3: A network whose ﬁrst layer is ﬁxed has a constant generalization loss due to degeneration effect in Theorem 5.5. This result is achieved under the assumption of 0-balancedness prior to pretraining, which happens e.g. when initializing the weights with an inﬁnitesimally small variance, as this property leads to the degeneracy of the output of the frozen k-layers. Though the proof of Theorem 5.5 depends on this 0-balancedness property of the network, the experiments shown in Figure 3 were conducted with a small initialization scale, that is not guaranteed to result in 0-balancedness, but rather in δ-approximate balancedness [32] when δ is small. These experiments show empirically that the phenomenon of learning failure is observed even when δ >0. Intuitively, this is because the effective rank of the weight matrices is close to one, and thus learning the second layer is an ill-conditioned problem, which leads to slower convergence and can prevent the model from ﬁne-tuning on the target data with a constant gradient step. A possible workaround to this failure of learning would be to initialize the weights prior to pretraining with a larger scale of initialization (e.g. with Xavier [38]), thus increasing the rank of each layer and preventing degeneracy. Pre-training with multiple source tasks (as suggested in e.g. [22]) may also help the ﬁne-tuning optimization. 85.3 Experiments We next describe experiments that support the results in this section. Theorem 5.3 predicts that deeper nets will successfully learn a case where source and target vectors are aligned, but with different norms. This is demonstrated in Figure 2a where source and target tasks are related viaθT = αθS+ ϵ, where ϵis a standard Gaussian vector whose norm is approximately 0.5 ∥θS∥. It can be seen that when α ≈1, there is no difference between models of different depth. However, as αincreases, adding depth has a positive effect on ﬁne-tuning accuracy. Theorem 5.4 predicts that the test loss for a deep linear model would depend only on the alignment of θS and θT (i.e. ˆθT −ˆθS ) and on the ∥θT∥, but not on ∥θS∥. This is demonstrated in Figure 2b where source and target task are initialized s.t. ˆθT −ˆθS ≈0.1. In each experiment, either θT = αˆθT or θS = αˆθS, where αis the “Scaling Factor”, and the other has norm of 1. It can be seen that increasing the norm of the target vector harms generalization much more than increasing the norm of the source vector, as the theorem predicts, even for a relatively shallow model. Theorem 5.5 states that ﬁxing the ﬁrst layer in deep linear nets can result in failure to ﬁne-tune. We illustrate this empirically in Figure 3, where we compare three two-layer linear models on the same target task: 1) A “Frozen” model that ﬁxes the ﬁrst layer after pretraining. 2) A “Vanilla” model that trains the network from scratch on the target, ignoring the source pre-training. 3) A “Finetune” model that ﬁrst trains on source and ﬁne-tunes to target. As predicted by theory, the ”frozen” model’s performance is poor, and ﬁne-tuning has better sample complexity. 6 Analyzing ﬁne-tuning in shallow ReLU networks Analyzing optimization and generalization in non-linear networks is challenging. However, analysis in the Neural Tangent Kernel (NTK) regime is sometimes simpler [39, 31]. Thus, here we take a ﬁrst step towards understanding ﬁne-tuning in non-linear networks by analyzing this problem in the NTK regime. Speciﬁcally, we consider the setting of a two-layer ReLU network with mneurons in the hidden layer. Hence, we consider Θ(t) = {W(t),a}and f(x; Θ(t)) = 1√m ∑m r=1 arσ(x⊤wr(t)) where σis the ReLU function, w1(t),..., wm(t) ∈Rd, the rows of W(t), are vectors in the ﬁrst layer, and a∈{−1,1}m is the vector of weights in the second layer. We initialize auniformly and ﬁx it during optimization as in [39]. Before pretraining, the ﬁrst layer parameters are initialized from a standard Gaussian with variance κ2. We also assume that ∥x∥= 1 for all x samples from D. We let f(X,Θ) ∈Rn be the vector of predictions of f on the data X. For the next theorem we do not assume linear teachers, and instead assume an arbitrary labeling function gS such that yS = gS(XS), for XS ∈RnS×d,yS ∈RnS the pretraining data and labels, respectively. We also assume that y = gT(X) for some arbitrary function gT. For simplicity, we assume |y|i ≤1 for i ∈[n]. We consider a setting where the pretraining phase is done using a two-layer network in the NTK regime, under the assumptions of Theorem 4.1 from [31] with respect to the variables m, κ, ηand sufﬁciently many iterations.3 Next, in the ﬁne-tuning phase, we train a network initialized with the weights given by the pretraining phase. We use the same value of mfor the ﬁne-tuning phase. We rely on the analysis given in [39, 31] and achieve an upper bound on the population risk of the ﬁne-tuned model: Theorem 6.1. Fix a failure probabilityδ∈(0,1). We assume that Assumption 3.1 holds. Suppose κ= O (λ0δ n ) , m≥κ−2 poly ( n,nS,λ−1 0 ,δ−1) . Consider any loss function ℓ: R ×R →[0,1] that is 1-Lipschitz in the ﬁrst argument such that ℓ(y,y) = 0. Then with probability at least 1 −δ,4 the two-layer neural network f(·,Θ(t)) ﬁne-tuned by GD for t ≥Ω ( 1 ηλ0 log ∥˜y∥−1 2 ) iterations has population loss: R(Θ(t)) ≤2 √ ˜y⊤(H∞)−1 ˜y n + O   √ log n λ0δ n  , (11) for ˜y ≡y −f(X,Θ(0)). 3See the supp for a bound on the number of iterations. 4Over the random initialization of the pretraining network. 9The above result shows that the true risk of the ﬁne-tuned model is related to the distance of learned outputs y from the outputs after pretraining f(X,Θ(0)). The proof of Theorem 6.1 is given in the supp. As in previous NTK regime analyses, this result holds when the weights of the ﬁne-tuned model do not “move” too far away from the weights at random initialization. Thus, the proof approach is to bound the distance between the Gram matrix H(t) and the inﬁnite-width gram matrix H∞with a decreasing function in m. The main challenge is that the weights W(0) are not initialized i.i.d as described above. To address this we provide a careful analysis of the dynamics and show that H(t) is close to H at random initialization, even when considering the pretraining phase, which in turn is close to H∞. We next apply our results to the case of linear source and target tasks. We thus assume thatgS,gT are linear functions with parameters θS,θT. For simplicity of exposition we assume f(x,Θ(0)) = x⊤θS exactly (Assumption 3.2). Before bounding the risk of ﬁne-tuning we bound the RHS of (11) in the linear case: Corollary 6.2. Suppose that gS(X) ≜X⊤θS, gT(X) ≜X⊤θT, and assume Assumption 3.2 holds. Then, √ ˜y⊤(H∞)−1 ˜y≤3 ∥θT −θS∥2 . This is a direct corollary of Theorem 6.1 from [31] on ˜y deﬁned above. Theorem 6.1 and Corollary 6.2 result in the a bound on the risk of the ﬁne-tuned model: Corollary 6.3. Under the conditions of Theorem 6.1 and Corollary 6.2, it holds that R(Θ(t)) ≤6 ∥θT −θS∥2√n + O   √ log n λ0δ n  . We note that ﬁne-tuning is improved as the distance between source and target decreases. In our analysis of linear networks (Theorem 4.2 and Theorem 5.4) we obtained a more ﬁne-grained result depending on the covariance structure. We conjecture that the non-linear case will have similar results, which will likely involve the covariance structure in the NTK feature space. 7 Discussion This paper gives a ﬁne-grained analysis of the process of ﬁne-tuning with linear teachers in several different architectures. It offers insights into the inductive bias of gradient-descent and the implied relation between the source task, the target task and the target covariance that is needed for this process to succeed. We believe our conclusions pave a way towards understanding why some pretrained models work better than others and what biases are transferred from those models during ﬁne-tuning. A limitation of our work is the simplicity of the models analyzed, and it would certainly be interesting to extend these. Our setting deals only with linear teachers, and assumes the label noise to be zero. Furthermore, we only show upper bounds on the population risk, and not matching lower bounds. For deep linear networks we assume a certain initialization which is less standard than normalized initializers such as Xavier. For non-linear models, we analyze the simple model of a shallow ReLU network, and only in the NTK regime. An interesting direction to explore is formulating a bound similar to Theorem 4.2 for regression in the RKHS space given by the NTK, where the covariance is now over the RKHS space and thus more challenging to analyze. Another interesting setting is classiﬁcation with exponential losses. Since the classiﬁer learned by GD in this case has diverging norm, it is not clear how ﬁne-tuning is beneﬁcial, although in practice it often is. We leave these questions for future work. Acknowledgments and Disclosure of Funding This work has been supported by the Israeli Science Foundation research grant 1186/18 and the Yandex Initiative for Machine Learning. AB is supported by the Google Doctoral Fellowship in Machine Learning. 10References [1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019. [2] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086–6096, 2019. [3] Mor Geva, Ankit Gupta, and Jonathan Berant. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 946–958, 2020. [4] Suchin Gururangan, Ana Marasovi´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith. Don’t stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342–8360, 2020. [5] Matthew E Peters, Sebastian Ruder, and Noah A Smith. To tune or not to tune? adapting pre- trained representations to diverse tasks. In Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 7–14, 2019. [6] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. [7] Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In International Conference on Learning Representations, 2019. [8] Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In Conference on Learning Theory, pages 3635–3673. PMLR, 2020. [9] Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In Conference on Learning Theory, pages 1305–1338. PMLR, 2020. [10] Jingfeng Wu, Difan Zou, Vladimir Braverman, and Quanquan Gu. Direction matters: On the implicit bias of stochastic gradient descent with moderate learning rate. In International Conference on Learning Representations, 2021. [11] Edward Moroshko, Blake E Woodworth, Suriya Gunasekar, Jason D Lee, Nati Srebro, and Daniel Soudry. Implicit bias in deep linear classiﬁcation: Initialization scale vs training accuracy. Advances in Neural Information Processing Systems, 33, 2020. [12] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems, 32, 2019. [13] Roei Sarussi, Alon Brutzkus, and Amir Globerson. Towards understanding learning in neural networks with linear teachers. In International Conference on Machine Learning, 2021. [14] Vaishnavh Nagarajan and J Zico Kolter. Generalization in deep networks: The role of distance from initialization. arXiv preprint arXiv:1901.01672, 2019. [15] Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks. In International Conference on Artiﬁcial Intelligence and Statistics, pages 4313–4324. PMLR, 2020. [16] Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards understanding the role of over-parametrization in generalization of neural networks. In International Conference on Learning Representations (ICLR), 2019. 11[17] Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017. [18] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to ﬁnd them. arXiv preprint arXiv:1912.02178, 2019. [19] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [20] Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning? arXiv preprint arXiv:2008.11687, 2020. [21] Kurtland Chua, Qi Lei, and Jason D Lee. How ﬁne-tuning allows for effective meta-learning. arXiv preprint arXiv:2105.02221, 2021. [22] Simon Shaolei Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via learning the representation, provably. In International Conference on Learning Representations, 2021. [23] Daniel McNamara and Maria-Florina Balcan. Risk bounds for transferring representations with and without ﬁne-tuning. In International Conference on Machine Learning, pages 2373–2381. PMLR, 2017. [24] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry. In International Conference on Machine Learning, pages 1832–1841. PMLR, 2018. [25] Peter L Bartlett, Philip M Long, G´abor Lugosi, and Alexander Tsigler. Benign overﬁtting in linear regression. Proceedings of the National Academy of Sciences, 117(48):30063–30070, 2020. [26] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high- dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019. [27] Alexander Tsigler and Peter L Bartlett. Benign overﬁtting in ridge regression. arXiv preprint arXiv:2009.14286, 2020. [28] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham M Kakade. Benign overﬁtting of constant-stepsize sgd for linear regression. arXiv preprint arXiv:2103.12692, 2021. [29] Shahar Azulay, Edward Moroshko, Mor Shpigel Nacson, Blake Woodworth, Nathan Srebro, Amir Globerson, and Daniel Soudry. On the implicit bias of initialization shape: Beyond inﬁnitesimal mirror descent. arXiv preprint arXiv:2102.09769, 2021. [30] Chulhee Yun, Shankar Krishnan, and Hossein Mobahi. A unifying view on implicit bias in training linear neural networks. In International Conference on Learning Representations , 2020. [31] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op- timization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning, pages 322–332. PMLR, 2019. [32] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2018. [33] Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation. iii. SIAM Journal on Numerical Analysis, 7(1):1–46, 1970. [34] Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. 12[35] Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In International Conference on Machine Learning, pages 244–253. PMLR, 2018. [36] Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In 7th International Conference on Learning Representations, ICLR 2019, 2019. [37] Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018. [38] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural networks. In Yee Whye Teh and Mike Titterington, editors,Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pages 249–256, Chia Laguna Resort, Sardinia, Italy, 13–15 May 2010. PMLR. [39] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In International Conference on Learning Representations, 2018. [40] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high- performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e- Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. [41] Charles R. Harris, K. Jarrod Millman, St ´efan J. van der Walt, Ralf Gommers, Pauli Vir- tanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Hal- dane, Jaime Fern´andez del R´ıo, Mark Wiebe, Pearu Peterson, Pierre G´erard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357–362, September 2020. [42] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St´efan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antˆonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientiﬁc Computing in Python. Nature Methods, 17:261–272, 2020. [43] John D. Hunter. Matplotlib: A 2d graphics environment. Computing in Science Engineering, 9(3):90–95, 2007. [44] Vladimir Koltchinskii and Karim Lounici. Concentration inequalities and moment bounds for sample covariance operators. Bernoulli, 23(1):110–133, 2017. [45] jlewk (https://mathoverﬂow.net/users/141760/jlewk). Difference between identity and a random projection. MathOverﬂow. URL:https://mathoverﬂow.net/q/393720 (version: 2021-05-25). [46] Keyulu Xu, Mozhi Zhang, Jingling Li, Simon S Du, Ken-ichi Kawarabayashi, and Stefanie Jegelka. How neural networks extrapolate: From feedforward to graph neural networks. arXiv preprint arXiv:2009.11848, 2020. 13Code In the code used for the experiments we used Pytorch [ 40], Numpy [41], SciPy [42], and Matplotlib [43]. A Proofs for linear regression This appendix includes proofs for Section 4. It starts by analyzing the solution achieved by applying gradient descent on a linear regression problem with non-zero initialization, and shows its exact population risk. Then, this risk is bounded from above by using concentration bounds to bound various aspects of the difference between the true target covariance and the estimated target covariance. Recall the assumptions: Assumption 3.1 (Main Text). XXT is non-singular. i.e. the rows of X are linearly-independent. Assumption 3.2 (Main Text). The pretraining optimization process learns the linear teacher perfectly, e.g. for linear regression we assume that f(x,ΘS) = x⊤θS, for x ∼D. Assumption 3.3 (Main Text). The ﬁne-tuning converges, i.e. limt→∞L(Θ(t)) = 0. A.1 Proof of Theorem 4.1 As mentioned in the main text, both parts of the theorem have been proven before [24, 25, 10]. The proof is provided for completeness, and can be skipped. Lemma A.1. Assume Assumption 3.3, and that there exists some vector w ∈Rd s.t. y = Xw (i.e. the data is generated via a linear teacher), then the solution achieved by using GD with initialization θ0 in order to minimize: min θ∈Rd 1 2 ∥Xθ−y∥2 2. (12) is θ⋆ = P⊥θ0 + P∥w. (13) Proof. First, observe that the gradient step for this problem is θt+1 = θt + ηXT(y −XθT). Hence, all of the steps are in the span of XT, and GD converges to a solution of the form: θ⋆ = θ0 + XTa for some a∈Rn. The vector θ⋆ must also achieve a loss of zero in Equation (12) (because we know that w achieves a loss of zero, and GD minimizes this objective). Therefore: Xθ⋆ = y X(θ0 + XTa) = y XXTa= y −Xθ0 a 1 = (XXT)−1(y −Xθ0) ⇒θ⋆ = θ0 + XT(XXT)−1(y −Xθ0), with (1) due to Assumption 3.1. Replacing y with Xw, and by using the deﬁnitions of P∥and P⊥from Section 3, it follows that θ0 + XT(XXT)−1(y −Xθ0) = θ0 + XT(XXT)−1(Xw −Xθ0) = ( I −XT(XXT)−1X ) θ0 + XT(XXT)−1Xw = P⊥θ0 + P∥w. □ We can now prove the theorem. 14Proof of Theorem 4.1 (Main Text). The proof for Eq.1 in the main text is straightforward by using Lemma A.1 with θ0 = θS and w = θT. As for Eq.2 in the main text, by Lemma A.1 it follows that γ= P⊥θS + P∥θT. Since P∥+ P⊥= I it follows that R(γ) = Ex∼D [( x⊤θT −f(x; Θ(t)) )2] = Ex∼D [( x⊤( θT −P⊥θS −P∥θT ))2] = Ex∼D [( x⊤P⊥(θT −θS) )2] = Ex∼D [ (θT −θS)T P⊥xx⊤P⊥(θT −θS) ] = (θT −θS)T P⊥Ex∼D [ xx⊤] P⊥(θT −θS) = (θT −θS)T PT ⊥ΣP⊥(θT −θS) = Σ0.5P⊥(θT −θS) 2 . thus concluding the proof. □ A.2 Proof of Theorem 4.2: Upper bound of the population risk for linear regression Recall the Davis-Kahan sin(Θ) theorem: Theorem A.2 ([33]). Let A = E0A0ET 0 + E1A1ET 1 and A+ H = F0Λ0FT 0 + F1Λ1FT 1 be symmetric matrices with [E0,E1] and [F0,F1] orthogonal. If the eigenvalues of A0 are contained in an interval (a,b), and the eigenvalues of Λ1 are excluded from the interval (a−δ,b + δ) for some δ >0, then ∥FT 1 E0∥≤ ∥FT 1 HE0∥ δ (14) for any unitarily invariant norm ∥·∥. The following theorem is a concentration bound on the difference between the true and estimated covariance matrices: Σ −˜Σ : Theorem A.3 (Theorem 9 from [44]). Let X,X1,...,X n be i.i.d. weakly square integrable centered random vectors in Ewith covariance operator Σ.If X is subgaussian and pregaussian, then there exists a constant c> 0 such that, for all δ≥1,with probability at least 1 −e−δ, ∥˜Σ −Σ∥≤ c∥Σ∥max {√ r(Σ) n ,r(Σ) n , √ δ n, δ n } ≜g(λ,δ,n), where r(Σ) := (E∥x∥)2 ∥Σ∥ ≤tr(Σ) ∥Σ∥ = ∑ iλi λ1 . The following lemma uses Theorem A.2 to upper bound the dot product between the d−nbottom eigenvectors of the estimated covariance and the top keigenvectors of the target covariance: Lemma A.4. For all 1 ≤k≤dsuch that λk >0 it holds that: ˜VT >nV≤k ≤∥˜Σ−Σ∥ λk Proof. In order to use Theorem A.2 with δ = λk to bound ∥˜VT >nV≤k∥, one must show that the conditions of Theorem A.2 are met. Let A = Σ, A + H = ˜Σ, E0 = V≤k, A0 = Λ≤k, F1 = ˜V>n, and Λ1 = ˜Λ>n. Notice that X is a rank-nmatrix, and so is the estimated covariance ˜Σ, hence it bottom d−neigenvalues are zero. Thus, all of the d−neigenvalues of Λ1 equal zero. Also, recall that the eigenvalues of Σ are in descending order. Thus, all of the eigenvalues of A0 are in 15the interval (λk,λ1) and all of the eigenvalues of Λ1 (which equal 0) are excluded from the interval (0,λ1 + λk). Hence the conditions of Theorem A.2 are met and for δ= λk: ∥˜VT >nV≤k∥≤ ∥˜VT >n( ˜Σ −Σ)V≤k∥ λk (1) ≤ ∥˜V>n∥∥˜Σ −Σ∥∥V≤k∥ λk (2) = ∥˜Σ −Σ∥ λk , with (1) due to Cauchy-Schwartz inequality, (2) due to ˜V>n, V≤k being orthonormal matrices, which concludes the proof. □ We can now prove the theorem. Proof of Theorem 4.2 (Main Text). Let ˜U˜Γ ˜VT be the singular value decomposition of X such that ˜U ∈Rn×n, ˜V ∈Rd×d are unitary matrices and let ˜vi be the i-th column of ˜V. First, notice that P∥= X⊤(XX⊤)−1X can be also written as I −˜V>n˜VT >n: X⊤(XX⊤)−1X = ˜V˜Γ⊤˜UT( ˜U˜Γ ˜VT ˜V˜Γ⊤˜UT)−1 ˜U˜Γ ˜VT (1) = ˜V˜Γ⊤˜UT( ˜U(˜Γ˜Γ⊤) ˜UT)−1 ˜U˜Γ ˜VT = ˜V˜Γ⊤˜UT( ˜U(˜Γ˜Γ⊤) ˜UT)−1 ˜U˜Γ ˜VT (2) = ˜V˜Γ⊤˜UT ˜U(˜Γ˜Γ⊤)−1 ˜UT ˜U˜Γ ˜VT (3) = ˜V˜Γ⊤(˜Γ˜Γ⊤)−1 ˜Γ ˜VT = ˜V ·diag(11:n,0n+1:d) ·˜VT = n∑ i=1 ˜vi ·˜vT i = d∑ i=1 ˜vi ·˜vT i − d∑ i=n+1 ˜vi ·˜vT i (4) = I − d∑ i=n+1 ˜vi ·˜vT i = I −˜V>n˜VT >n. Where (1),(3),(4) are due to ˜U, ˜V being unitary, and (2) is due to ˜U(˜Γ˜Γ⊤) ˜UT( ˜U(˜Γ˜Γ⊤)−1 ˜UT) = I. From Eq.2 in the main text it follows that: R(γ) = Σ0.5P⊥(θT −θS) 2 = (θT −θS)T ˜V>n˜VT >nΣ ˜V>n˜VT >n(θT −θS) = (θT −θS)T ˜V>n˜VT >nVΛVT ˜V>n˜VT >n(θT −θS), Notice that P⊥˜ΣP⊥= 0, as was shown in [25]: P⊥˜Σ = P⊥˜V ˜Λ ˜VT = P⊥ ( ˜V≤n˜Λ≤n˜VT ≤n + ˜V>n˜Λ>n˜V⊤ >n ) = ˜V>n˜VT >n˜V≤n˜Λ≤n˜VT ≤n + ˜V>n˜VT >n˜V>n˜Λ>n˜VT >n (1) = 0 where (1) is due to ˜V>n, ˜V≤n being orthogonal and ˜λj = 0,∀j >n. 16Then: R(γ) = (θS −θT)⊤P⊥ΣP⊥(θS −θT) = (θS −θT)⊤P⊥ ( Σ −˜Σ ) P⊥(θS −θT) =  ( Σ −˜Σ )0.5 P⊥(θS −θT)  2 ≤ Σ −˜Σ ∥P⊥(θS −θT)∥2 , (15) where the last inequality is due to the Cauchy-Schwartz inequality. The next step in the proof is to bound ∥P⊥(θS −θT)∥2. We start by bounding ∥P⊥(θS −θT)∥by decomposing (θT −θS) to its top-kspan component and bottom-kspan component. First notice that since P⊥= ˜V>n˜VT >n, ∥P⊥(θS −θT)∥= ˜V⊤ >n(θS −θT) , we can write ∀k∈[d]: ∥P⊥(θS −θT)∥= ∥˜VT >n(θT −θ0)∥ = ∥˜VT >nVVT(θT −θ0)∥ = ∥˜VT >nV≤kVT ≤k(θT −θ0) + ˜VT >nV>kVT >k(θT −θ0)∥ ≤∥˜VT >nV≤k∥∥VT ≤k(θT −θ0)∥+ ∥˜VT >nV>k∥∥VT >k(θT −θ0)∥, (16) Where the last inequality is due to Cauchy Schwartz for matrix-vector. The last step in the proof is to bound ∥˜VT >nV≤k∥by using Lemma A.4 ∀k ∈[d] : λk >0, and bound ∥˜VT >nV>k∥by 1 as follows: ∥˜VT >nV>k∥≤∥ ˜V>n∥∥V>k∥≤ 1, due to ˜V>n and V>k being orthonormal matrices and because spectral norm is sub-multiplicative. Plugging (16) into (15) gives the inequality: R(γ) ≤  Σ −˜Σ  3/2 λk ∥P≤k(θS −θT)∥+ Σ −˜Σ  1/2 ∥P>k(θS −θT)∥  2 . Since 2a2 + 2b2 ≥(a+ b)2, it follows that: R(γ) ≤ 2 Σ −˜Σ  3 λ2 k ∥P≤k(θS −θT)∥2 + 2 Σ −˜Σ ∥P>k(θS −θT)∥2 . To conclude the proof we apply Theorem A.3 from [ 44] to provide a high probability bound forΣ −˜Σ , as was done in [25]. □ 17B Proofs for deep linear networks In this section we analyze the solution achieved by applying gradient ﬂow optimization to ﬁne-tuning a deep linear regression task (i.e. a regression task using a deep linear network as the regression model). Our results show that the population risk of a ﬁne-tuned deep linear model depends not only on the source and target tasks and the target covariance, as was shown in the previous section, but also on the depth of the model. We show that as the depth of the model goes to inﬁnity, its population risk depends on the difference between the directions of the source and target task (i.e. the difference between their normalized vectors), instead on the difference between the un-normalized task vectors. In Appendix B.2 this is shown by analysing two settings where this effect is most pronounced: one where we make an assumption on the target task (but not on the target covariance), and one where we make an assumption on the target covariance (but not on the target task). We conclude in Appendix B.3 by showing that ﬁne-tuning only some of the layers can lead to failure to learn. We begin by recalling some deﬁnitions. An L-layer linear fully-connected network is deﬁned as β(t) = W1(t) ···WL−1(t)WL(t), where Wl ∈Rdl×dl+1 for l∈[L−1] (we use d1 = d) and WL ∈RdL. Thus, the linear network is equivalent to a linear function with weights β. The weights of a deep linear network are called 0-balanced (or perfectly balanced) at time tif: W⊤ j (t)Wj(t) = Wj+1(t)W⊤ j+1(t) for j ∈[L−1]. (17) B.1 Proof of Theorem 5.2: The inductive bias of deep linear network ﬁne-tuning For this section, let ul, vl and sl denote the top left singular vector, top right singular vector and top singular value of the weights Wl, respectively. Deﬁne t= 0 as the end of pretraining. Before proving the theorem, we state several useful lemmas. Lemma B.1. Assume that at time tthe weights W1(t),..., WL(t) are 0-balanced. Then Wl(t) = ul(t)sl(t)v⊤ l (t), vl(t) = ul+1(t), (18) and: sl(t) = ∥β(t)∥ 1/L for l∈[L]. (19) Proof for Lemma B.1. This proof is a similar to the proof of Theorem 1 in [35]. Focusing on j = L−1 balancedness implies that: WL−1(t)⊤WL−1(t) = WL(t)WL(t)⊤. Hence, W⊤ L−1(t)WL−1(t) is (at most) rank-1 and so is WL−1(t). By iterating jfrom L−2 to 1, it follows that Wl(t) is rank-1 for j ∈[L]. Consider the SVD of the weights at time t. Since all weights are rank-1, they can be decomposed such that Wl(t) = ul(t)sl(t)vl(t)⊤. Plugging this into (17) it follows that vj(t)s2 j(t)v⊤ j (t) = uj+1(t)s2 j+1(t)u⊤ j+1(t) for j ∈[L−1], Thus proving (18) and showing that the top singular values of all the layers in timetare equal to each other.5 5maybe add in footnote that because the two matrices have the same SVD, their spectra are equal. 18We now consider the norm of the end to end solution at time t, β(t): ∥β(t)∥= ∥W1(t) ···WL(t)∥ = ∥u1(t)s1(t)v⊤ 1 s2(t) ···sL(t)∥ = ∥u1(t) L∏ i=1 sl(t)∥= L∏ i=1 sl(t)∥u1(t)∥= L∏ i=1 sl(t). Since all of the top singular values at time tequal each other, and ∥u1∥= 1 by construction, the result follows. □ The following Lemma is also used in the analysis: Lemma B.2 (Theorem 1 from [35]). Suppose a deep linear network is optimized using GF , starting from a 0-balanced initialization, i.e. initialization in which weights are 0-balanced. Then the weights stay balanced throughout optimization. We are now ready to prove the theorem. Proof of Theorem 5.2. First consider the pretraining of the model under Assumption 3.2. Assume that before the pretraining, the model weights are perfectly balanced. From Lemma B.2 it follows that after pretraining on the source task, i.e. at t = 0, the weights of the model are still balanced. From Lemma B.1, this means they are also rank-1. From Assumption 3.2: XSβ(0) = yS, and since nS >d this implies: β(0) = θS. (20) Lemma B.1 gives us that: β(0) = W1(0) ···WL(0) = u1(0) L∏ i=1 sl(0) = u1(0)sL 1 (0), Hence: u1(0) = θS ∥θS∥, and s1(0) = ∥θS∥ 1/L , (21) Hence: W1(0) = u1(0)s1(0)v⊤ 1 (0) = θS ∥θS∥∥θS∥ 1/L v⊤ 1 (0) = θS ∥θS∥ (L−1)/L v⊤ 1 (0). (22) We next analyze the ﬁne-tuning dynamics. Lemma B.2 ensures that if the pretrained model has 0-balanced weights, then the weights will remain 0-balanced during ﬁnetune. This implies that Lemma B.1 holds for all t≥0. Observe the gradient ﬂow dynamics of the layers during ﬁne-tuning: ˙Wl(t) = −WT l−1(t) ···WT 1 (t)XTr(t)WT L(t) ···WT l+1(t) for l∈[L], where r(t) ∈Rn is the residual vector satisfying [r]i = x⊤ i β(t) −yi. From Lemma B.1: ˙Wl(t) = −vl−1(t)sl−1(t)uT l−1(t)vl−2(t)sl−2(t)uT l−2(t) ··· v1(t)s1(t)uT 1 (t)XTr(t)vL(t)sL−1(t)uT L(t) ··· vl+1(t)sl+1(t)uT l+1(t) for l∈[L]. 19Using (18) and (19) it follows that ∀t≥0: ˙Wl(t) = −vl−1(t) (l−1∏ i=1 si(t) ) u1(t)TXTr(t) ( L∏ i=l+1 si(t) ) uT l+1(t) for l∈[L] = −vl−1(t)sl−1(t)uT 1 (t)XTr(t)sL−l(t)uT l+1(t) for l∈[L]. For W1, ˙W1(t) = −XTr(t)sL−1(t)uT 2 (t) = −XTr(t)sL−1(t)vT 1 (t), (23) Where the last equality is due to (18). Hence ˙W1 is always a rank-1 matrix whose columns are in the row space of X. This implies that the decomposition W1 into two orthogonal components W⊥ 1 and W∥ 1 so that W∥ 1 = P∥W1 and W⊥ 1 = P⊥W1 yields that ∀t≥0 it follows that ˙W⊥ 1 (t) = 0, ˙W∥ 1(t) = ˙W1(t) = XTr(t)sL−1(t)vT 1 (t). Hence, W⊥ 1 (t) does not change for all t≥0. Using (22) it follows: W⊥ 1 (t) = W⊥ 1 (0) (24) = P⊥ ( θS ∥θS∥ L−1 L v⊤ 1 (0) ) = P⊥θS ∥θS∥ L−1 L v⊤ 1 (0). (25) The next lemma states that v1(t) does not change during optimization if ∥P⊥W1(0)∥F >0. Lemma B.3. Suppose we run GF over a deep linear network starting from 0-balanced initialization. Also assume that at initialization W1(0) is rank-1 and: ∥P⊥W1(0)∥F >0, Then for all t> 0: v1(t) = v1(0). Proof. Assume towards contradiction that there exists t> 0 s.t. v1(t) ̸= v1(0). From W1(t) being rank-1 (Lemma B.1), it follows that P⊥W1(t) = P⊥u1(t)s(t)v⊤ 1 (t) = (P⊥u1(t)s(t)) v⊤ 1 (t), And from the decomposition of W1(t) to W∥ 1(t) and W⊥ 1 (t), (24) and W1(0) being rank-1 it follows that: P⊥W1(t) = W⊥ 1 (t) = W⊥ 1 (0) = P⊥u1(0)s1(0)v⊤ 1 (0), Hence: (P⊥u1(t)s(t)) v⊤ 1 (t) = (P⊥u1(0)s1(0)) v⊤ 1 (0). From (23) we see that the orthogonal part of u1(t) does not change during ﬁne-tune: ˙u1(t) = ˙W1(t) ·∂W1(t) ∂u1(t) = −XTr(t)sL−1(t)vT 1 (t)v1(t)s(t) = −XTr(t)sL(t) hence: P⊥ ˙u1(t) = 0 ⇒P⊥u1(t) = P⊥u1(0). (26) Since v1(t) ̸= v1(0), and because non-degenerate singular values always have unique left and right singular vectors (up to a sign), W⊥ 1 (t) = W⊥ 1 (0) only if: s(t) = s1(0) = 0, by contradiction to the assumption that s1(0) = ∥P⊥W1(0)∥F > 0, or if v1(t) = −v1(0) and P⊥u1(t) = −P⊥u1(0), which contradicts (26). □ 20In the case where ∥P⊥W1(0)∥F = 0 , since P⊥W1(t) = P⊥W1(0), it follows that W1(t) = P∥W1(t), which is similar to the case in [30], for which the solution is known to be P∥θT. Also, from (25), this implies P⊥θS = 0, and the expression for the end-to-end solution in Eq.5 in the main text holds. The analysis continues for ∥P⊥W1(0)∥F >0. By using Lemma B.1 and Lemma B.3 it follows that: W⊥ 1 (t)W2(t) ···WL(t) (1) = W⊥ 1 (0)W2(t) ···WL(t) (2) = P⊥θS ∥θS∥ L−1 L v⊤ 1 (0)W2(t) ···WL(t) (3) = P⊥θS ∥θS∥ L−1 L v⊤ 1 (t)W2(t) ···WL(t) = P⊥θS ∥θS∥ L−1 L v⊤ 1 (t)u2(t)∥β(t)∥ L−1 L (4) = P⊥θS ∥θS∥ L−1 L v⊤ 1 (t)v1(t)∥β(t)∥ L−1 L = (∥β(t)∥ ∥θS∥ )L−1 L P⊥θS. (27) With (1) due to (24), (2) due to (25), (3) due to Lemma B.3 and (4) due to Lemma B.1. From the requirement of Assumption 3.3 that limt→∞Xβ(t) = y, it follows that: lim t→∞ XW1(t) ···WL(t) = y ⇒lim t→∞ XW∥ 1(t) ·W2(t) ···WL(t) = y ⇒lim t→∞ W∥ 1(t) ·W2(t) ···WL(t) = XT ( XXT)−1 y, (28) Which is the only solution for this equation in the span of X, and due to Assumption 3.1. Eq.5 in the main text follows from (27) and (28): lim t→∞ β(t) = lim t→∞ W1(t) ·W2(t) ···WL(t) = lim t→∞ ( W∥ 1(t) + W⊥ 1 (t) ) ·W2(t) ···WL(t) = lim t→∞ W⊥ 1 (t) ·W2(t) ···WL(t) + W∥ 1(t) ·W2(t) ···WL(t) = (∥limt→∞β(t)∥ ∥θS∥ )L−1 L P⊥θS + P∥θT. (29) To prove Eq.6 from the main text, consider the norm oflimt→∞β(t). ∥lim t→∞ β(t)∥= √ (∥limt→∞β(t)∥ ∥θS∥ )2(L−1) L ∥P⊥θS∥2 + ∥P∥θT∥2 ⇒∥lim t→∞ β(t)∥2 = (∥limt→∞β(t)∥ ∥θS∥ )2(L−1) L ∥P⊥θS∥2 + ∥P∥θT∥2 ⇒∥lim t→∞ β(t)∥2 − (∥limt→∞β(t)∥ ∥θS∥ )2(L−1) L ∥P⊥θS∥2 −∥P∥θT∥2 = 0. 21At the limit L→∞ we get: lim l→∞  ∥lim t→∞ β(t)∥2 − (∥limt→∞β(t)∥ ∥θS∥ )2(L−1) L ∥P⊥θS∥2 −∥P∥θT∥2   =∥lim l→∞ lim t→∞ β(t)∥2 − (∥liml→∞limt→∞β(t)∥ ∥θS∥ )2 ∥P⊥θS∥2 −∥P∥θT∥2 = 0 ⇒∥liml→∞limt→∞β(t)∥2 ∥θS∥2 ( ∥θS∥2 −∥P⊥θS∥2 ) = ∥P∥θT∥2, Thus: ∥liml→∞limt→∞β(t)∥ ∥θS∥ = ∥P∥θT∥√ ∥θS∥2 −∥P⊥θS∥2 = ∥P∥θT∥ ∥P∥θS∥. And it follows that at this limit: lim L→∞ lim t→∞ β(t) = ∥P∥θT∥ ∥P∥θS∥P⊥θS + P∥θT. (30) □ From the same lines of proof as in Appendix A.1 it follows that Corollary B.4. For the conditions in Theorem 5.2 in the main text, R( lim L→∞ lim t→∞ β(t)) = Σ0.5 ( P⊥(θT −∥P∥θT∥ ∥P∥θS∥θS) ) 2 . B.2 Proofs of Theorems 5.3 and 5.4: How does depth affect the population risk? Corollary B.4 above contains dependence on P∥which is a random variable. We next provide high-probability risk bounds that can be derived from this result. The bounds are obtained under slightly different assumptions, either on the target task or on the target distribution, but both highlight the fact that ﬁne-tuning in the L→∞ case will depend on ˆθS −ˆθT rather than the un-normalized θS −θT. Recall the deﬁnition of the ﬁne-tuning solution as L→∞: β≜ lim L→∞ lim t→∞ β(t). In the ﬁrst setting we will assume that θT is a scaled version of θS, without any assumptions on D. Theorem 5.3 from the main text demonstrates a gap between perfect ﬁne-tuning for the L→∞ case and non-zero ﬁne-tuning error for L= 1. Proof of Theorem 5.3 (Main Text). First notice: ∥P∥θT∥ ∥P∥θS∥= ∥P∥αθS∥ ∥P∥θS∥ = α∥P∥θS∥ ∥P∥θS∥= α, (31) which from Eq.6 in the main text gives the solution β= αP⊥θS + P∥θT = P⊥θT + P∥θT = θT. On the other hand, for the L= 1 solution γit follows from Eq.2 in the main text that Σ0.5P⊥(θT −θS) 2 = Σ0.5P⊥ ( θT −θT α ) 2 = (α−1 α )2 Σ0.5P⊥θT 2 , which is greater than zero for all α̸= 1. □ 22In the second setting we assume that D= N(0,1)d, without any assumptions on θT. Here it shows that while the population risk of the L = 1 solution depends on ∥θT −θS∥, the population risk of the inﬁnitely-deep linear solution depends on the normalized ˆθT −ˆθS and ∥θT∥, i.e. on the alignment of θT and θS and the norm of θT. Theorem 5.4 (Main Text). Assume that the conditions of Theorem 5.2 hold, and let X ∼N(0,1)d. Suppose n≤d, then there exists a constant c> 0 such that for an ϵ> 0 it holds that with probability at least 1 −4 exp(−cϵ2n) −4 exp ( −cϵ2(d−n) ) the population risk for the L→∞ end-to-end β is bounded: R(β) ≤d−n d (1 + ϵ)2 ∥θT∥2 ˆθT −ˆθS  2 + d−n d ζ(∥θT∥)2, (32) for ζ(∥θT∥) ≈ϵ∥θT∥. For the L= 1 linear regression solution γthis risk is bounded by R(γ) ≤d−n d (1 + ϵ)2 ∥θT −θS∥2 . Proof of Theorem 5.5 (Main Text). We start by analyzing R(β): R(β) = Σ0.5P⊥ ( θT − P∥θT  P∥θS θS ) 2 (1) = I0.5P⊥ ( θT − P∥θT  P∥θS θS ) 2 = P⊥ ( θT −∥P⊥θT∥P∥θS θS ) 2 , where (1) is due to Σ = I from the deﬁnition of the distribution of X. We then bound the RHS with: P⊥ ( θT − P∥θT  P∥θS θS ) 2 ≤ P⊥ ( θT − P∥θT  P∥θS θS ) −P⊥ ( ∥θT∥( ˆθT −ˆθS) ) + P⊥ ( ∥θT∥( ˆθT −ˆθS) ) 2 ≤ P⊥ ( θT − P∥θT  P∥θS θS ) −P⊥ ( ∥θT∥( ˆθT −ˆθS) ) 2 + P⊥ ( ∥θT∥( ˆθT −ˆθS) ) 2 . We see that we can bound the expression on the left: P⊥ ( θT − P∥θT  P∥θS θS ) −P⊥ ( ∥θT∥( ˆθT −ˆθS) ) 2 = P⊥ ( θT − P∥θT  P∥θS θS −θT + ∥θT∥ˆθS ) 2 = P⊥ ( ∥θT∥ ∥θS∥θS − P∥θT  P∥θS θS ) 2 ≤ P⊥θS ( ∥θT∥ ∥θS∥− P∥θT  P∥θS  ) 2 ≤∥P⊥θS∥2  ∥θT∥ ∥θS∥− P∥θT  P∥θS   2 23Let P∥be the projection matrix onto the row space of X, then from [45], P∥is a projection onto a random n-dimensional subspace uniformly distributed in the Grassmannian Gd,n, and P⊥is a projection onto a random d−n-dimensional subspace uniformly distributed in the Grassmannian Gd,d−n. According to Lemma 5.3.2 in [37], with probability at least 1 −4 exp(−cϵ2n) 1 −ϵ 1 + ϵ ∥θT∥ ∥θS∥≤∥P∥θT∥ ∥P∥θS∥≤1 + ϵ 1 −ϵ ∥θT∥ ∥θS∥, which bounds:  ∥θT∥ ∥θS∥− P∥θT  P∥θS   2 ≤  ∥θT∥ ∥θS∥−1 + ϵ 1 −ϵ ∥θT∥ ∥θS∥  2 = (∥θT∥ ∥θS∥ )2 4ϵ2 (1 −ϵ)2 . Again, by applying Lemma 5.3.2 from [ 37], with probability at least 1 −4 exp ( −cϵ2(d−n) ) − 2 exp ( −cϵ2(d−n) ) : ∥P⊥θS∥2 ≤(1 + ϵ)2 d−n d ∥θS∥2 , P⊥∥θT∥ ( ˆθT −ˆθS ) 2 ≤(1 + ϵ)2 d−n d ∥θT∥ ( ˆθT −ˆθS ) 2 . Thus the following bound is obtained: R(β) ≤ P⊥ ( θT − P∥θT  P∥θS θS ) −P⊥ ( ∥θT∥( ˆθT −ˆθS) ) 2 + P⊥ ( ∥θT∥( ˆθT −ˆθS) ) 2 ≤(1 + ϵ)2 d−n d ∥θT∥( ˆθT −ˆθS)  2 + 4ϵ2(1 + ϵ)2 (1 −ϵ)2 d−n d ∥θS∥2 ∥θT∥2 ∥θS∥2 = (1 + ϵ)2 d−n d ∥θT∥( ˆθT −ˆθS)  2 + 4ϵ2(1 + ϵ)2 (1 −ϵ)2 d−n d ∥θT∥2 . Deﬁne ζ(∥θT∥) = 2ϵ(1+ϵ) (1−ϵ) ∥θT∥, which concludes the proof for the inﬁnite depth case. Now for the upper bound of the population risk of the L= 1 solution γ. Look at Eq.2, and from P⊥ being a random projection, it follows that with probability at least 1 −2 exp ( −cϵ2(d−n) ) : R(γ) ≤ Σ0.5P⊥(θT −θS) 2 = ∥IP⊥(θT −θS)∥2 ≤(1 + ϵ)2 d−n d ∥θT −θS∥2 . □ B.3 Proof of Theorem 5.5: The effect of ﬁxing layers during ﬁne-tuning Proof. Since we assume that the weights before pretraining are 0-balanced, it follows from Lemma B.1 and Lemma B.2 that all layers W1(t),... Wk(t) are rank-1. From Assumption 3.2 it follows that at the end of pretraining β(0) = θS, and from (22) it follows that u1(0) = ˆθS. Consider the setting where the ﬁrst klayers are ﬁxed. It follows that Wi(t) = Wi(0) ∀t≥0, 0 ≤i≤k. 24Then from Lemma B.1 it follows that for t≥0 and for any x ∈Rd: x⊤W1(t) ···Wk(t) = x⊤W1(0) ···Wk(0) = x⊤u1(0) k∏ i=1 siv⊤ k (0) = x⊤∥θS∥ k/L u1(0) ∥θS∥ k/L v⊤ k (0) = x⊤θS∥θS∥ k−L/L v⊤ k (0) = ∥θS∥ k−L/L ⟨x,θS⟩v⊤ k (0). Let’s deﬁne b(t) ≜Wk+1(t) ···WL(t), then for any constant c1(t) ≜⟨vk,b(t)⟩it follows : x⊤β(t) = x⊤W1(t) ···Wk(t) ·Wk+1(t) ···WL(t) = ∥θS∥ k−L/L ⟨x,θS⟩v⊤ k (0)b(t) = c1(t) ∥θS∥ k−L/L ⟨x,θS⟩. By setting c(t) = c1(t) ∥θS∥ k−L/L we conclude the proof. □ 25C Proofs for the shallow ReLU section This section shows that ﬁne-tuning from a shallow ReLU model pretrained on θS has sample complexity depending on ∥θT −θS∥, compared to training from a random initialization which depends on ∥θT∥. We would like to adapt the results from [31] to the case of ﬁne-tuning in the NTK regime, where we can take better advantage of the fact that the bound in Theorem 4.1 in [31] fundamentally depends on ∥˜y∥, thus enabling us to bound the distance of each weight from t= 0 by using ˜y instead of y for our case, where u(0) is known. The proof scheme is as follows: 1. First we show that ∥H(t) −H∞∥= O( 1√m), thus ensuring we are indeed in the NTK regime for mbounded from bellow as in Theorem 6.1 from the main text. 2. Then, we can use an adaption of Theorem 4.1 from [31] to bound the distance of each weight ∥wr(t) −wr(0)∥∀r∈[m]. 3. Since W(0) is ﬁxed, we can use the Rademacher bound in Theorem 5.1 from [ 31] with W(0) instead of W(init) to obtain a bound that depends on ˜y⊤H∞˜y instead of y⊤H∞y. 4. For ˜y = X(θT −θS), we can use Corollary 6.2 from [31] with β= (θT −θS) to obtain the generalization error using the Rademacher bound above. C.1 Staying in the NTK regime Start with the ﬁrst item: showing that ∥H(t) −H∞∥= O( 1√m). This is done by bounding the distance each wr∀r∈[m] travels during both the pretraining and ﬁne-tuning optimization, which is achievable by using Theorem 4.1 from [39] ”as is” for the pretraining part, and adapting it to the ﬁne-tuning part. Assumptions For brevity, we assume for the pretraining data that |xSi|≤ 1,|ySi|≤ 1 for all i∈[nS]. Also assume the following for all results: Assumption C.1. We assume that W(init), i.e. the weights at t = init, were i.i.d. initialized wr ∼N(0,I), ar ∼unif [{−1,1}] for r∈[m]. Also assume for X,Xs: Assumption C.2. Deﬁne matrix H∞∈Rn×n with H∞ ij = Ew∼N(0,I) [ x⊤ i xjI { w⊤xi ≥0,w⊤xj ≥0 }] . We assume λ0 ≜λmin (H∞) >0, and λ0S ≜λmin (H∞ S ) >0 for HS being the NTK gram matrix of the pretraining data XS. The assumption that λ0 >0 is justiﬁed by combining Assumption 3.1 and Theorem 3.1 from [39]. The assumption that λ0S > 0, which is actually the assumption for Theorem C.4, holds for most real-data data-sets and w.h.p for most real-life distributions, as discussed in [39]. Assumption C.3. We assume that m = Ω ( n6 s λ4 0sκ2δ3 ) , κ = O ( ϵδ√nS + ϵδ√n ) and ηT = O (λ0 n2 ) , ηS = O (λ0S n2 S ) . We now restate a few results from [39] which are applied directly for the part of pretraining: Theorem C.4 (Theorem 3.1 from [39]). If for any i̸= j, xi ∦ xj , then λ0 >0. Theorem C.5 (Theorem 3.3 from [39] for pretraining). Assume Assumption C.1, Assumption C.2 and Assumption C.3 hold, then with probability at least 1 −δover the random initialization at time t= init, we have: 1 2 ∥ys −u(init)∥= O(nS/δ) . 26Lemma C.6 (Lemma C.1 from [31]). Assume Assumption C.1, Assumption C.2 and Assumption C.3 hold, then there exists C >0 such that with probability at least 1 −δover the random initialization at time t= init we have ∥wr(0) −wr(init)∥2 ≤4√ns∥ys −u(init)∥√mλ0S ∀r∈[m]. Plugging Theorem C.5 into Lemma C.6 we get: Corollary C.7. Assume Assumption C.1, Assumption C.2 and Assumption C.3 hold, then there exists C >0 s.t. with probability at least 1 −2δover the random initialization at time t= init we have ∥wr(0) −wr(init)∥2 ≤ CnS√ mδλ0S ∀r∈[m]. Lemma C.8 (Lemma 3.2 from [39]). If w1,..., wm at t= init are i.i.d. generated from N(0,I), then with probability at least 1 −δ, the following holds. For any set of weight vectors w1,..., wm ∈ Rd that satisfy for any r∈[m], ∥wr(init) −wr∥2 ≤cδκλ0 n2 for some small positive constants c, then the matrix H ∈Rn×n deﬁned by Hij = 1 mx⊤ i xj m∑ r=1 I { w⊤ r xi ≥0,w⊤ r xj ≥0 } satisﬁes ∥H −H(init)∥2 < λ0 4 and λmin (H) > λ0 2 . We state the following lemmas that is used in the analysis: Lemma C.9 (Similar to Lemma C.2 from [31]). Assume Assumption C.1 holds. For some R> 0 we deﬁne: Ar,i ≜ { |x⊤ i wr(init)|≤ R } , (33) then with probability at least 1 −δon the initialization of W(init) we get: E[I{Ar,i}] ≤ 2R√ 2πκ, and: n∑ i=1 m∑ r=1 I{Ar,i}= O (mnR κδ ) . where the expectation is with respect to W(init). Proof. Since wr(init) has the same distribution as N(0,κ2) we have E[I{Ar,i}] ≤E[I { |x⊤ i wr(init)|≤ R } ] = Pr z∼N(0,κ2) [|z|≤ R] = ∫ R −R 1√ 2πκe−x2/2κ2 dx ≤ 2R√ 2πκ. Then we know E[∑n i=1 ∑m r=1 I{Ar,i}] ≤2mnR√ 2πκ. Due to Markov, with probability at least 1 −δwe have: n∑ i=1 m∑ r=1 I{Ar,i}= O (mnR κδ ) . □ We now state our equivalent for Theorem 4.1 from [39] : 27Theorem C.10 (Adaption of Theorem 4.1 from [39]). Suppose Assumption C.1 and Assumption C.2 hold and for all i∈[n], ∥xi∥2 = 1 and |yi|≤ Cfor some constant C. if we set the number of hidden nodes m= Ω (n5 ∥˜y∥2 λ4 0δ2 + n6 s λ4 0sκ2δ3 ) , and we set the step sizes ηT = O (λ0 n2 ) , ηS = O (λ0S n2 S ) then with probability at least 1 −2δover the random initialization we have for t= 0,1,2,... ∥y −u(t)∥2 2 ≤ ( 1 −ηλ0 2 )t ∥˜y∥2 2 ; (34) ∥wr(t) −wr(0)∥≤ 4√n∥˜y∥√mλ0 , ∀r∈[m]. Proof of Theorem C.10. We follow the exact proof as in [39], with the exception of using Lemma C.9 instead of Lemma 4.1, and Lemma C.8 instead of Lemma 3.2. The lower bound for mis derived from the requirement on the constant Rthat bounds the distance of wr(t) from the random initialization at t= init. Notice that: ∥wr(t) −wr(init)∥≤∥ wr(0) −wr(init)∥+ ∥wr(t) −wr(0)∥, ∀r∈[m], where the bound for the left expression on the R.H.S is given by with probability 1 −δby Corol- lary C.7. The bound for the right expression on the R.H.S is given as a corollary of (34): ∥wr(t) −wr(0)∥≤ η t−1∑ s=0  ∂L(X,Θ(s)) ∂wr(s) ≤η t∑ s=0 √n∥y −u(s)∥√m ≤η t∑ s=0 √n ( 1 −nλ0 2 )s/2 √m ∥y −u(s)∥ ≤η ∞∑ s=0 √n ( 1 −nλ0 2 )s/2 √m ∥y −u(s)∥= 4√n∥˜y∥√mλ0 . Hence we require R= CnS√ mδλ0S + 4√n∥˜y∥√mλ0 . From this requirement we derive the lower bound for m. □ Using Corollary C.7 and Theorem C.10 we obtain a the following corollary: Corollary C.11. Assume Assumption C.1, Assumption C.2 and Assumption C.3 hold, exists C >0 s.t. with probability at least 1 −2δover the random initialization at time t= init we have ∥wr(t) −wr(init)∥2 ≤∥wr(0) −wr(init)∥2 + ∥wr(t) −wr(0)∥2 ≤ CnS√ mδλ0S + 4√n∥˜y∥√mλ0 ∀r∈[m]. Restate Lemma C.2 and Lemma C.3 from [31]: Lemma C.12 (Adaption of Lemma C.2 from [31]). Under the same setting as Theorem C.10, with probability at least 1 −8δover the random initialization, for all t≥0 we have: ∥H(0) −H(init)∥F = O ( n2nS√mδ 3/2λ0Sκ ) , ∥H(t) −H(init)∥F = O ( n2nS√mδ 3/2λ0Sκ + n 5/2 ∥˜y∥√mλ0κδ ) , ∥Z(t) −Z(0)∥F = O (√ nnS√mδ 3/2κλ0S + n 3/2 ∥˜y∥√mλ0κδ ) , for Z(t) ≜ 1 m ∑n i=1 ∑m r=1 I { w⊤ r (t)xi >0 } . 28Proof. For the ﬁrst and seconds equality we use the exact proof of Lemma C.2 from [31], replacing the value of R with CnS√ mδλ0S and CnS√ mδλ0S + 4√n∥˜y∥√mλ0 respectively (by using Corollary C.7 and Corollary C.11 to bound the norm of the distance of each weight from initialization). The third equality also follows the same lines, with the difference being in: E [ ∥Z(t) −Z(0)∥2 F ] ≤ 1 m n∑ i=1 m∑ r=1 E [ I{Ar,i}+ I{∥wr(t) −wr(0)∥> 4√n∥˜y∥√mλ0 } ] ≤ 1 m ·mn· 2R√ 2πκ + n mδ. The last pass is justiﬁed due to the bound on ∥wr(t) −wr(0)∥for all r∈[m] with probability 1 −δ from Theorem C.10. The wanted result is obtained, again, by plugging the R.H.S of Corollary C.11 instead of R. □ Lemma C.13 (Lemma C.3 from [31]). with probability at least 1 −δ, we have ∥H(init) −H∞∥= O ( n √ log n δ√m ) . Using the results above, the wanted results of this section follows: Corollary C.14. Under the same setting as Theorem C.10, with probability at least 1 −9δover the random initialization we have have ∥H(t) −H∞∥= O ( n2nS√mδ 3/2λ0Sκ + n 5/2 ∥˜y∥√mλ0κδ ) , ∥H(0) −H∞∥= O ( n2nS√mδ 3/2λ0Sκ ) . Proof. This corollary is direct by bounding∥H(t) −H∞∥≤∥ H(init) −H∞∥+∥H(t) −H(init)∥ and using Lemma C.13 and Lemma C.12 to bound the R.H.S for the general t >0 case and for t= 0. □ C.2 Bound the distance from initialization Write the eigen-decomposition H∞= n∑ i=1 λiviv⊤ i , where v1,..., vn ∈Rn are orthonormal eigenvectors of H∞and λ1,...,λ n are corresponding eigenvalues. also deﬁne Ii,r(t) ≜I { w⊤ r (t)xi ≥0 } . Theorem C.15 (Adaption of Theorem 4.1 from [31]). Assume Assumption C.2, and suppose m= Ω ( n5∥˜y∥4 2 ϵ2κ2δ2λ4 0 + n4n2 s∥˜y∥2 2 ϵ2λ2 0sλ2 0κ2δ3 ) . Then with probability at least 1 −δ over the random initialization before pretraining (t= init), for all t= 0,1,2,... we have: ∥y −u(t)∥2 = √ n∑ i=1 (1 −ηλi)2t( v⊤ i ˜y )2 ±ϵ. (35) We ﬁrst note the important difference between this result and the original theorem is in the treatment of u(0), the predictions of the model att= 0. While the original theorem shows that these predictions could be treated as negligible noise (for large enough m), we instead use them as part of the bound to the convergence of the training loss. 29Proof. The core of our proof is to show that when mis sufﬁciently large, the sequence {u(t)}∞ t=0 stays close to another sequence {˜u(t)}∞ t=0 which has a linear update rule: ˜u(0) = u(0), ˜u(t+ 1) = ˜u(t) −ηH∞(˜u(t) −y) . (36) From (36) we have ˜u(t+ 1) −y = (I −ηH∞) (˜u(t) −y) , which implies ˜u(t) −y = (I −ηH∞)t(˜u(0) −y) = −(I −ηH∞)t˜y. Note that (I −ηH∞)t has eigen-decomposition (I −ηH∞)t = n∑ i=1 (1 −ηλi)tviv⊤ i and that ˜y can be decomposed as ˜y = n∑ i=1 (v⊤ i ˜y)vi. Then we have ˜u(t) −y = − n∑ i=1 (1 −ηλi)t(v⊤ i ˜y)vi, which implies ∥˜u(t) −y∥2 2 = n∑ i=1 (1 −ηλi)2t(v⊤ i ˜y)2. (37) To prove that the two sequences stay close, we follow the exact proof of Theorem 4.1 in Appendix C of [31]. We start by observing the difference between the predictions at two successive steps: ui(t+ 1) −ui(t) = 1√m m∑ r=1 ar [ σ ( wr(t+ 1)⊤xi ) −σ ( wr(t)⊤xi )] . (38) For each i∈[n], divide the mneurons into two parts: the neurons that can change their activation pattern of data-point xi during optimization and those which can’t. Since |xi|≤ 1, a neuron cannot change its activation pattern with respect to xi if |x⊤ i wr(init)|>R and |wr(t) −wr(init)|≤ Rfor the value of Rin Corollary C.11. Deﬁne the indices of the neurons in this group (i.e. cannot change their activation pattern...) as as ¯Si, and the indices of the complementary group as Si. From Lemma C.9 we know that with probability 1 −δ, for R= ( nS√ mδλ0S + √n∥˜y∥√mλ0 ) |¯Si|≤ O (mn κδ ( nS√ mδλ0S + √n∥˜y∥√mλ0 )) . (39) Following the same steps as in [31] and notice that (38) can be treated as: u(t+ 1) −u(t) = −ηH(t) (u(t) −y) + ϵ(t), (40) where: ϵi(t) ≜ 1√m ∑ r∈¯Si [ σ ( wr(t+ 1)⊤xi ) −σ ( wr(t)⊤xi )] + η m n∑ j=1 (uj(t) −yj)x⊤ j xi ∑ r∈¯Si Ir,i(t)Ir,j(t). 30Next use (39) to bound ∥ϵ(t)∥: ∥ϵ(t)∥2 ≤∥ϵ(t)∥1 ≤ n∑ i=1 2η√n|¯Si| m ∥u(t) −y∥2 = O (√mn3/2 κδ3/2 (√ δ∥˜y∥2 λ0 + ns√nλ0s )) 2η√n m ∥u(t) −y∥2 = O ( ηn2 √mκδ3/2 (√ δ∥˜y∥2 λ0 + ns√nλ0s )) ∥u(t) −y∥2 . Notice from Corollary C.14 that H(t) stays close to H∞. Then it is possible to rewrite Equation (40) as u(t+ 1) −u(t) = −ηH∞(u(k) −y) + ζ(t), (41) where ζ(t) = −η(H∞−H(t)) (u(k) −y) + ϵ(t). Using Corollary C.14 it follows that ∥ζ(t)∥2 ≤η∥H∞−H(t)∥2 ∥u(t) −y∥2 + ∥ϵ(t)∥2 = O (ηn5/2 ∥˜y∥2√mκδλ0 + ηn2ns√mλ0sκδ3/2 ) ∥u(t) −y∥2 + O ( ηn2 √mκδ3/2 (√ δ∥˜y∥2 λ0 + ns√nλ0s )) ∥u(t) −y∥2 = O (ηn5/2 ∥˜y∥2√mκδλ0 + ηn2ns√mλ0sκδ3/2 ) ∥u(t) −y∥2 . (42) Apply (41) recursively and get: u(t) −y = −(I −ηH∞)t ˜y + t−1∑ s=0 (I −ηH∞)tζ(t−1 −s). (43) For the left term in (43) we’ve shown in (37) that: −(I −ηH∞)t(˜y)  2 = √ n∑ i=1 (1 −ηλi)2t(v⊤ i ˜y)2. The right term in (43) can be bounded using (42): t−1∑ s=0 (I −ηH∞)sζ(t−1 −s)  2 ≤ t−1∑ s=0 ∥I −ηH∞∥s 2 ∥ζ(t−1 −s)∥2 ≤ t−1∑ s=0 (1 −ηλ0)sO (ηn5/2 ∥˜y∥2√mκδλ0 + ηn2ns√mλ0sκδ3/2 ) ∥u(t−1 −s) −y∥2 ≤ t−1∑ s=0 (1 −ηλ0)sO (ηn5/2 ∥˜y∥2√mκδλ0 + ηn2ns√mλ0sκδ3/2 )( 1 −ηλ0 4 )t−1−s ∥˜y∥2 ≤t ( 1 −ηλ0 4 )t−1 O ( ηn5/2 ∥˜y∥2 2√mκδλ0 + ηn2ns∥˜y∥2√mλ0sκδ3/2 ) . Combining all of the above it follows: ∥u(t) −y∥2 = √ n∑ i=1 (1 −ηλi)2t(v⊤ i ˜y)2 ±O ( t ( 1 −ηλ0 4 )t−1 ( ηn5/2 ∥˜y∥2 2√mκδλ0 + ηn2ns∥˜y∥2√mλ0sκδ3/2 )) = √ n∑ i=1 (1 −ηλi)2t(v⊤ i ˜y)2 ±O ( n5/2 ∥˜y∥2 2√mκδλ2 0 + n2ns∥˜y∥2√mλ0sλ0κδ3/2 ) . 31where we used max t≥0 { t(1 −ηλ0/4)t−1} = O(1/(ηλ0)). From the choices of κand m, the above error term is at most ϵ. This completes the proof of Theorem C.15. □ C.3 Deriving a population risk bound Before proving Theorem 6.1 from the main text, we start by stating and proving some Lemmas: Lemma C.16. Suppose m≥κ−2 poly ( ∥˜y∥2 ,n,n s,λ−1 0 ,λ−1 0s ,δ−1) and η = O (λ0 n2 ) . Then with probability at least 1 −δover the random initialization at t= init, we have for all t≥0: • ∥wr(t) −wr(0)∥2 = O (√n∥˜y∥2√mλ0 ) (∀r∈[m]), and • ∥W(t) −W(0)∥F ≤ √ ˜y⊤(H∞)−1 ˜y + poly ( ∥˜y∥2,n,ns, 1 λ0 , 1 λ0s ,1 δ ) m1/4κ1/2 . Proof. The bound on the movement of each wr is proven in Theorem C.10. The second bound is achieved by coupling the trajectory of {W(t)}∞ k=0 with another simpler trajectory { ˜W(t) }∞ k=0 deﬁned as: ˜W(0) = W(0), vec ( ˜W(t+ 1) ) = vec ( ˜W(t) ) (44) −ηZ(0) ( Z(0)⊤vec ( ˜W(t) ) −y ) . First we give a proof of ˜W(∞) −˜W(0)  F = √ ˜y⊤H(0)−1˜y as an illustration for the proof of Lemma C.16. Deﬁne v(t) = Z(0)⊤vec ( ˜W(t) ) ∈ Rn. Then from (44) we have v(0) = Z(0)⊤vec (W(0)) and v(k+ 1) = v(t) −ηH(0)(v(t) −y), yielding v(t) −y = −(I −ηH(0))t˜y. Plugging this back to (44) we get vec ( ˜W(t+ 1) ) −vec ( ˜W(t) ) = ηZ(0)(I −ηH(0))t˜y. Then taking a sum over k= 0,1,... we have vec ( ˜W(∞) ) −vec ( ˜W(0) ) = ∞∑ k=0 ηZ(0)(I −ηH(0))k˜y = Z(0)H(0)−1˜y. The desired result thus follows:˜W(∞) −˜W(0)  2 F = ˜y⊤H(0)−1Z(0)⊤Z(0)H(0)−1˜y = ˜y⊤H(0)−1˜y. Now we bound the difference between the trajectories. Recall the update rule for W: vec (W(t+ 1)) = vec (W(t)) −ηZ(t)(u(t) −y). (45) Follow the same steps from Lemma 5.3 from [31], using the results from Theorem C.15 when needed to obtain the proof for this lemma. According to the proof of Theorem C.15 we can write u(t) −y = −(I −ηH∞)t˜y + e(t), (46) where ∥e(t)∥= O ( t ( 1 −ηλ0 4 )t−1 · ( ηn5/2 ∥˜y∥2 2√mκδλ0 + ηn2ns∥˜y∥2√mλ0sκδ3/2 )) . (47) 32Plugging (46) into (45) and taking a sum over t= 0,1,...,T −1, we get: vec (W(T)) −vec (W(0)) = T−1∑ t=0 (vec (W(t+ 1)) −vec (W(t))) = − T−1∑ t=0 ηZ(t)(u(t) −y) = T−1∑ t=0 ηZ(t) ( (I −ηH∞)t˜y −e(t) ) = T−1∑ t=0 ηZ(t)(I −ηH∞)t˜y − T−1∑ t=0 ηZ(t)e(t) = T−1∑ t=0 ηZ(0)(I −ηH∞)t˜y + T−1∑ t=0 η(Z(t) −Z(0))(I −ηH∞)t˜y − T−1∑ t=0 ηZ(t)e(t). (48) The second and the third terms in (48) are considered perturbations, and we can upper bound their norms easily. For the second term, from Lemma C.8 we get:  T−1∑ t=0 η(Z(t) −Z(0))(I −ηH∞)ty  2 ≤ T−1∑ t=0 η·O (√ n3/2 ∥˜y∥2√mκδλ0 + nns√mκλ0sδ3/2 ) ∥I −ηH∞∥t 2 ∥˜y∥2 ≤O ( η √ n3/2 ∥˜y∥2√mκδλ0 + nns√mκλ0sδ3/2 )T−1∑ t=0 (1 −ηλ0)t∥˜y∥2 = O   √ n3/2 ∥˜y∥3 2√mκδλ3 0 + nns∥˜y∥2 2√mκλ0sλ2 0δ3/2  . (49) For the third term we get:  T−1∑ t=0 ηZ(t)e(t)  2 ≤ T−1∑ t=0 η√n·O ( t ( 1 −ηλ0 4 )t−1 · ( ηn5/2 ∥˜y∥2 2√mκδλ0 + ηn2ns∥˜y∥2√mλ0sκδ3/2 )) = O (( η2n3 ∥˜y∥2 2√mκδλ0 + η2n5/2ns∥˜y∥2√mλ0sκδ3/2 )T−1∑ t=0 t ( 1 −ηλ0 4 )t−1) = O (( η2n3 ∥˜y∥2 2√mκδλ0 + η2n5/2ns∥˜y∥2√mλ0sκδ3/2 ) · 1 ηλ0 ) = O ( ηn3 ∥˜y∥2 2√mκδλ2 0 + ηn5/2ns∥˜y∥2√mλ0sλ0κδ3/2 ) . (50) Deﬁne K = η∑T−1 t=0 (I −ηH∞)t. using ∥H(0) −H∞∥F = O ( n2ns√mλ0sκδ3/2 ) (Corollary C.14) we have 33 T−1∑ t=0 ηZ(0)(I −ηH∞)t˜y  2 2 (51) = ∥Z(0)K˜y∥2 2 (52) = ˜y⊤KZ(0)⊤Z(0)K˜y (53) = ˜y⊤KH(0)K˜y (54) ≤˜y⊤KH∞K˜y + ∥H(0) −H∞∥2 ∥K∥2 2 ∥˜y∥2 2 (55) ≤˜y⊤KH∞K˜y + O ( n2ns√mλ0sκδ3/2 ) · ( η T−1∑ t=0 (I −ηλ0)t )2 ∥˜y∥2 2 (56) = ˜y⊤KH∞K˜y + O ( n2ns∥˜y∥2 2√mλ0sλ2 0κδ3/2 ) . (57) Let the eigen-decomposition of H∞be H∞= ∑n i=1 λiviv⊤ i . Since K is a polynomial of H∞, it has the same set of eigenvectors as H∞, and we have K = n∑ i=1 η T−1∑ t=0 (1 −ηλi)tviv⊤ i = n∑ i=1 1 −(1 −ηλi)T λi viv⊤ i . It follows that KH∞K = n∑ i=1 (1 −(1 −ηλi)T λi )2 λiviv⊤ i ⪯ n∑ i=1 1 λi viv⊤ i = (H∞)−1 . Plugging this into (51), we get  T−1∑ t=0 ηZ(0)(I −ηH∞)t˜y2 ≤ √˜y⊤(H∞)−1˜y + O ( n2ns∥˜y∥2 2√mλ0sλ2 0κδ3/2 ) (58) ≤ √ ˜y⊤(H∞)−1˜y + O   √ n2ns∥˜y∥2 2√mλ0sλ2 0κδ3/2  . (59) Finally, plugging the three bounds (49), (50) and (58) into (48), we have ∥W(T) −W(0)∥F = ∥vec (W(T)) −vec (W(0))∥2 ≤ √ ˜y⊤(H∞)−1˜y + O   √ n2ns∥˜y∥2 2√mλ0sλ2 0κδ3/2  + O   √ n3/2 ∥˜y∥3 2√mκδλ3 0 + nns∥˜y∥2 2√mκλ0sλ2 0δ3/2   + O ( ηn3 ∥˜y∥2 2√mκδλ2 0 + ηn5/2ns∥˜y∥2√mλ0sλ0κδ3/2 ) = √ ˜y⊤(H∞)−1˜y + poly ( ∥˜y∥2 ,n,n s, 1 λ0 , 1 λ0s ,1 δ ) m1/4κ1/2 . This ﬁnishes the proof of Lemma C.16. □ Lemma C.17. Given R > 0, with probability at least 1 −δ over the random initialization (W(init),a), simultaneously for every B >0, the following function class FW(0),a R,B = {fW : ∥wr −wr(0)∥2 ≤R(∀r∈[m]), ∥W −W(0)∥F ≤B} 34has empirical Rademacher complexity bounded as: RS ( FW(0),a R,B ) = 1 nEε∈{±1}n   sup f∈FW(0),a R,B n∑ i=1 εif(xi)   ≤ B√n + 2R(R+ Cns√ mδλ0S )√m κ + R √ 2 log 2 δ. Proof. We need to upper bound RS ( FW(0),a R,B ) = 1 nEε∼{±1}n   sup f∈FW(0),a R,B n∑ i=1 εif(xi)   = 1 nEε∼{±1}n   sup W:∥W−W(0)∥2,∞≤R ∥W−W(0)∥F≤B n∑ i=1 εi m∑ r=1 1√marσ(w⊤ r xi)  , where ∥W −W(0)∥2,∞= max r∈[m] ∥wr −wr(0)∥2. Similar to the proof of Lemma C.9, we deﬁne events: ˜Ar,i ≜ {⏐⏐wr(0)⊤xi ⏐⏐≤R } , i ∈[n],r ∈[m]. Since we only look at W such that ∥wr −wr(0)∥2 ≤Rfor all r ∈[m], if I{˜Ar,i}= 0 we must have I{w⊤ r xi >0}= I{wr(0)xi ≥0}= Ir,i(0). Thus we have: I { ¬˜Ar,i } σ ( w⊤ r xi ) = I { ¬˜Ar,i } Ir,i(0)w⊤ r xi, It follows that: n∑ i=1 εi m∑ r=1 arσ ( w⊤ r xi ) − n∑ i=1 εi m∑ r=1 arIr,i(0)w⊤ r xi = m∑ r=1 n∑ i=1 ( I { ˜Ar,i } + I { ¬˜Ar,i }) εiar ( σ ( w⊤ r xi ) −Ir,i(0)w⊤ r xi ) = m∑ r=1 n∑ i=1 I { ˜Ar,i } εiar ( σ ( w⊤ r xi ) −Ir,i(0)w⊤ r xi ) = m∑ r=1 n∑ i=1 I { ˜Ar,i } εiar ( σ ( w⊤ r xi ) −Ir,i(0)wr(0)⊤xi −Ir,i(0)(wr −wr(0))⊤xi ) = m∑ r=1 n∑ i=1 I { ˜Ar,i } εiar ( σ ( w⊤ r xi ) −σ ( wr(0)⊤xi ) −Ir,i(0)(wr −wr(0))⊤xi ) ≤ m∑ r=1 n∑ i=1 I { ˜Ar,i } ·2R. 35Thus we can bound the Rademacher complexity as: RS ( FW(0),a R,B ) = 1 nEε∼{±1}n   sup W:∥W−W(0)∥2,∞≤R ∥W−W(0)∥F≤B n∑ i=1 εi m∑ r=1 ar√mσ ( w⊤ r x )   ≤1 nEε∼{±1}n   sup W:∥W−W(0)∥2,∞≤R ∥W−W(0)∥F≤B n∑ i=1 εi m∑ r=1 ar√mIr,i(0)w⊤ r xi  + 2R n√m m∑ r=1 n∑ i=1 I { ˜Ar,i } ≤1 nEε∼{±1}n [ sup W:∥W−W(0)∥F≤B n∑ i=1 εi m∑ r=1 ar√mIr,i(0)w⊤ r xi ] + 2R n√m m∑ r=1 n∑ i=1 I { ˜Ar,i } = 1 nEε∼{±1}n [ sup W:∥W−W(0)∥F≤B vec (W)⊤Z(0)ε ] + 2R n√m m∑ r=1 n∑ i=1 I { ˜Ar,i } = 1 nEε∼{±1}n [ sup W:∥W−W(0)∥F≤B vec (W −W(0))⊤Z(0)ε ] + 2R n√m m∑ r=1 n∑ i=1 I { ˜Ar,i } ≤1 nEε∼{±1}n [B·∥Z(0)ε∥2] + 2R n√m m∑ r=1 n∑ i=1 I { ˜Ar,i } ≤B n √ Eε∼{±1}n [ ∥Z(0)ε∥2 2 ] + 2R n√m m∑ r=1 n∑ i=1 I { ˜Ar,i } = B n ∥Z(0)∥F + 2R n√m m∑ r=1 n∑ i=1 I { ˜Ar,i } . Next we bound ∥Z(0)∥F and ∑m r=1 ∑n i=1 I { ˜Ar,i } . For ∥Z(0)∥F, notice that ∥Z(0)∥2 F = 1 m m∑ r=1 ( n∑ i=1 Ir,i(0) ) ≤n. Now observe the following lemma: Lemma C.18. With probability 1 −δ, if ⏐⏐wr(init)⊤xi ⏐⏐>R + Cns√ mδλ0S then I{˜Ar,i}= 0. Proof. From Corollary C.7 exists C > 0 s.t. with probability 1 −δ, for all r ∈ [m] : ∥wr(0) −wr(init)∥≤ Cns√ mδλ0S . From the triangle inequality: ⏐⏐wr(0)⊤xi ⏐⏐≥ wr(0)⊤xi  = wr(init)⊤xi −(wr(init) −wr(0))⊤xi  ≥ wr(init)⊤xi − (wr(init) −wr(0))⊤xi . Since ∥x∥= 1, and with the same probability above: (wr(init) −wr(0))⊤xi ≤ Cns√ mδλ0S , 36thus ⏐⏐wr(0)⊤xi ⏐⏐≥ wr(init)⊤xi − (wr(init) −wr(0))⊤xi  ≥ wr(init)⊤xi − Cns√ mδλ0S >R + Cns√ mδλ0S − Cns√ mδλ0S = R. □ For ∑m r=1 ∑n i=1 I { ˜Ar,i } , from Lemma C.18 we notice that m∑ r=1 n∑ i=1 I { ˜Ar,i } ≤ m∑ r=1 n∑ i=1 I{Ar,i}, for Ar,i being deﬁned as in Lemma C.9. Since all mneurons are independent at t= init and from Lemma C.9 and Corollary C.7 we know E[∑n i=1 I{Ar,i}] ≤ √ 2n(R+ Cns√ mδλ0S ) √πκ . Then by Hoeffding’s inequality, with probability at least 1 −δ/2 we have m∑ r=1 n∑ i=1 I { ˜Ar,i } ≤ m∑ r=1 n∑ i=1 I{Ar,i}≤ mn   √ 2(R+ Cns√ mδλ0S ) √πκ + √ log 2 δ 2m  . Therefore, with probability at least 1 −δ, the Rademacher complexity is bounded as: RS ( FW(0),a R,B ) ≤B n (√n ) + 2R n√mmn   √ 2(R+ Cns√ mδλ0S ) √πκ + √ log 2 δ 2m   = B√n + 2 √ 2R(R+ Cns√ mδλ0S )√m √πκ + R √ 2 log 2 δ, completing the proof of Lemma C.17. (Note that the high probability events used in the proof do not depend on the value of B, so the above bound holds simultaneously for every B.) □ C.4 Proof of Theorem 6.1 (Main Text) Proof of Theorem 6.1 (Main Text). First of all, from Assumption 3.1 we have λmin(H∞) ≥λ0. The rest of the proof is conditioned on this happening. We follow exactly the same steps as in [31] with minor changes. From Theorem C.10, Lemma C.16 and Lemma C.17, we know that for any sampleS, with probability at least 1 −δ/3 over the random initialization, the followings hold simultaneously: (i) Optimization succeeds (Theorem C.10): 1 2 ∥˜y −u(t)∥≤ ( 1 −ηλ0 2 )t ·∥˜y∥2 ≤1 2. This implies an upper bound on the training error L(X; Θ(t)) = 1 n ∑n i=1 ℓ(fW(t)(xi),yi) = 1 n ∑n i=1 ℓ(ui(t),yi): L(X; Θ(t)) = 1 n n∑ i=1 [ℓ(ui(t),yi) −ℓ(yi,yi)] ≤1 n n∑ i=1 |ui(t) −yi| ≤ 1√n∥u(t) −y∥2 = √ 21 2 ∥˜y −u(t)∥ n ≤ 1√n. 37(ii) ∥wr(t) −wr(0)∥2 ≤R(∀r ∈[m]) and ∥W(t) −W(0)∥F ≤B, where R = O (√n∥˜y∥2√mλ0 ) and B = √ ˜y⊤(H∞)−1 ˜y + poly ( ∥˜y∥2,n,ns, 1 λ0 , 1 λ0s ,1 δ ) m1/4κ1/2 . Note that B ≤O (√ n λ0 ) . (iii) Let Bi = i(i= 1,2,... ). Simultaneously for all i, the function class FW(0),a R,Bi has Rademacher complexity bounded as RS ( FW(0),a R,Bi ) ≤ Bi√n + 2R(R+ Cns√ mδλ0S )√m κ + R √ 2 log 10 δ . Let i∗be the smallest integer such that B ≤Bi∗. Then we have i∗≤O (√ n λ0 ) and Bi∗ ≤B+ 1. From above we know fW(t) ∈FW(0),a R,Bi∗ , and RS ( FW(0),a R,Bi∗ ) ≤B+ 1√n + 2R(R+ Cns√ mδλ0S )√m κ + R √ 2 log 10 δ = √ ˜y⊤(H∞)−1 ˜y √n + 1√n + poly ( ∥˜y∥2 ,n,n s, 1 λ0 , 1 λ0s ,1 δ ) m1/4κ1/2 + 2R(R+ Cns√ mδλ0S )√m κ + R √ 2 log 10 δ ≤ √ ˜y⊤(H∞)−1 ˜y n + 1√n + poly ( ∥˜y∥2 ,n,n s, 1 λ0 , 1 λ0s ,1 δ ) m1/4κ1/2 ≤ √ ˜y⊤(H∞)−1 ˜y n + 2√n. Next, from the theory of Rademacher complexity and a union bound over a ﬁnite set of different i’s, for any random initialization (W(init),a), with probability at least 1 −δ/3 over the sample S, we have sup f∈FW(0),a R,Bi {R(f) −L(f)}≤ 2RS ( FW(0),a R,Bi ) + O   √ log n λ0δ n  , ∀i∈ { 1,2,...,O (√n λ0 )} . Finally, taking a union bound, we know that with probability at least 1 −2 3 δover the sample Sand the random initialization (W(init),a), the followings are all satisﬁed (for some i∗): L(X,Θ(t)) ≤ 1√n, f(·,Θ(t)) ∈FW(0),a R,Bi∗ , RS ( FW(0),a R,Bi∗ ) ≤ √ ˜y⊤(H∞)−1 ˜y n + 2√n, sup f∈FW(0),a R,Bi∗ {R(f) −L(f)}≤ 2RS ( FW(0),a R,Bi∗ ) + O   √ log n λ0δ n  . These together can imply: R(Θ(t)) ≤ 1√n + 2RS ( FW(0),a R,Bi∗ ) + O   √ log n λ0δ n   ≤ 1√n + 2   √ ˜y⊤(H∞)−1 ˜y n + 2√n  + O   √ log n λ0δ n   = 2 √ ˜y⊤(H∞)−1 ˜y n + O   √ log n λ0δ n  . This completes the proof. □ 38C.5 Linear teachers: Proof of corollary 6.3 We now consider the case where gS(x) = x⊤θS, g T(x) = x⊤θT, which is the case in Corollary 6.3. We will start with stating the random initialization population risk bound for this case, which we will compare our result to: Corollary C.19 (Population risk bound for random initialization from [31]). Assume that the random initialized model with weights Θ(t) was trained according to Theorem 5.1 from [ 31] and that y = XθT, then with probability 1 −δ R(Θ(t)) ≤3 √ 2 ∥θT∥2√n + O   √ log n λ0δ n  . (60) This corollary is a direct result of plugging y = XθT into Corollary 6.2 from [31], and plugging the result into Theorem 5.1 from [31]. As discussed in Section 6.1, we will assume that f(X; Θ(0)) = XθS. Since our model is non-linear, this assumption is not trivial, and requires some clariﬁcation. For inﬁnite width, Lemma 1 from [46] tells us that nS = 2dcan sufﬁce to achieve this, if the samples are chosen according to some conditions. For the case of ﬁnite width m, like is assumed in Theorem 6.1, no such equivalent exist. However, we can use Corollary C.19 for the pretraining, and achieve anϵbound on the pretraining population risk, for sufﬁciently large nS = Ω ( ∥θS∥2 ϵ2 ) . Then, approximate relaxations can be derived when we assume the two functions are ϵclose (i.e. f(x,Θ(0)) = x⊤θS + ϵ). We now restate our two corollaries from the main text: Corollary 6.2 (Main Text). Suppose that gS(X) ≜X⊤θS, gT(X) ≜X⊤θT, and assume Assump- tion 3.2 holds. Then, √ ˜y⊤(H∞)−1 ˜y≤3 ∥θT −θS∥2 . This is a direct corollary of Theorem 6.1 from [31] on ˜y deﬁned above. Corollary 6.3 (Main Text). Under the conditions of Theorem 6.1 and Corollary 6.2, it holds that R(Θ(t)) ≤6 ∥θT −θS∥2√n + O   √ log n λ0δ n  . Comparing this to Corollary C.19 gives us the exact condition for when it is better to use ﬁne-tuning instead of random initialization, which is ∥θT −θS∥< ∥θT∥√ 2 . We will now provide a proof for this results: Proof of Corollary 6.3. In order to achieve this bound, we use the assumption onf(X; Θ(0)), which gives us: ˜y = XθT −XθS = X(θT −θS). Hence, we can treat ˜y as if it was created by a linear label generation function θT −θS. Hence, by using Theorem 6.1 from [31] we can bound √ ˜y(H∞)−1˜y ≤3 ∥θT −θS∥. Plugging this into Theorem 6.1 concludes the proof. □ 39",
      "meta_data": {
        "arxiv_id": "2107.01641v2",
        "authors": [
          "Gal Shachaf",
          "Alon Brutzkus",
          "Amir Globerson"
        ],
        "published_date": "2021-07-04T14:15:50Z",
        "pdf_url": "https://arxiv.org/pdf/2107.01641v2.pdf"
      }
    },
    {
      "title": "Economic Hyperparameter Optimization With Blended Search Strategy"
    },
    {
      "title": "Practical Differentially Private Hyperparameter Tuning with Subsampling",
      "abstract": "Tuning the hyperparameters of differentially private (DP) machine learning\n(ML) algorithms often requires use of sensitive data and this may leak private\ninformation via hyperparameter values. Recently, Papernot and Steinke (2022)\nproposed a certain class of DP hyperparameter tuning algorithms, where the\nnumber of random search samples is randomized itself. Commonly, these\nalgorithms still considerably increase the DP privacy parameter $\\varepsilon$\nover non-tuned DP ML model training and can be computationally heavy as\nevaluating each hyperparameter candidate requires a new training run. We focus\non lowering both the DP bounds and the computational cost of these methods by\nusing only a random subset of the sensitive data for the hyperparameter tuning\nand by extrapolating the optimal values to a larger dataset. We provide a\nR\\'enyi differential privacy analysis for the proposed method and\nexperimentally show that it consistently leads to better privacy-utility\ntrade-off than the baseline method by Papernot and Steinke.",
      "full_text": "Practical Differentially Private Hyperparameter Tuning with Subsampling Antti Koskela and Tejas Kulkarni Nokia Bell Labs Espoo, Finland Abstract Tuning the hyperparameters of differentially private (DP) machine learning (ML) algorithms of- ten requires use of sensitive data and this may leak private information via hyperparameter values. Recently, Papernot and Steinke (2022) proposed a certain class of DP hyperparameter tuning algo- rithms, where the number of random search samples is randomized. Commonly, these algorithms still considerably increase the DP privacy parameterε over non-tuned DP ML model training and can be computationally heavy as evaluating each hyperparameter candidate requires a new training run. We focus on lowering both the DP bounds and the compute cost of these methods by using only a random subset of the sensitive data for the hyperparameter tuning and by extrapolating the optimal values to a larger dataset. We provide a Rényi differential privacy analysis for the proposed method and experimentally show that it consistently leads to better privacy-utility trade-off than the baseline method by Papernot and Steinke. 1 Introduction Our aim is two-fold: to decrease the computational cost as well as the privacy cost of hyperparameter tuning of DP ML models. The reasons for this are clear. As the dataset sizes grow and models get more complex, blackbox optimization of hyperparameters becomes more expensive since evaluation of a single set of hyperparameters often requires retraining a new model. On the other hand, tuning the hyperpa- rameters often depends on the use of sensitive data, so it requires privacy protection as well, as illustrated by the example by Papernot and Steinke (2022). Intuitively, the leakage from hyperparameters is much smaller than from the model parameters, however, providing tuning algorithms with low additional DP cost has turned out challenging. Current best algorithms (Papernot and Steinke, 2022) still come with a considerable DP cost overhead. Although our methods and results are applicable to general DP mechanisms, we focus in particular on tuning of the DP stochastic gradient descent (DP-SGD) (Song et al., 2013; Bassily et al., 2014; Abadi et al., 2016) which has become the most widely used method to train ML models with DP guarantees. Compared to plain SGD, DP brings additional hyperparameters to tune: the noise levelσ and the clipping constant C. Additionally, also the subsampling ratioγ affects the DP guarantees, as well as length of the training. Tuning all the hyperparameters of DP-SGD commonly requires use of sensitive data. We use the results by Papernot and Steinke (2022) as building blocks of our methods. Their work was based on the analysis of Liu and Talwar (2019) who provided the first results for DP black-box 1 arXiv:2301.11989v3  [cs.LG]  13 Feb 2024optimization of hyperparameters, where, if the base training algorithm is(ε, 0)-DP, then the tuned model is approximately (3ε, 0)-DP. Papernot and Steinke (2022) provided a Rényi differential privacy (RDP) analysis for a class of black-box tuning algorithms, where the number of runs in the hyperparameter tuning is randomized. As the privacy bounds are in terms of RDP and assume only RDP bounds about the candidate model training algorithms, they are particularly suitable to tuning DP-SGD. However, still, running these algorithms increase theε-values two or three-fold or more, and they can be computationally heavy as evaluating each candidate model requires training a new model. Our novelty is to consider using only a random subset of the sensitive data for the tuning part and use the output hyperparameter values (and potentially the model) for training subsequent models. Using a random subset for the privacy and computation costly part automatically leads to both lower DP privacy leakage as well as computational cost. We also consider ways to appropriately extrapolate the optimal value from the small subset of data to a larger dataset. The RDP bounds for the DP tuning methods by Papernot and Steinke (2022) assume that the RDP- values of the candidate model training algorithms are fixed. We also consider ways to use these bounds for tuning hyperparameters that affect the RDP-values of the base algorithm, being the noise levelσ, the subsampling ratioγ and the length of training in case of DP-SGD. 1.1 Related Work on Hyperparameter Tuning Chaudhuri and Vinterbo (2013) were the first ones to focus on DP bounds for hyperparameter tuning. An improvement was made by Liu and Talwar (2019) who considered black-box tuning of(ε, δ)-DP mechanisms. Mohapatra et al. (2022) showed that for reasonable numbers of adaptively chosen private candidates a naive RDP accounting (i.e., RDP parameters grow linearly w.r.t. the number of model evaluations) often leads to lower DP bounds than the methods by Liu and Talwar (2019). Papernot and Steinke(2022)gaveRDPboundsforblack-boxtuningalgorithmsthatgrowonlylogarithmicallyw.r.t. the number of model evaluations. In a non-DP setting, hyperparameter tuning with random subsamples has been considered for SVMs (Horváth et al., 2017) and for large datasets in healthcare (Waring et al., 2020). SmallrandomsubsetsofdatahavebeenusedinBayesianoptimizationofhyperparameters(Swerskyetal., 2013; Klein et al., 2017). Recent works (Killamsetty et al., 2021, 2022) consider using subsets of data for hyperparameter tuning of deep learning models. 1.2 Our Contributions • We propose a subsampling strategy to lower the privacy cost and computational cost of DP hyper- parameter tuning. We provide a tailored RDP analysis for the proposed strategy. Our analysis is in terms of RDP and we use existing results for tuning Papernot and Steinke (2022) and DP-SGD (Zhu and Wang, 2019) as building blocks. • We propose algorithms to tune hyperparameters that affect the RDP guarantees of the base model training algorithms. We provide a rigorous RDP analysis for these algorithms. • We carry out experiments on several standard datasets, where we are able to improve upon the baseline tuning method by a clear margin. While our experiments focus mainly on training of deep learning models with DP-SGD and DP-Adam, our framework is currently applicable to any computation that involves selecting the best among several alternatives (consider e.g., DP model selection, Thakurta and Smith, 2013). 22 Background: DP, DP-SGD and DP Hyperparameter Tuning We first give the basic definitions. An input dataset containing n data points is denoted as X = {x1, . . . , xn}. Denote the set of all possible datasets byX. We sayX and Y are neighbors if we get one by adding or removing one data element to or from the other (denotedX ∼ Y ). Consider a randomized mechanism M : X → O, whereO denotes the output space. The(ε, δ)-definition of DP can be given as follows (Dwork, 2006). Definition 1. Let ε >0 and δ ∈ [0, 1]. We say that a mechanismM is (ε, δ)-DP, if for all neighboring datasets X and Y and for every measurable setE ⊂ Owe have: Pr(M(X) ∈ E) ≤ eεPr(M(Y ) ∈ E) + δ. We will also use the Rényi differential privacy (RDP) (Mironov, 2017) which is defined as follows. Rényi divergence of orderα >1 between two distributionsP and Q is defined as Dα(P||Q) = 1 α − 1 log Z \u0012P(t) Q(t) \u0013α Q(t) dt. (2.1) Definition 2. We say that a mechanismM is (α, ε)-RDP, if for all neighboring datasetsX and Y , the output distributions ofM(X) and M(Y ) have Rényi divergence of orderα at mostε, i.e., max X∼Y Dα \u0000 M(X)||M(Y ) \u0001 ≤ ε. We can convert from Rényi DP to approximate DP using, for example, the following formula: Lemma3 (Canonneetal.2020) . Suppose the mechanismM is \u0000 α, ε′\u0001 -RDP. ThenM is also(ε, δ(ε))-DP for arbitraryε ≥ 0 with δ(ε) = exp \u0000 (α − 1)(ε′ − ε) \u0001 α \u0012 1 − 1 α \u0013α−1 . (2.2) As is common in practice, we carry out the RDP accounting such that we do bookkeeping of total ε(α)-values for a list of RDP-orders (e.g. integer α’s) and in the end convert to(ε, δ)-guarantees by minimizing over the values given by Equation(2.2) w.r.t.α. RDP accounting for compositions of DP mechanisms is carried using standard RDP composition results (Mironov, 2017). DP-SGD differs from SGD such that sample-wise gradients of a random mini-batch are clipped to have anL2-norm at mostC and normally distributed noise with varianceσ2 is added to the sum of the gradients of the mini-batch (Abadi et al., 2016). One iteration is given by θj+1 = θj − ηj \u0010 1 |B| X x∈Bj clip(∇f(x, θj), C) + Zj \u0011 , (2.3) where the noiseZj ∼ N(0, C2σ2 |B|2 Id), f denotes the loss function,θ the model parameters,ηj the learning rate hyperparameter at iterationj and |B| is the expected batch size (if we carry out Poisson subsampling of mini-batches,|Bj| varies). There are several results that enable the RDP analysis of DP-SGD iterations (Abadi et al., 2016; Balle et al., 2018; Zhu and Wang, 2019). The following result by Zhu and Wang (2019) is directly applicable to analyzing DP-SGD, however, we also use it for analyzing a variant of our hyperparameter tuning method. 3Theorem 4(Zhu and Wang 2019). SupposeM is a \u0000 α, ε(α) \u0001 -RDP mechanism, w.r.t. to the add/remove neighbourhood relation. Consider the subsampled mechanism(M◦ subsamplePoisson(γ))(X), wheresubsamplePoisson(γ) denotes Poisson subsampling with sampling ratio γ. If M is \u0000 α, ε(α) \u0001 -RDP then M ◦subsamplePoisson(γ) is\u0000 α, ε′(α) \u0001 -RDP (α ≥ 2 is an integer), where ε′(α) = 1 α − 1 log \u0012 (1 − γ)α−1(αγ − γ + 1) + \u0012α 2 \u0013 γ2(1 − γ)α−2 exp(ε(2)) + 3 Xα j=3 \u0012α j \u0013 γj(1 − γ)α−j exp((j − 1)ε(j)) \u0013 . We remark that the recent works (Koskela et al., 2020; Gopi et al., 2021; Zhu et al., 2022) give methods to carry out(ε, δ)-analysis of DP-SGD tightly. As the state-of-the-art bounds for hyperparameter tuning methods are RDP bounds (Papernot and Steinke, 2022), for simplicity, we will also analyze DP-SGD using RDP. Intuitively, the leakage from hyperparameters is much smaller than from the model parameters, how- ever, considering it in the final accounting is needed to ensure rigorous DP guarantees. Currently the most practical (ε, δ)-guarantees for DP hyperparameter tuning algorithms are those of (Papernot and Steinke, 2022). In the results of Papernot and Steinke (2022) it is important that the number of runsK with the hyperparameter tuning is randomized. They analyze various distributions for drawingK, how- ever, we focus on using the Poisson distribution as it is the most concentrated around the mean among all the alternatives. The corresponding hyperparameter tuning algorithm and its privacy guarantees are given by Thm.5. First recall: K is distributed according to a Poisson distribution with meanµ > 0, if for all non- negative integer valuesk: P(K = k) = e−µ · µk k! . Theorem 5 (Papernot and Steinke 2022). Let Q : XN → Ybe a randomized algorithm satisfying\u0000 α, ε(α) \u0001 -RDP and (bε, bδ)-DP for some α ∈ (1, ∞) and ε, bε, bδ ≥ 0. Assume Y is totally ordered. Let the Poisson distribution parameterµ > 0. Define the hyperparameter tuning algorithmA : XN → Y as follows. Draw K from a Poisson distribution with meanµ. Run Q(X) for K times. Then A(X) returns the best value of thoseK runs (both the hyperparameters and the model parameters). IfK = 0, A(X) returns some arbitrary output. Ifebε ≤ 1 + 1 α−1 , then A satisfies \u0000 α, ε′(α) \u0001 -RDP, whereε′(α) = ε(α) + µ · bδ + log µ α−1 . 3 DP Hyperparameter Tuning with a Random Subset We next consider our main tool: we carry out the private hyperparameter tuning on a random subset, and if needed, extrapolate the found hyperparameter values to larger datasets that we use for training subsequent models. In our approach the subset of data used for tuning is generally smaller than the data used for training the final model and thus we extrapolate the hyperparameter values. 3.1 Our Method: Small Random Subset for Tuning Our method works as below: 41. Use Poisson subsampling to drawX1 ⊂ X: draw a random subsetX1 such that eachx ∈ X is included inX1 with probabilityq. 2. Compute (θ1, t1) = Mtune(X1), where Mtune is a hyperparameter tuning algorithm (e.g., the method by Papernot and Steinke, 2022) that outputs the vector of optimal hyperparameterst1 and the corresponding model parametersθ1. 3. If needed, extrapolate the hyperparameterst1 to the datasetX \\ X1: t1 → t2. 4. Compute θ2 = Mbase(θ1, t2, X\\ X1), whereMbase is the base mechanism (e.g., DP-SGD). Denote the whole mechanism byM. Then, we may write M(X) = \u0000 Mtune(X1), Mbase \u0000 Mtune(X1), X\\ X1 \u0001\u0001 , (3.1) where X1 ∼ subsamplePoisson(q)(X). Additionally, we consider a variation of our method in which we use the full datasetX instead ofX \\ X1 from step 3 onwards, i.e., the mechanism M(X) = \u0000 Mtune(X1), Mbase \u0000 Mtune(X1), X \u0001\u0001 , (3.2) where X1 ∼ subsamplePoisson(q)(X). We call these methods variant 1 and variant 2, respectively. The RDP bounds for the variant 2 can be obtained with a standard subsampling and composition result (e.g., Thm 4). We provide a tailored privacy analysis of the variant 1 in Section 3.3. 3.2 Extrapolating the DP-SGD Hyperparameters We use simple heuristics to transfer the optimal hyperparameter values found for the small subset of data to a larger dataset. The clipping constantC, the noise levelσ, the subsampling ratioγ and the total number of iterationsT are kept constant in this transfer. As a consequence the(ε, δ)-privacy guarantees are also the same for the models trained with the smaller and the larger dataset. For scaling the learning rate, we use the heuristics used by van der Veen et al. (2018): we scale the learning rateη with the dataset size. I.e., if we carry out the hyperparameter tuning using a subset of sizem and find an optimal value η∗, we multiplyη∗ by n/m when transferring to the dataset of sizen. This can be also heuristically motivated as follows. ConsiderT iterations of the DP-SGD (2.3). With the above rules, the distribution of the noise that gets injected into the model trained with dataset of size n is TX j=1 Zj ∼ N   0, T · \u0000 n m η∗\u00012 σ2C2 (γ · n)2 Id ! ∼ N \u0012 0, T · η∗2σ2C2 (γ · m)2 Id \u0013 which is exactly the distribution of the noise that was added to the model trained with the subsample of sizem. This principle of keeping the noise constant when scaling the hyperparameters was also used by Sander et al. (2022). We arrive at our scaling rule also by taking a variational Bayesian view of DP-SGD. Mandt et al. (2017) model the stochasticity of the SGD mini-batch gradients in a region approximated by a constant quadratic convex loss by invoking the central limit theorem, and arrive at a continuous-time multivariate Ornstein-Uhlenbeck (OU) process for which the discrete approximation is given by ∆θ = −ηg(θ) + ηp |B| L∆W, ∆W ∼ N(0, Id), (3.3) 5where |B| denotes the batch size of the SGD approximation,g(θ) the full gradient andLLT the covari- ance matrix of the SGD noise. By minimizing the Kullback–Leibler divergence between the stationary distribution of this OU-process and the Gaussian posterior distributionf(θ) ∝ exp \u0000 − n · L(θ) \u0001 , where L(θ) denotes the quadratic loss function andn is the size of the dataset, they arrive at the expression η∗ = 2|B| n d Tr(LLT ) for the optimal learning rate value (see Thm. 1, Mandt et al., 2017). We consider the case where the additive DP noise dominates the SGD noise, and instead of the update (3.3) consider the update ∆θ = −ηg(θ) + η · σ · C |B| ∆W, ∆W ∼ N(0, Id) (3.4) which equals the DP-SGD update (2.3) with the mini-batch gradient replaced by the full gradient. Essen- tially the difference between (3.3) and (3.4) is p |B| replaced by|B|, and by the reasoning used in (Thm. 1, Mandt et al., 2017), we see that the learning rate value that minimizes the KL divergence between the approximate posterior and the Gaussian posteriorf(θ) is then given by η∗ = 2|B|2 n d Tr(σ2C2I) = 2 |B|2 n · σ2C2 = 2 γ2n σ2C2 . (3.5) The scaling rule (3.5) also indicates that the optimal value of the learning rate should be scaled linearly with the size of the dataset in caseγ, σ and C are kept constant. Training of certain models benefits from use of adaptive optimizers such as Adam (Kingma and Ba, 2014) or RMSProp, e.g., due to sparse gradients. Then the above extrapolation rules for DP-SGD are not necessarily meaningful anymore. In our experiments, when training a neural network classifier using Adam with DP-SGD gradients, we found that keeping the value of the learning rate fixed in the transfer to the larger dataset lead to better results than increasing it as in case of DP-SGD. We mention that there are principled ways of extrapolating the hyperparameters in non-DP setting such as those of Klein et al. (2017). 3.3 Privacy Analysis The RDP analysis of the variant 2 given in Equation (3.2) is straightforward. Since the tuning setX1 is sampled with Poisson subsampling with subsampling ratioq, we may write the mechanism as an adaptive composition M(X) = \u0010 fMtune(X), Mbase \u0000 fMtune(X), X \u0011 , where fMtune(X) = (Mtune ◦ subsamplePoisson(q))(X). Using the RDP values given by Thm. 5 forMtune and the subsampling amplification result of Thm. 4, we obtain RDP bounds forfMtune(X). Using RDP bounds for Mbase (e.g., DP-SGD) and RDP composition results, we further get RDP bounds for the mechanism M given in (3.2). Tailored RDP-Analysis.When we use the variant (3.1), i.e., we only use the rest of the dataX\\X1 for Mbase, we can get even tighter RDP bounds. The following theorem gives tailored RDP bounds for the mechanism (3.1). Similarly to the analysis by Zhu and Wang (2019) for the Poisson subsampled Gaussian mechanism, we obtain RDP bounds using 6the RDP bounds of the non-subsampled mechanisms and by using binomial expansions (the proof is given in Appendix C). Theorem 6.LetM be the mechanism(3.1), such that the subsetX1 is Poisson sampled with subsampling ratio q, 0 ≤ q ≤ 1 and letα >1. Denote byεtune(α) and εbase(α) the RDP-values of mechanismsMtune and Mbase, respectively. Then,M is \u0000 α, ε(α) \u0001 -RDP for ε(α) = max{ε1(α), ε2(α)}, where ε1(α) = 1 α − 1 log \u0012 qα · exp \u0000 (α − 1)εtune(α) \u0001 + (1 − q)α · exp \u0000 (α − 1)εbase(α) \u0001 + α−1X j=1 \u0012α j \u0013 · qα−j · (1 − q)j · exp \u0000 (α − j − 1)εtune(α − j) \u0001 exp \u0000 (j − 1)εbase(j) \u0001\u0013 (3.6) and ε2(α) = 1 α − 1 log \u0012 (1 − q)α−1 · exp \u0000 (α − 1)εbase(α) \u0001 + α−1X j=1 \u0012α − 1 j \u0013 · qj · (1 − q)α−1−j · exp \u0000 j · εtune(j + 1) \u0001 · exp \u0000 (α − j − 1)εbase(α − j) \u0001\u0013 . (3.7) Remark 7. The RDP bound given by Thm. 6 is optimal in a sense that it approachesεtune(α) and εbase(α) as q → 1 and q → 0, respectively. Remark 8. We can initialize the subsequent model trainingMbase using the modelθ1. This adaptivity is included in all the RDP analyses of both mechanisms(3.1) and (3.2). Notice that in the bounds (3.6) and (3.7) the RDP parameter of the tuning algorithm,εtune(α), is weighted with the parameterq and the RDP parameter of the base algorithm,εbase(α), is weighted with 1 − q. This lowers the overall privacy cost in case the tuning set is chosen small enough. Figure 1 illustrates how the(ε, δ)-bounds of the two variants (3.1) and (3.2) behave as functions of the sampling parameterq used for sampling the tuning setX1, when the base mechanism DP-SGD is run for 50 epochs with the subsampling ratioγ = 0.01 and noise levelσ = 2.0. The bounds for the variant 1 given in Equation (3.1) are computed using the RDP results of Thm. 6 and the bounds for the variant 2 are computed using the subsampling amplification result of Thm. 4. The RDP bounds are converted to (ε, δ)-bounds using the conversion rule of Lemma 3 withδ = 10−5. The fact that the bounds for the variants 1 and 2 cross whenµ = 45 at small values ofq suggests that the bounds of Thm. 6 could still be tightened. 3.4 Computational Savings Our scaling approach for DP-SGD described in Section 3.2 implies that the DP-SGD subsampling ratio γ, the noise levelσ and the number of iterationsT are the same when evaluating the private candidate models using the tuning setX1 and when evaluating the final model using the larger dataset. Thus, if we run the base algorithm forE epochs, we easily see that the expected number of required gradient evaluations for the variant 1 given in (3.1) is given byE ·(µ ·q ·n + (1−q) ·n) and for the variant 2 given 7Figure 1: Comparison of(ε, δ)-bounds for the variant 1 given in Equation (3.1) and the variant 2 given in Equation (3.2) as a function of the subsampling ratioq used for sampling the tuning setX1. Also shown is the(ε, δ)-bound for the baseline algorithm described in Thm. 5. Hereµ refers to the expected number of model evaluations in the tuning algorithm. in (3.2) it is given byE ·(µ ·q ·n + n), whereas the baseline requires in expectationµ ·n ·E evaluations in case it is carrying out tuning with the same hyperparameter candidates as our method. Since the number of iterationsT is kept fixed, there are some constant overheads in the compute cost such as those coming from the model updates. Therefore the actual speed ups are slightly smaller. For example, in our experiments withµ = 15 and q = 0.1, the baseline requires µ µ·q+1 ≈ 6 times more gradient evaluations than our method and whenµ = 45 and q = 0.1 the baseline requires≈ 8 times more gradient evaluations. The actual speed ups are shown in the figures of Section 5. 4 Dealing with DP-SGD Hyperparameters that Affect the DP Guarantees Thm. 5 gives RDP-parameters of orderα for the tuning algorithm, assuming the underlying candidate picking algorithm is \u0000 α, ε(α) \u0001 -RDP. In case of DP-SGD, if we are tuning the learning rateη or clipping constant C, and fix rest of the hyperparameters, these \u0000 α, ε(α) \u0001 -RDP bounds are fixed for all the hy- perparameter candidates. However, if we are tuning hyperparameters that affect the DP guarantees, i.e., the subsampling ratioγ, the noise levelσ or the length of the trainingT, it is less straightforward to determine suitable uniformε(α)-upper bounds. As is common practice, we consider a gridΛ of α-orders for RDP bookkeeping (e.g. integer values ofα’s). 4.1 Grid Search with Randomization To deal with this problem, we first set an approximative DP target value(ε, δ) that we use to adjust some of the hyperparameters. For example, if we are tuning the subsampling ratioγ and training length 8T, we can, for each choice of(γ, T), adjust the noise scaleσ so that the resulting training iteration is at most (ε, δ)-DP. Vice versa, we may tuneγ and σ, and take maximal value ofT such that the resulting training iteration is at most(ε, δ)-DP. More specifically, we first fixε, δ >0 which represent the target approximative DP bound for each can- didate model. Denote byε(T, δ, γ, σ) the ε-value of the subsampled Gaussian mechanism with parameter values γ, σand T and for fixedδ. To each pair of(γ, T), we attach a noise scaleσγ,T such that it is the smallest number with which the resulting composition is(ε, δ)-DP: σγ,T = min{σ ∈ R+ : ε(T, δ, γ, σ) ≤ ε}. As the RDP values increase monotonously w.r.t. the number of compositions, it is straightforward to find σγ,T , e.g., using the bisection method. Alternatively, we could fix a few values ofσ, and to each combination of(γ, σ), attach the largestT (denoted Tγ,σ ) such that the target(ε, δ)-guarantee holds. We consider a finite gridΓ of possible hyperparameter valuest (e.g., t = (γ, σ, T), whereT is adjusted to γ and σ). Then, for allt ∈ Γ, we compute the corresponding RDP valueεt(α) for eachα ∈ Λ. Finally, for eachα ∈ Λ, we set ε(α) = max t∈Γ εt(α). Then, since for each random draw oft, the DP-SGD trained candidate model isε(α)-RDP, by Lemma 9 givenbelow, thecandidatepickingalgorithm Q isalso ε(α)-RDP.Thisapproachisusedintheexperiments of Figure 4, where we jointly tuneT, γ and η. 4.2 RDP Analysis For completeness, in Appendix D we prove the following result which gives RDP bounds for the case we randomly draw hyperparemeters that affect the privacy guarantees of the candidate models. Lemma 9. Denote by β the random variable of which outcomes are the hyperparameter candidates (drawing either randomly from a grid or from given distributions). Consider an algorithmQ, that first randomly picks hyperparameterst ∼ β, then runs a randomized mechanismM(t, X). Suppose M(t, X) is \u0000 α, ε(α) \u0001 -RDP for allt. Then, Q is \u0000 α, ε(α) \u0001 -RDP. 5 Experimental Results We perform our evaluations on standard benchmark datasets for classification: CIFAR-10 (Krizhevsky and Hinton, 2009), MNIST (LeCun et al., 1998), FashionMNIST (Xiao et al., 2017) and IMDB (Maas et al., 2011). Full details of the experiments are given in Appendix A. When reporting the results, we set δ = 10−5 in all experiments. Learning rate tuning.Figures 2 and 3 summarize the results for learning rate tuning using our meth- ods and the baseline method by Papernot and Steinke (2022). The learning rate grid size is either 9 or 10 and the grid values are specified in Table 2 (Appendix). We fix the subsampling ratioγ and the number of epochs to the values given in Table 1 (Appendix) for all models. The batch sizes in the submodels are defined by scalingγ on the corresponding dataset sizes. We useq = 0.1 which means that, for example, if the Poisson subsampling of DP-SGD gives in expectation a batch size of 128 in the tuning phase of our 9(a) trained with DP-SGD  (b) trained with DP-SGD (c) trained with DP-SGD  (d) trained with DP-SGD Figure 2: Tuning learning rate with DP-SGD. Test accuracies are averaged across 10 independent runs and the error bars denote the standard error of the mean. The numbers in the legends refer to the mean training timings of the baseline scaled with respect to minimum of variant 1 and 2. For example, for CIFAR-10, the average training time for the baseline method is 6.06 times bigger than for the fastest of our methods. For perspective, we also add curves showing the privacy cost of training a single model with optimal hyperparameters obtained from the baseline. methods, the expected batch sizes for the final models of variant 1 and 2 are 1152 and 1280, respectively. We useµ = 15 (the expected number of runs for the tuning algorithm). Tuning all hyperparameters.Next, we jointly optimize the number of epochs, the DP-SGD subsam- 10(a) trained with DP-Adam  (b) trained with DP-Adam (c) trained with DP-Adam  (d) trained with DP-Adam Figure 3: Tuning learning rate with DP-Adam. Test accuracies are averaged across 10 independent runs and the error bars denote the standard error of the mean. The numbers in the legends refer to the mean training timings of the baseline scaled with respect to minimum of variant 1 and 2. For example, for FashionMNIST, the average training time for the baseline method is 9.03 times bigger than the fastest of our methods. For perspective, we also add curves showing the privacy cost of training a single model with optimal hyperparameters obtained from the baseline. Figure 6 (Appendix) shows a more detailed version of this plot. pling ratioγ, and the learning rateη using the hyperparameter candidates given in Table 2 (Appendix). The remaining setup is the same as in the previous experiment. Figure 4 shows the same quantities as Figures 2 and 3, however, higher values ofµ are used to accommodate to increased hyperparameter 11(a) trained with DP-SGD  (b) trained with DP-SGD (c) trained with DP-SGD  (d) trained with DP-SGD Figure 4: Tuning of subsampling ratio, training epochs, and learning rate with DP-SGD. Test accuracies are averaged across 10 independent runs and the error bars denote the standard error of the mean. The numbers in the legends refer to the mean timings of the baseline method scaled with respect to the minimum of variant 1 and 2. For perspective, we also add curves showing the privacy cost of training a single model with optimal hyperparameters obtained from the baseline. Figure 7 (Appendix) shows a more detailed version of this plot. spaces. Takeaways. Overall, for both DP-SGD and DP-Adam, we observe that both variants of our method provide better privacy-utility trade-off and have a lower computational cost than the baseline method. 12Additionally, we also note the benefits of tailored analysis (Thm 6) in Figure 2 in terms of slightly higher accuracy for variant 1 compared to variant 2 for DP-SGD. In Figure 4 where we are also tuning the batch size and epochs, the noise levels are higher compared to Figure 2. One reason for slightly better privacy-utility trade-off for variant 2 with DP-SGD is possibly the fact that our tailored bound is less tight for higher values ofµ and small values ofq (see Figure 1). 6 Discussion We have considered a simple strategy for lowering the privacy cost and computational cost of DP hyper- parameter tuning: we carry out tuning using a random subset of data and extrapolate the optimal values to the larger training set used to train the final model. We have also provided methods to tune the hy- perparameters that affect the DP guarantees of the model training themselves, those being the noise level and subsampling ratio in case of DP-SGD. Our experiments show a clear improvement over the baseline method by Papernot and Steinke (2022) when tuning DP-SGD for neural networks and using simple but well-justified heuristics for the extrapolation. One obvious limitation of our method is that it is limited to DP-SGD although we show also positive results for the Adam optimizer combined with DP-SGD gradients. Finding out whether effective scaling rules could be derived for DP-SGD with momentum, DP-FTRL (Kairouz et al., 2021) or more refined adaptive DP optimizers such as DP2-RMSprop (Li et al., 2023), is left for future work. An interesting avenue of future work is also to find more black-box type of extrapolation methods, something that has been considered in the non-DP case (see, e.g. Klein et al., 2017). Another interesting question is how to carry out more accurate privacy analysis using the so-called privacy loss distributions (PLDs) and numerical accounting. The main reason for using RDP instead of PLD accounting in this work was that the tightest privacy bounds for DP hyperparameter tuning are given in terms of RDP. We have used Poisson subsampling to obtain the tuning set as it is relatively easy to analyze, however, other sampling methods could be analyzed as well. Acknowledgments We would like to thank our colleague Laith Zumot for discussions about hyperparameter tuning methods at the early stages of the project. Bibliography Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., and Zhang, L. (2016). Deep learning with differential privacy. InProceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pages 308–318. Balle, B., Barthe, G., and Gaboardi, M. (2018). Privacy amplification by subsampling: Tight analyses via couplings and divergences. InAdvances in Neural Information Processing Systems, volume 31. Bassily, R., Smith, A., and Thakurta, A. (2014). Private empirical risk minimization: Efficient algorithms and tight error bounds. In 2014 IEEE 55th annual symposium on foundations of computer science, pages 464–473. IEEE. Bun, M. and Steinke, T. (2016). Concentrated differential privacy: Simplifications, extensions, and lower bounds. In Theory of Cryptography Conference, pages 635–658. Springer. 13Canonne, C., Kamath, G., and Steinke, T. (2020). The discrete Gaussian for differential privacy. In Advances in Neural Information Processing Systems, volume 33. Chaudhuri, K. and Vinterbo, S. A. (2013). A stability-based validation procedure for differentially private machine learning.Advances in Neural Information Processing Systems, 26. Dong, J., Roth, A., Su, W. J., et al. (2022). Gaussian differential privacy.Journal of the Royal Statistical Society Series B, 84(1):3–37. Dwork, C. (2006). Differential privacy. In Proc. 33rd Int. Colloq. on Automata, Languages and Prog. (ICALP 2006), Part II, pages 1–12. Gopi, S., Lee, Y. T., and Wutschitz, L. (2021). Numerical composition of differential privacy.Advances in Neural Information Processing Systems, 34:11631–11642. Horváth, T., Mantovani, R. G., and de Carvalho, A. C. (2017). Effects of random sampling on svm hyper-parameter tuning. InInternational Conference on Intelligent Systems Design and Applications, pages 268–278. Springer. Kairouz, P., McMahan, B., Song, S., Thakkar, O., Thakurta, A., and Xu, Z. (2021). Practical and private (deep) learning without sampling or shuffling. InInternational Conference on Machine Learning, pages 5213–5225. PMLR. Killamsetty, K., Abhishek, G. S., Lnu, A., Ramakrishnan, G., Evfimievski, A., Popa, L., and Iyer, R. (2022). Automata: Gradient based data subset selection for compute-efficient hyper-parameter tuning. Advances in Neural Information Processing Systems, 35:28721–28733. Killamsetty, K., Durga, S., Ramakrishnan, G., De, A., and Iyer, R. (2021). Grad-match: Gradient matching based data subset selection for efficient deep model training. InInternational Conference on Machine Learning, pages 5464–5474. PMLR. Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. In International Conference on Learning Representations. Klein, A., Falkner, S., Bartels, S., Hennig, P., and Hutter, F. (2017). Fast Bayesian optimization of ma- chine learning hyperparameters on large datasets. InInternational Conference on Artificial Intelligence and Statistics, pages 528–536. PMLR. Koskela, A., Jälkö, J., and Honkela, A. (2020). Computing tight differential privacy guarantees using FFT. In International Conference on Artificial Intelligence and Statistics, pages 2560–2569. PMLR. Krizhevsky, A. and Hinton, G. (2009). Learning multiple layers of features from tiny images. Technical report, University of Toronto. LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324. Li, T., Zaheer, M., Liu, K. Z., Reddi, S. J., McMahan, H. B., and Smith, V. (2023). Differentially private adaptive optimization with delayed preconditioners. InInternational Conference on Learning Representations. 14Liaw, R., Liang, E., Nishihara, R., Moritz, P., Gonzalez, J. E., and Stoica, I. (2018). Tune: A research platform for distributed model selection and training.arXiv preprint arXiv:1807.05118. Liese, F. and Vajda, I. (2006). On divergences and informations in statistics and information theory. IEEE Transactions on Information Theory, 52(10):4394–4412. Liu, J. and Talwar, K. (2019). Private selection from private candidates. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 298–309. Maas, A., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. (2011). Learning word vectors for sentiment analysis. InProceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142–150. Mandt, S., Hoffman, M. D., and Blei, D. M. (2017). Stochastic gradient descent as approximate Bayesian inference. Journal of Machine Learning Research, 18:1–35. McSherry, F. D. (2009). Privacy integrated queries: an extensible platform for privacy-preserving data analysis. InProceedings of the 2009 ACM SIGMOD International Conference on Management of data, pages 19–30. Mironov, I. (2017). Rényi differential privacy. In2017 IEEE 30th computer security foundations sympo- sium (CSF), pages 263–275. IEEE. Mironov, I., Talwar, K., and Zhang, L. (2019). Rényi differential privacy of the sampled Gaussian mechanism. arXiv preprint arXiv:1908.10530. Mohapatra, S., Sasy, S., He, X., Kamath, G., and Thakkar, O. (2022). The role of adaptive optimizers for honest private hyperparameter selection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 7806–7813. Papernot, N. and Steinke, T. (2022). Hyperparameter tuning with Rényi differential privacy. InInter- national Conference on Learning Representations. Papernot, N., Thakurta, A., Song, S., Chien, S., and Erlingsson, Ú. (2020). Tempered sigmoid activations for deep learning with differential privacy. InAAAI Conference on Artificial Intelligence. Sander, T., Stock, P., and Sablayrolles, A. (2022). Tan without a burn: Scaling laws of dp-sgd.arXiv preprint arXiv:2210.03403. Smith, J., Asghar, H. J., Gioiosa, G., Mrabet, S., Gaspers, S., and Tyler, P. (2022). Making the most of parallel composition in differential privacy.Proceedings on Privacy Enhancing Technologies, 1:253–273. Song, S., Chaudhuri, K., and Sarwate, A. D. (2013). Stochastic gradient descent with differentially private updates. In 2013 IEEE global conference on signal and information processing, pages 245–248. IEEE. Steinke, T. (2022). Composition of differential privacy & privacy amplification by subsampling.arXiv preprint arXiv:2210.00597. Swersky, K., Snoek, J., and Adams, R. P. (2013). Multi-task Bayesian optimization.Advances in neural information processing systems, 26. 15Thakurta, A. G. and Smith, A. (2013). Differentially private feature selection via stability arguments, and the robustness of the lasso. InConference on Learning Theory, pages 819–850. PMLR. van der Veen, K. L., Seggers, R., Bloem, P., and Patrini, G. (2018). Three tools for practical differential privacy. NeurIPS 2018 Privacy Preserving Machine Learning workshop, arXiv:1812.02890. Van Erven, T. and Harremos, P. (2014). Rényi divergence and kullback-leibler divergence.IEEE Trans- actions on Information Theory, 60(7):3797–3820. Waring, J., Lindvall, C., and Umeton, R. (2020). Automated machine learning: Review of the state-of- the-art and opportunities for healthcare.Artificial intelligence in medicine, 104:101822. Xiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. Technical report. Yousefpour, A., Shilov, I., Sablayrolles, A., Testuggine, D., Prasad, K., Malek, M., Nguyen, J., Ghosh, S., Bharadwaj, A., Zhao, J., et al. (2021). Opacus: User-friendly differential privacy library in PyTorch. In NeurIPS 2021 Workshop Privacy in Machine Learning, arXiv:2109.12298. Zhu, Y., Dong, J., and Wang, Y.-X. (2022). Optimal accounting of differential privacy via characteristic function. InInternational Conference on Artificial Intelligence and Statistics, pages 4782–4817. PMLR. Zhu, Y. and Wang, Y.-X. (2019). Poisson subsampled Rényi differential privacy. InInternational Con- ference on Machine Learning, pages 7634–7642. 16Appendix A Full Description of Experiments Quality Metric and Evaluation. In all of our experiments, we have a partitioning of the available data into train and test sets and we choose the best model based on the test accuracy. The quality score applied on the test set is a relatively low sensitivity function, and therefore, even for a private test set, parallel composition (for an RDP bound of parallel compositions, we refer to Appendix E) can accommodate DP evaluation of a quality metric in the training budget itself. However, we assume that only the training dataset is private and the test data is public for simplicity. This assumption of test dataset being public and the approach of making only two (train and test) partitions of available data instead of three (train, validation, and test) has been considered in many prior works (Mohapatra et al., 2022; Papernot and Steinke, 2022) to study the proposed method in isolation. Our utility plots (Figures 2 to Figure 8) show the test accuracy of the final model against the final approximate DP ε which includes the privacy cost of the tuning process and of the final model for all methods. We fixδ = 10−5 always when reporting theε-values. We fixq = 0.1 for all of our methods in all experiments. We mention that in several cases smaller value ofq would have lead to better privacy- accuracy trade-off of the final model, however, we use the same valueq = 0 .1 in all experiments for consistency. Methods. We consider the both variants of our proposed approach in our experiments. The RDP parameters for the variant 1 are obtained by using Thm. 6 (new RDP result) and for variant 2 by combining Thm. 4 (subsampling amplification) with Thm. 5 (RDP cost of the hyperparameter tuning). We scale the hyperparameters in our methods for the training data of the final model as discussed in Sec. 3.2. The method by Papernot and Steinke (2022) described in Thm. 5 is the baseline. Datasets and Models.We carry out our experiments on the following standard benchmark datasets for classification: CIFAR-10 (Krizhevsky and Hinton, 2009), MNIST (LeCun et al., 1998), FashionM- NIST (Xiao et al., 2017) and IMDB (Maas et al., 2011). For MNIST and IMDB, we use the convolutional neural networks from the examples provided in the Opacus library Yousefpour et al. (2021). For Fashion- MNIST, we consider a simple feedforward 3-layer network with hidden layers of width 120. For CIFAR-10, we use a Resnet20 pre-trained on CIFAR-100 (Krizhevsky and Hinton, 2009) dataset so that only the last fully connected layer is trained. We minimize the cross-entropy loss in all models. We optimize with DP-SGD and DP-Adam for all datasets. As suggested by Papernot et al. (2020), we replace all ReLU activations with tempered sigmoid functions in CIFAR-10, MNIST, and FashionMNIST networks, which limits the magnitudes of non-private gradients and improves model accuracies. Hyperparameters. For these datasets, in one of the experiments we tune only the learning rate (η), and in the other oneη, γ (subsampling ratio), and the number of epochs, while fixing the clipping constant C. For DP-Adam, we do not scale the learning rate. The number of trainable parameters and the hyperparameter grids are provided in Table 2 (Appendix B). The numbers of epochs are chosen to suit our computational constraints. Following the procedure described in Section 4.1, we always adjust σ such that we compute the smallestσ that satisfies a target final(ε, δ) bound for each (γ, epoch) pair. Furthermore following the RDP accounting procedure desribed in Section 4.1 we obtain uniform RDP guarantees for the candidate models and furthermore RDP guarantees for our methods. Implementation. FortheimplementationofDP-SGDandDP-Adam, weusetheOpacuslibrary(Yousef- pour et al., 2021). For scalability, we explore the hyperparameter spaces with Ray Tune (Liaw et al., 2018) on a dedicated multi-GPU cluster. We use two sets of gpus and one set is much weaker than the 17other. All three methods have independent randomness (e.g. K hyperparameter candidates sampled, DP noise). Ray tune maintains a job queue and the number of models trained parallelly depends on the gpu count. Each model is trained on 0.5 gpu, and each gpu has enough memory to accommodate 2 models. Models finish their entire training on the same gpu that was allocated to them at the start of their training. Measuring training time. We want to reduce the dependence of our measurements on the available hardware resources (e.g. number of gpus). For each method, we store the per epoch time in seconds for each model being trained in parallel, and finally sum the timings for all epochs of allK models (as if they were trained serially). The clock for each model starts only when it is under execution. Among variant 1 and 2, the final training time depends mainly on the value of K sampled, though variant 1 trains the final model with slightly smaller data. The baseline method takes much more time to run compared to our methods on weaker GPUs, which explains the differences in speed gains. 18B Hyperparameter Tables Table 1: Tuningη: rest of the hyperparameters are fixed to these values. The hyperaparameter grids for the learning rates are the same as in the second experiment and are given in Table 2. MNIST FashionMNIST CIFAR-10 IMDB γ = B N 0.0213 0.0213 0.0256 0.0256 epochs 40 40 40 110 Table 2: Tuningσ, η and T: datasets and the corresponding hyperparameter grids. train/test set parameters C B learning rate ( η) epochs MNIST 60k/10k ∼26k 1 {128, 256} {10−i}i∈{4,3.5,3,2.5,2,1.5,1,0.5,0} {10,20,30,40} FashionMNIST 60k/10k ∼109k 3 {128, 256} {10−i}i∈{3,2.5,2,1.5,1,0.5,0,−0.5,−1,−1.5} {10,20,30,40} CIFAR-10 50k/10k 0.65k 3 {64, 128} {10−i}i∈{3,2.5,2,1.5,1,0.5,0,−0.5,−1,−1.5} {20,30,40} IMDB 25k/25k ∼464k 1 {64, 128} {10−i}i∈{4,3.5,3,2.5,2,1.5,1,0.5,0} {70,90,110} C Proof of Thm 6 Proof. For the proof, let us denote for shortMtune =: M1 and Mbase =: M2. Let X ∈ Xn and x′ ∈ X We first consider bounding the Rényi divergenceDα \u0000 M(X ∪ {x′})||M(X) \u0001 . Looking at our approach which uses Poisson subsampling with subsampling ratioq to obtain the dataset X1, and conditioning the output on the randomness in choosingX1, we can write the mechanism as a mixture over all possible choices ofX1 as M(X) = X X1 pX1 · \u0000 M1(X1), M2(M1(X1), X\\X1) \u0001 , (C.1) where pX1 is the probability of samplingX1. Since each data element is inX1 with probabilityq, we can furthermore writeM(X ∪ {x′}) as a mixture M(X ∪ {x′}) = X X1 pX1 · \u0012 q · \u0000 M1(X1 ∪ {x′}), M2(M1(X1 ∪ {x′}), X\\X1) \u0001 + (1 − q) · \u0000 M1(X1), M2(M1(X1), X\\X1 ∪ {x′}) \u0001\u0013 . (C.2) From the quasi-convexity of the Rényi divergence (Van Erven and Harremos, 2014) and the expres- sions (C.1) and (C.2), it follows that Dα \u0000 M(X ∪ {x′})||M(X) \u0001 ≤ sup X1 Dα \u0000 q · \u0000 M1(X1 ∪ {x′}), M2(M1(X1 ∪ {x′}), X\\X1) \u0001 + (1 − q) · \u0000 M1(X1), M2(M1(X1), X\\X1 ∪ {x′}) \u0001 || \u0000 M1(X1), M2(M1(X1), X\\X1) \u0001\u0001 . (C.3) Our aim is to express the right-hand side of the inequality (C.3) in terms of RDP parameters ofM1 and M2. To this end, take an arbitraryX1 ⊂ X, and denote by 19• eP(t) the density function ofM1(X1 ∪ {x′}), • P(t) the density function ofM1(X1), • eQ(t, s) the density function ofM2(t, X\\X1 ∪ {x′}) for auxiliary variablet (the output ofM1), • Q(t, s) the density function ofM2(t, X\\X1) for auxiliary variablet. Then, we see that P \u0000\u0000 M1(X1), M2(M1(X1), X\\X1) \u0001 = (t, s) \u0001 = P(t) · Q(t, s) and similarly that P \u0000 q · \u0000 M1(X1 ∪ {x′}), M2(M1(X1 ∪ {x′}), X\\X1) \u0001 + (1 − q) · \u0000 M1(X1), M2(M1(X1), X\\X1 ∪ {x′}) \u0001 = (t, s) \u0001 = q · P \u0000\u0000 M1(X1 ∪ {x′}), M2(M1(X1 ∪ {x′}), X\\X1) \u0001 = (t, s) \u0001 + (1 − q) · P \u0000\u0000 M1(X1), M2(M1(X1), X\\X1 ∪ {x′}) \u0001 = (t, s) \u0001 = q · eP(t) · Q(t, s) + (1− q) · P(t) · eQ(t, s). By the definition of the Rényi divergence, we have that exp \u0012 (α − 1)Dα \u0000 q · \u0000 M1(X1 ∪ {x′}), M2(M1(X1 ∪ {x′}), X\\X1) \u0001 + (1 − q) · \u0000 M1(X1), M2(M1(X1), X\\X1 ∪ {x′}) \u0001 || \u0000 M1(X1), M2(M1(X1), X\\X1) \u0001\u0001\u0013 = Z Z   q · eP(t) · Q(t, s) + (1− q) · P(t) · eQ(t, s) P(t) · Q(t, s) !α · P(t) · Q(t, s) dt ds. (C.4) 20which can be expanded as Z Z   q · eP(t) · Q(t, s) + (1− q) · P(t) · eQ P(t) · Q(t, s) !α P(t) · Q(t, s) dt ds = Z Z   q · eP(t) P(t) + (1 − q) · eQ(t, s) Q(t, s) !α P(t) · Q(t, s) dt ds = Z Z qα  eP(t) P(t) !α P(t) · Q(t, s) dt ds + Z Z (1 − q)α  eQ(t, s) Q(t, s) !α P(t) · Q(t, s) dt ds + Z Z α · qα−1 · (1 − q) ·  eP(t) P(t) !α−1 P(t) · eQ(t, s) dt ds + Z Z α · q · (1 − q)α−1 ·  eQ(t, s) Q(t, s) !α−1 Q(t, s) · eP(t) dt ds + Z Z α−2X j=2 \u0012α j \u0013 · qα−j · (1 − q)j ·    eP(t) P(t) !α−j P(t)      eQ(t, s) Q(t, s) !j Q(t, s)   dt ds. (C.5) We next bound five integrals on the right hand side of Equation (C.5). For the first two integrals, we use the RDP-bounds forM1 and M2 to obtain Z Z  eP(t) P(t) !α P(t)Q(t, s) dt ds = Z  eP(t) P(t) !α P(t) dt ≤ exp \u0000 (α − 1)ε1(α) \u0001 . (C.6) and Z Z  eQ(t, s) Q(t, s) !α Q(t, s)P(t) ds dt ≤ Z exp \u0000 (α − 1)ε2(α) \u0001 P(t) dt = exp \u0000 (α − 1)ε2(α) \u0001 , (C.7) where ε1 and ε2 give the RDP-parameters of order α for M1 and M2, respectively. The third and fourth integral can be bounded analogously. In the second inequality we have also used the fact that the RDP-parameters ofM2 are independent of the auxiliary variablet. Similarly, for the third integral, we have Z Z    eP(t) P(t) !α−j P(t)      eQ(t, s) Q(t, s) !j Q(t, s)   ds dt ≤ Z    eP(t) P(t) !α−j P(t)  exp \u0000 (j − 1)ε2(j) \u0001 dt ≤ exp \u0000 (α − j − 1)ε1(α − j) \u0001 · exp \u0000 (j − 1)ε2(j) \u0001 . (C.8) 21Substituting (C.6), (C.7) (and similar expressions for the third and fourth integral) and (C.8) to Equa- tion (C.5), we get a bound for the right-hand side of Equation (C.4). SinceX1 ⊂ X was arbitrary, we arrive at the claim via the inequality (C.3). Next, weconsiderbounding Dα \u0000 M(X)||M(Y ) \u0001 . Theproofgoessimilarlyastheonefor Dα \u0000 M(Y )||M(X) \u0001 . Denote ε2(α) = Dα \u0000 M(X)||M(X ∪ {x′}) \u0001 . With the notation of proof of Thm. 6, we see that, instead of the right-hand side of(C.4), we need to bound exp \u0000 (α − 1)ε2(α) \u0001 = Z Z   P(t) · Q(t, s) q · eP(t) · Q(t, s) + (1− q) · P(t) · eQ(t, s) !α (q · eP(t) · Q(t, s) + (1− q) · P(t) · eQ(t, s)) dt ds. In order to use here the series approach, we need to use Lemma C.1:   P · Q q · eP · Q + (1 − q) · P · eQ !α (q · eP · Q + (1 − q) · P · eQ) =   P · Q q · eP · Q + (1 − q) · P · eQ !α−1 · P · Q =   q · eP P + (1 − q) · eQ Q !1−α · P · Q =   q · eP P Q eQ + (1 − q) !1−α ·  eQ Q !1−α · P · Q =   q · eP P Q eQ + (1 − q) !1−α · \u0012Q eQ \u0013α−1 · P · Q ≤   q · P eP eQ Q + (1 − q) !α−1 · \u0012Q eQ \u0013α−1 · P · Q =   q · P eP eQ Q + (1 − q) !α−1 · \u0012Q eQ \u0013α · P · eQ, (C.9) 22where in the inequality we have used Lemma C.1. Now we can expand \u0010 q · P eP eQ Q + 1 − q \u0011α−1 :   1 − q + q · P eP eQ Q !α−1 · \u0012Q eQ \u0013α · P · eQ =   α−1X j=0 \u0012α − 1 j \u0013 qj · (1 − q)α−1−j · \u0012P eP \u0013j  eQ Q !j  · \u0012Q eQ \u0013α · P · eQ =   α−1X j=0 \u0012α − 1 j \u0013 qj · (1 − q)α−1−j · \u0012P eP \u0013j \u0012Q eQ \u0013α−j   · P · eQ = α−1X j=0 \u0012α − 1 j \u0013 qj · (1 − q)α−1−j · \u0012P eP \u0013j+1 eP · \u0012Q eQ \u0013α−j eQ. Then, we use the knownε1(α) and ε2(α)-values as in the inequalities (C.8) to arrive at the claim. C.1 Auxiliary Lemma We need the following inequality for the proof of Thm 6. Lemma C.1(Lemma 35, Steinke 2022). For allp ∈ [0, 1] and x ∈ (0, ∞), 1 1 − p + p x ≤ 1 − p + p · x. D Additional Details to Section 4 D.1 Random Search Here, we assume we are given some distributions of the hyperparameter candidates and the algorithmQ draws hyperparameters using them. In order to adjust the noise level for each candidate, we take aα-line as an RDP upper bound. More specifically, we require that the candidate models are \u0000 α, c· α \u0001 -RDP for some c >0 and for allα ∈ Λ. Then the noise scaleσγ,T for each draw of(γ, T) is the minimum scale for which the \u0000 α, c· α \u0001 -RDP bound holds, i.e., σγ,T = min{σ ∈ R+ : T · εγ,σ (α) ≤ c · α for allα ∈ Λ}. Similarly, we can find the maximumT based onσ and γ such that the mechanism is \u0000 α, c· α \u0001 -RDP for all α ∈ Λ. Again, by Lemma 9 below, the candidate picking algorithmQ is thenc · α-RDP and we may use Thm. 5 to obtain RDP bounds for the tuning algorithm. 23D.1.1 Adjusting the Parameters T and σ for DP-SGD We next discuss the reasons for the success of strategies described in Section 4. It is often a good approximation to say that the RDP-guarantees of the Poisson subsampled Gaussian mechanism are lines as functions of the RDP orderα, i.e., that the guarantees are those a Gaussian mechanism with some sensitivity and noise level values. For example, (Thm. 11, Mironov et al., 2019) show that the Poisson subsampled Gaussian mechanism is \u0000 α, 2γ2α/σ2\u0001 -RDP when α is sufficiently small. Also, (Thm. 38, Steinke, 2022) show that if the underlying mechanism isρ-zCDP, then the Poisson subsampled version with subsampling ratio γ is \u0000 α, 10γ2ρα \u0001 -RDP when α is sufficiently small. Notice that the Gaussian mechanism withL2-sensitivity ∆ and noise levelσ is (∆2/2σ2)-zCDP (Bun and Steinke, 2016). We numerically observe, that the larger the noise levelσ and the smaller the subsampling ratioγ, the better the line approximation of the RDP-guarantees (see Figure 9). In case the privacy guarantees (either (ε, δ)-DP or RDP) are approximately those of a Gaussian mechanisms with some sensitivity and noise level values, both of the approaches for tuning the hyper- parameters γ, σ and T described in Section 4 would lead to very little slack. This is because for the Gaussian mechanism, both the RDP guarantees (Mironov, 2017) and(ε, δ)-DP guarantees (Dong et al., 2022) depend monotonously on the scaled parameter eσ = σ ∆ · √ T . This means that if we adjust the training lengthT based on values ofσ by having some target(δ, ε)- bound for the candidate model with grid search of Section 4.1, the resulting RDP upper bounds of different candidates will not be far from each other (and similarly for adjustingσ based on value ofT). Similarly, for random search of Section D.1, when adjustingT based on values ofσ, the RDP guarantees of all the candidate models would be close to the upper bound (c · α, c >0), i.e., they would not be far from each other. D.2 Proof of Lemma 9 Lemma D.1. Denote by β the random variable of which outcomes are the hyperparameter candidates (drawing either randomly from a grid or from given distributions). Consider an algorithmQ, that first randomly picks hyperparameterst ∼ β, then runs a randomised mechanismM(t, X). Suppose M(t, X) is \u0000 α, ε(α) \u0001 -RDP for allt. Then, Q is \u0000 α, ε(α) \u0001 -RDP. Proof. Supposethehyperparameters t areoutcomesofarandomvariable β. LetX andY beneighbouring datasets. Then, ifp(t, s) and q(t, s) (as functions ofs) give the density functions ofM(t, X) and M(t, Y), respectively, we have that Q(X) ∼ Et∼β p(t, s) and Q(Y ) ∼ Et∼β q(t, s). Since for any distributionsp and q, and for anyα >1, exp \u0000 (α − 1)Dα(p||q) \u0001 = R \u0010 p(t) q(t) \u0011α q(t) dt gives an f-divergence (forf(x) = xα), it is also jointly convex w.r.t.p and q (Liese and Vajda, 2006). Using 24Jensen’s inequality, we have exp \u0000 (α − 1)Dα \u0000 Q(X)||Q(Y ) \u0001\u0001 = Z \u0012Et∼β p(t, s) Et∼β q(t, s) \u0013α · Et∼β q(t, s) ds ≤ Et∼β Z \u0012p(t, s) q(t, s) \u0013α · q(t, s) ds ≤ Et∼β exp \u0000 (α − 1)Dα \u0000 M(t, X)||M(t, Y) \u0001\u0001 = exp ((α − 1)ε(α)) from which the claim follows. E f-Divergence of Parallel Compositions We first formulate the parallel composition result for generalf-divergences (Lemma E.1). We then obtain the RDP bound for parallel compositions as a corollary (Cor. E.2). Our Lemma E.1 below can be seen as anf-divergence version of the(ε, 0)-DP result given in (Thm. 4 McSherry, 2009). Corollary 2 by (Smith et al., 2022) gives the corresponding result in terms ofµ-Gaussian differential privacy (GDP), and it is a special case of our Lemma E.1 asµ-GDP equals the(ε, δ)-DP (i.e., the hockey-stick divergence) of the Gaussian mechanism with a certain noise scale (Cor. 1, Dong et al., 2022). We definef-divergence for distributions onRd as follows. Consider two probability densitiesP and Q defined onRd, such that ifQ(x) = 0 then alsoP(x) = 0, and a convex functionf : [0 , ∞) → R. Then, an f-divergence (Liese and Vajda, 2006) is defined as Df (P||Q) = Z f \u0012P(t) Q(t) \u0013 Q(t) dt. In case the data is divided into disjoint shards and separate mechanisms are applied to each shard, thef- divergence upper bound for two neighbouring datasets can be obtained from the individualf-divergence upper bounds: Lemma E.1. Suppose a datasetX ∈ XN is divided intok disjoint shardsXi, i ∈ [k], and mechanisms Mi, i ∈ [k], are applied to the shards, respectively. Consider the adaptive composition M(X) = \u0000 M1(X1), M2(X2, M1(X1)), . . . ,Mk(Xk, M1(X1), . . . ,Mk−1(Xk−1)) \u0001 . Then, we have that max X∼Y Df \u0000 M(X)||M(Y ) \u0001 ≤ max i∈[k] max X∼Y Df \u0000 Mi(X)||Mi(Y ) \u0001 . Proof. Let X and Y be divided intok equal-sized disjoint shards and supposeY is a neighbouring dataset suchthat X andY differin jth shard, i.e.,Xj ∼ Yj andY = \u0000 Y1, Y2, . . . , Yk \u0001 = \u0000 X1, . . . , Xj−1, Yj, Xj+1, . . . , Xk \u0001 . 25Then, we see that P \u0000 M(X) = (a1, . . . , ak) \u0001 P \u0000 M(Y ) = (a1, . . . , ak) \u0001 = P \u0000 M1(X1) = a1 \u0001 · P \u0000 M1(X2, a1) = a2 \u0001 ··· P \u0000 Mk(Xk, a1, . . . , ak−1) = ak \u0001 P \u0000 M1(Y1) = a1 \u0001 · P \u0000 M1(Y2, a1) = a2 \u0001 ··· P \u0000 Mk(Yk, a1, . . . , ak−1) = ak \u0001 = P \u0000 Mj(Xj, a1, . . . , aj−1) = aj \u0001 P \u0000 Mj(Yj, a1, . . . , aj−1) = aj \u0001 . and furthermore, denotinga = (a1, . . . , ak), Df \u0000 M(X)||M(Y )) \u0001 = Z f   P \u0000 M(X) = a \u0001 P \u0000 M(Y ) = a \u0001 ! P \u0000 M(Y ) = a \u0001 da = Z f   P \u0000 Mj(Xj, a1, . . . , aj−1) = aj \u0001 P \u0000 Mj(Yj, a1, . . . , aj−1) = aj \u0001 ! P \u0000 M(Y ) = a \u0001 da = Z f   P \u0000 Mj(Xj, a1, . . . , aj−1) = aj \u0001 P \u0000 Mj(Yj, a1, . . . , aj−1) = aj \u0001 ! P \u0000 M1(Y1) = a1 \u0001 · P \u0000 M1(Y2, a1) = a2 \u0001 ··· P \u0000 Mj(Yj, a1, . . . , aj−1) = aj \u0001 da1 . . .daj = Df (Mj(Xj)||Mj(Yj)) \u0001 · Z P \u0000 M1(Y1) = a1 \u0001 · P \u0000 M1(Y2, a1) = a2 \u0001 ··· P \u0000 Mj−1(Yj−1, a1, . . . , aj−2) = aj−1 \u0001 da1 . . .daj−1 = Df (Mj(Xj)||Mj(Yj)) \u0001 . Thus, Df (M(X)||M(Y )) = Df (Mj(Xj)||Mj(Yj)) ≤ max X∼Y Df (Mj(X)||Mj(Y )) and also, we have that max X∼Y Df (M(X)||M(Y )) = max i∈[k] max X∼Y Df (Mi(X)||Mi(Y )). Corollary E.2.Suppose a datasetX ∈ XN is divided intok disjoint shardsXi, i ∈ [k], and mechanisms Mi, i ∈ [k], are applied to the shards, respectively. Consider the mechanism M(X) = \u0000 M1(X1), . . . ,Mk(Xk) \u0001 . Suppose eachMi is \u0000 α, εi(α) \u0001 -RDP, respectively. Then,M is \u0000 α, maxi∈[k] εi(α) \u0001 -RDP. Proof. This follows from Lemma E.1 since exp \u0000 (α − 1)Dα(M(X)||M(Y )) \u0001 = Z   P \u0000 M(X) = a \u0001 P \u0000 M(Y ) = a \u0001 !α P \u0000 M(Y ) = a \u0001 da 26is anf-divergence forf(x) = xα. Thus, by Lemma E.1 we have that max X∼Y exp \u0000 (α − 1)Dα(M(X)||M(Y )) \u0001 ≤ max i∈[k] max X∼Y exp \u0000 (α − 1)Dα(Mi(X)||Mi(Y )) \u0001 from which it follows that max X∼Y Dα(M(X)||M(Y )) ≤ max i∈[k] max X∼Y Dα(Mi(X)||Mi(Y )) = max i∈[k] εi(α). 27(a) trained with DP-SGD (b) trained with DP-SGD (c) trained with DP-SGD (d) trained with DP-SGD Figure 5: Tuning only the learning rate with DP-SGD. Test accuracies are averaged across 10 independent runs and the error bars denote the standard error of the mean. The number in the legends in the first column refer to the scaled mean training timings for the baseline method with respect to the fastest of variant 1 and 2. The second column plots finalε vs. mean σ. Our methods inject significantly smaller noise compared to the baseline for allε regimes. We also observe that due to tight analysis in Thm 6,σ for variant 1 is consistently lower than for variant 2. As a result, we see slightly higher accuracy for variant 1 in many cases. The third column plots finalε vs. mean optimal η. Note that due to randomess in the candidate selection process, optimalη’s for all three methods need not be the same. For perspective, we also add curves showing the privacy cost of training a single model with optimal hyperparameters obtained from the baseline. 28(a) trained with DP-Adam (b) trained with DP-Adam (c) trained with DP-Adam (d) trained with DP-Adam Figure 6: Tuning only the learning rate with DP-Adam. Test accuracies are averaged across 10 inde- pendent runs and the error bars denote the standard error of the mean. No learning rate scaling was applied. The number in the legends in the first column refer to the scaled mean training timings for the baseline method with respect to the fastest of variant 1 and 2. The second column plots finalε vs. mean σ. Our methods inject significantly smaller noise compared to the baseline for allε regimes. We also observe that due to tight analysis in Thm 6,σ for variant 1 is consistently lower than for variant 2. As a result, we see slightly higher accuracy for variant 1 in many cases. The third column plots finalε vs. mean optimalη. Note that due to randomess in the candidate selection process, optimalη’s for all three methods need not be the same. For perspective, we also add curves showing the privacy cost of training a single model with optimal hyperparameters obtained from the baseline. 29(a) trained with DP-SGD (b) trained with DP-SGD (c) trained with DP-SGD (d) trained with DP-SGD Figure 7: Tuning all hyperparameters (η, epochs, and γ) with DP-SGD. Test accuracies are averaged across 10 independent runs and the error bars denote the standard error of the mean. The numbers in the legends in the first column refer to the scaled mean training timings for the baseline method with respect to the fastest of variant 1 and 2. The second column plots finalε vs. meanσ. Our methods inject significantly smaller noise compared to the baseline for allε regimes even at high values ofµ. Since there are several hyperparameters at play together, it is difficult to attribute slightly higher accuracy for variant 2 compared to 1 to any one factor. However, we suspect slight inferiority of our bound for higher values of µ (Figure 1, right plot) and higher training dataset for the final model in variant 2 could be the main reasons. The third (and fourth) column plots finalε vs. mean optimalη (and γ). Noise levelσ is a proxy for number of epochs, since we obtain aσ for each combination ofγ and number of epochs. Therefore, we omit plots showing finalϵ vs. epochs plot for readability. Note that due to randomness in the candidate selection process, optimalη’s andγ’s for all three methods need not be the same. For perspective, we also add curves showing the privacy cost of training a single model with optimal hyperparameters obtained from the baseline. 30(a) trained with DP-Adam (b) trained with DP-Adam (c) trained with DP-Adam Figure 8: Tuning all hyperparameters (η, epochs, andγ) with DP-Adam. Test accuracies are averaged across 10 independent runs and the error bars denote the standard errors of the means. The number in the legends in the first column refer to the scaled mean training timings for the baseline method with respect to the fastest of variant 1 and 2. The second column plots finalε vs. mean σ. Our methods inject significantly smaller noise compared to the baseline for allε regimes even at high values ofµ. Since there are several hyperparameters at play together, it is difficult to attribute slightly higher accuracy for variant 2 compared to 1 to any one factor. However, we suspect slight inferiority of our bound for higher values ofµ (Figure 1, right plot) and higher training dataset for the final model in variant 2 could be the main reasons. The third (and fourth) column plots finalε vs. mean optimalη (and γ). Noise level σ is a proxy for number of epochs, since we obtain aσ for each combination ofγ and number of epochs. Therefore, we omit plots showing finalϵ vs. epochs plot for readability. Note that due to randomness in the candidate selection process, optimalη’s andγ’s for all three methods need not be the same. 315 10 15 20 25 0.05 0.10 0.15 0.20 0.25 0.30 0.35( ) =2, T = 1000 Gauss RDP-fit ( =2) =4, T = 4000 Gauss RDP-fit ( =4) =6, T = 8000 Gauss RDP-fit ( =6) 5 10 15 20 25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6( ) =2, T = 1000 Gauss RDP-fit ( =2) =4, T = 4000 Gauss RDP-fit ( =4) =6, T = 8000 Gauss RDP-fit ( =6) Figure 9: DP-SGD RDP curves for different values of noise levelσ and number of compostionsT. Left: γ = 1/100, right: γ = 1/50 and the corresponding lines with the smallest slope that give upper bounds for the RDP orders up toα = 24. 32",
      "meta_data": {
        "arxiv_id": "2301.11989v3",
        "authors": [
          "Antti Koskela",
          "Tejas Kulkarni"
        ],
        "published_date": "2023-01-27T21:01:58Z",
        "venue": "NeurIPS 2023",
        "pdf_url": "https://arxiv.org/pdf/2301.11989v3.pdf"
      }
    }
  ]
}